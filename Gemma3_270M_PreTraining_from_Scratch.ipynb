{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3461c975279b484099c4d564dcbd0a9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f71713eaf41d40bbade776f3516791a1",
              "IPY_MODEL_2fdcc207c648468794bd6e2773af212a",
              "IPY_MODEL_fe6d1b2857e74255b7036dd1d0c3ee59"
            ],
            "layout": "IPY_MODEL_3c4c1873f7a94276a42fa257ae5bd5d0"
          }
        },
        "f71713eaf41d40bbade776f3516791a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9405abd4968343f3bcdd95420dd8faad",
            "placeholder": "​",
            "style": "IPY_MODEL_cdaf48c24fcb47d79cf9ba8f27d7b861",
            "value": "Processing Files (2 / 2)      : 100%"
          }
        },
        "2fdcc207c648468794bd6e2773af212a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0212db93c984c66bc609854d07d7b6a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd3e5a4134214a34a3af509efb42237d",
            "value": 1
          }
        },
        "fe6d1b2857e74255b7036dd1d0c3ee59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0778e0f4c6474c84be361f8d0a35b5a3",
            "placeholder": "​",
            "style": "IPY_MODEL_fb91965f15f440d29c15784f173a2316",
            "value": "  329MB /  329MB, 32.9MB/s  "
          }
        },
        "3c4c1873f7a94276a42fa257ae5bd5d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9405abd4968343f3bcdd95420dd8faad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdaf48c24fcb47d79cf9ba8f27d7b861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0212db93c984c66bc609854d07d7b6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cd3e5a4134214a34a3af509efb42237d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0778e0f4c6474c84be361f8d0a35b5a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb91965f15f440d29c15784f173a2316": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81183fc28c5c4c469b1c145b8ed61bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f29dcae923034d99a46f9edd8c656751",
              "IPY_MODEL_81920ba5350640c0b1fd323316e35403",
              "IPY_MODEL_546fcfa06b814091ba568656853db1e9"
            ],
            "layout": "IPY_MODEL_c9a5de4a6e6a46ee8e090c73f6dba92b"
          }
        },
        "f29dcae923034d99a46f9edd8c656751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a573b3b59cea4ea681c762c9f383491f",
            "placeholder": "​",
            "style": "IPY_MODEL_8852f7996aec4b48af87c92257e32b2e",
            "value": "New Data Upload               : 100%"
          }
        },
        "81920ba5350640c0b1fd323316e35403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c83b5a4083c4fb19d07e3e5569f2ea1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08737d7be6ea43398e313a9ea9a99f3a",
            "value": 1
          }
        },
        "546fcfa06b814091ba568656853db1e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c33f9b80d0449a78ed15f5991926caf",
            "placeholder": "​",
            "style": "IPY_MODEL_a2a6c7e725f045f388ddeaec533c7479",
            "value": "  329MB /  329MB, 32.9MB/s  "
          }
        },
        "c9a5de4a6e6a46ee8e090c73f6dba92b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a573b3b59cea4ea681c762c9f383491f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8852f7996aec4b48af87c92257e32b2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c83b5a4083c4fb19d07e3e5569f2ea1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "08737d7be6ea43398e313a9ea9a99f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c33f9b80d0449a78ed15f5991926caf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2a6c7e725f045f388ddeaec533c7479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1e5713b9c3c47169f862ba44151d771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9b34d5823784867a650f83261f2ca2e",
              "IPY_MODEL_ea65472728744f24b11f01fced4e4e95",
              "IPY_MODEL_9a9f3333f0b849c599d8e5b47c4edb51"
            ],
            "layout": "IPY_MODEL_342e204d53b14894aa9f187d2df5eee8"
          }
        },
        "d9b34d5823784867a650f83261f2ca2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3462bc37b0b94b1cb707b0a9b2ded188",
            "placeholder": "​",
            "style": "IPY_MODEL_9134d1298c7f488aac90ba1800922ea8",
            "value": "  ..._export/pytorch_model.bin: 100%"
          }
        },
        "ea65472728744f24b11f01fced4e4e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3443086ca974654bdebae8194c94750",
            "max": 329341959,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32089d1b852e491bb3ff8bdaa3cacf20",
            "value": 329341959
          }
        },
        "9a9f3333f0b849c599d8e5b47c4edb51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52d8a05076b4406a9349678c7d9bc6f1",
            "placeholder": "​",
            "style": "IPY_MODEL_a44538869a46472fa4bb8ce162e4baae",
            "value": "  329MB /  329MB            "
          }
        },
        "342e204d53b14894aa9f187d2df5eee8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3462bc37b0b94b1cb707b0a9b2ded188": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9134d1298c7f488aac90ba1800922ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3443086ca974654bdebae8194c94750": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32089d1b852e491bb3ff8bdaa3cacf20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52d8a05076b4406a9349678c7d9bc6f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a44538869a46472fa4bb8ce162e4baae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1778f07ab2624953922aa2932c76a0e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5963234a9e0940d2977a75e3cf558c1e",
              "IPY_MODEL_fdd2c74a43644d989851a361eeb7f0f3",
              "IPY_MODEL_be00eb2f82fb4b10a7621c8e452e7046"
            ],
            "layout": "IPY_MODEL_2270581310614782a2dc3078c94d90e2"
          }
        },
        "5963234a9e0940d2977a75e3cf558c1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99ff6b9d5e0347788f8f5eb47df63fc9",
            "placeholder": "​",
            "style": "IPY_MODEL_edb4b23730f3477ea54c794cf8312d6f",
            "value": "  ...hf_export/loss_curves.png: 100%"
          }
        },
        "fdd2c74a43644d989851a361eeb7f0f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb510280b81e4aa4b82f1d686f6fdaf6",
            "max": 102699,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4cb71cffc4f848888c8b3af0be166501",
            "value": 102699
          }
        },
        "be00eb2f82fb4b10a7621c8e452e7046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b73a3ef5bd048638c34affa269eb83b",
            "placeholder": "​",
            "style": "IPY_MODEL_bd96dd5cde22457bbf9233ae75c052e4",
            "value": "  103kB /  103kB            "
          }
        },
        "2270581310614782a2dc3078c94d90e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99ff6b9d5e0347788f8f5eb47df63fc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edb4b23730f3477ea54c794cf8312d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb510280b81e4aa4b82f1d686f6fdaf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cb71cffc4f848888c8b3af0be166501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b73a3ef5bd048638c34affa269eb83b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd96dd5cde22457bbf9233ae75c052e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvG71CME84Dk",
        "outputId": "f7e8051f-38ee-4d6a-c553-3e6523676507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GPU: NVIDIA A100-SXM4-40GB\n",
            "   VRAM: 42.4 GB\n",
            "   CUDA: 12.8\n",
            "   PyTorch: 2.9.0+cu128\n",
            "🚀 Ready to go!\n"
          ]
        }
      ],
      "source": [
        "# ✅ GPU Verification Cell\n",
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"❌ Still no GPU! Go to Runtime → Change runtime type → Select GPU\")\n",
        "else:\n",
        "    print(f\"✅ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"   CUDA: {torch.version.cuda}\")\n",
        "    print(f\"   PyTorch: {torch.__version__}\")\n",
        "    print(\"🚀 Ready to go!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 1: Environment Setup, Installation & Imports                      ║\n",
        "# ║  Credits: Vizuara Team - Raj | Google DeepMind | Sebastian Raschka      ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "# Step 1: Install dependencies\n",
        "!pip install datasets tiktoken tqdm matplotlib -q\n",
        "\n",
        "# Step 2: Mount Google Drive (saves checkpoints permanently)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, sys, json, math, time, struct, inspect\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "from datasets import load_dataset\n",
        "import tiktoken\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ── Device Configuration ──\n",
        "device = torch.device(\"cuda\")\n",
        "device_type = \"cuda\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "print(f\"✅ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "print(f\"   Mixed Precision: {dtype}\")\n",
        "\n",
        "# ── Directory Setup ──\n",
        "PROJECT_DIR = Path(\"/content/drive/MyDrive/Gemma3_270M_Project\")\n",
        "DATA_DIR = Path(\"/content/data\")\n",
        "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
        "MODEL_DIR = PROJECT_DIR / \"checkpoints\"\n",
        "RESULTS_DIR = PROJECT_DIR / \"results\"\n",
        "\n",
        "for d in [DATA_DIR, PROCESSED_DIR, MODEL_DIR, RESULTS_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\n📁 Checkpoints & Results → Google Drive (survives disconnects)\")\n",
        "print(f\"   {MODEL_DIR}\")\n",
        "\n",
        "# ── Reproducibility ──\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"  🚀 Cell 1 Done! Environment ready.\")\n",
        "print(f\"{'='*60}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq2fPgiw9CmI",
        "outputId": "b242194a-a33f-4acc-994b-ac5076bfefa9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ GPU: NVIDIA A100-SXM4-40GB\n",
            "   VRAM: 42.4 GB\n",
            "   Mixed Precision: torch.bfloat16\n",
            "\n",
            "📁 Checkpoints & Results → Google Drive (survives disconnects)\n",
            "   /content/drive/MyDrive/Gemma3_270M_Project/checkpoints\n",
            "\n",
            "============================================================\n",
            "  🚀 Cell 1 Done! Environment ready.\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 2: Model Configuration — Gemma 3 270M Architecture               ║\n",
        "# ║                                                                         ║\n",
        "# ║  Total Parameters: ~270M (170M embedding + 100M transformer)            ║\n",
        "# ║  This cell defines EVERY hyperparameter of the model.                   ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "@dataclass\n",
        "class Gemma3Config:\n",
        "    \"\"\"\n",
        "    Complete configuration for Gemma 3 270M.\n",
        "\n",
        "    Architecture breakdown:\n",
        "    ┌─────────────────────────────────────────────────┐\n",
        "    │  Embedding Layer:  vocab_size × emb_dim         │\n",
        "    │                    50,257 × 640 = ~32M params   │\n",
        "    │  (Note: Using GPT-2 tokenizer instead of        │\n",
        "    │   Gemma's 256K vocab → saves ~138M params!)     │\n",
        "    │                                                 │\n",
        "    │  Transformer:  18 layers × ~5.5M each           │\n",
        "    │                = ~100M params                    │\n",
        "    │                                                 │\n",
        "    │  Output Layer:  Tied with embedding (no extra)  │\n",
        "    │                                                 │\n",
        "    │  Total: ~132M (with GPT-2 vocab)                │\n",
        "    │  Original Gemma3: ~270M (with 256K vocab)       │\n",
        "    └─────────────────────────────────────────────────┘\n",
        "    \"\"\"\n",
        "\n",
        "    # ── Tokenizer ──\n",
        "    vocab_size: int = 50_257          # GPT-2 tokenizer vocabulary\n",
        "                                       # Original Gemma3 uses 262,144\n",
        "\n",
        "    # ── Core Dimensions ──\n",
        "    context_length: int = 32_768      # Max sequence length (32K tokens)\n",
        "    emb_dim: int = 640                # Embedding dimension (d_model)\n",
        "    n_layers: int = 18                # Number of transformer blocks\n",
        "    n_heads: int = 4                  # Number of query attention heads\n",
        "    head_dim: int = 256               # Dimension per head (emb_dim is NOT n_heads × head_dim here!)\n",
        "    hidden_dim: int = 2048            # FFN intermediate dimension\n",
        "\n",
        "    # ── Multi-Query Attention (MQA) ──\n",
        "    n_kv_groups: int = 1              # KV groups = 1 means Multi-Query Attention\n",
        "                                       # All query heads share 1 key and 1 value head\n",
        "                                       # Saves memory: KV cache is n_kv_groups × head_dim\n",
        "                                       # Instead of:   n_heads × head_dim\n",
        "\n",
        "    # ── QK Normalization ──\n",
        "    qk_norm: bool = True              # Apply RMS norm to Q and K before attention\n",
        "                                       # Stabilizes training at scale\n",
        "\n",
        "    # ── Attention Scaling ──\n",
        "    query_pre_attn_scalar: int = 256  # Scale factor for attention scores\n",
        "                                       # attn = (Q @ K^T) / sqrt(scalar)\n",
        "                                       # Gemma3 uses head_dim (256) instead of usual sqrt(d_k)\n",
        "\n",
        "    # ── RoPE (Rotary Positional Encoding) ──\n",
        "    rope_base: float = 1_000_000.0    # Base frequency for FULL attention layers (global)\n",
        "                                       # Higher base = slower rotation = captures long-range deps\n",
        "    rope_local_base: float = 10_000.0 # Base frequency for SLIDING WINDOW layers (local)\n",
        "                                       # Standard base = faster rotation = captures local patterns\n",
        "\n",
        "    # ── Sliding Window Attention ──\n",
        "    sliding_window: int = 512         # Window size for local attention layers\n",
        "                                       # Each token attends to only 512 nearest tokens\n",
        "                                       # Reduces O(n²) → O(n × w) where w=512\n",
        "                                       # For n=32,768: 64× cheaper than full attention!\n",
        "\n",
        "    # ── Layer Types (15 sliding + 3 full) ──\n",
        "    # Full attention at layers 6, 12, 18 (every 6th layer)\n",
        "    # This gives the model periodic \"global view\" of entire context\n",
        "    layer_types: tuple = (\n",
        "        \"sliding_attention\",   # Layer 1  ─┐\n",
        "        \"sliding_attention\",   # Layer 2   │\n",
        "        \"sliding_attention\",   # Layer 3   │ Local context\n",
        "        \"sliding_attention\",   # Layer 4   │ (512 tokens)\n",
        "        \"sliding_attention\",   # Layer 5  ─┘\n",
        "        \"full_attention\",      # Layer 6  ← GLOBAL (sees all 32K tokens)\n",
        "        \"sliding_attention\",   # Layer 7  ─┐\n",
        "        \"sliding_attention\",   # Layer 8   │\n",
        "        \"sliding_attention\",   # Layer 9   │ Local context\n",
        "        \"sliding_attention\",   # Layer 10  │\n",
        "        \"sliding_attention\",   # Layer 11 ─┘\n",
        "        \"full_attention\",      # Layer 12 ← GLOBAL\n",
        "        \"sliding_attention\",   # Layer 13 ─┐\n",
        "        \"sliding_attention\",   # Layer 14  │\n",
        "        \"sliding_attention\",   # Layer 15  │ Local context\n",
        "        \"sliding_attention\",   # Layer 16  │\n",
        "        \"sliding_attention\",   # Layer 17 ─┘\n",
        "        \"full_attention\",      # Layer 18 ← GLOBAL (final layer)\n",
        "    )\n",
        "\n",
        "    # ── Precision ──\n",
        "    dtype: torch.dtype = torch.bfloat16  # Training precision\n",
        "\n",
        "\n",
        "# ── Training Configuration ──\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"\n",
        "    Training hyperparameters for pre-training on TinyStories.\n",
        "\n",
        "    Learning Rate Schedule:\n",
        "        ┌────┐\n",
        "        │    ╲  cosine decay\n",
        "    LR  │     ╲───────────\n",
        "        │    ╱              ╲\n",
        "        │   ╱ warmup         ╲  min_lr\n",
        "        └──┴─────────────────────── Steps\n",
        "           0  1000            150K\n",
        "    \"\"\"\n",
        "    # ── Core ──\n",
        "    max_iters: int = 60_000           # Total training iterations\n",
        "    batch_size: int = 32              # Sequences per micro-batch\n",
        "    block_size: int = 128             # Context window per sequence (tokens)\n",
        "                                       # Each batch = 32 × 128 = 4,096 tokens\n",
        "\n",
        "    # ── Optimizer (AdamW) ──\n",
        "    learning_rate: float = 1e-4       # Peak learning rate\n",
        "    min_lr: float = 5e-5              # Minimum LR after cosine decay\n",
        "    weight_decay: float = 0.1         # L2 regularization\n",
        "    beta1: float = 0.9                # AdamW β₁ (momentum)\n",
        "    beta2: float = 0.95               # AdamW β₂ (RMS of gradients)\n",
        "    eps: float = 1e-9                 # AdamW epsilon\n",
        "\n",
        "    # ── Learning Rate Schedule ──\n",
        "    warmup_steps: int = 1_000         # Linear warmup from 0 → learning_rate\n",
        "    lr_decay_iters: int = 60_000      # Cosine decay over this many steps\n",
        "\n",
        "    # ── Gradient Accumulation ──\n",
        "    gradient_accumulation_steps: int = 32  # Effective batch = 32 × 32 = 1,024 sequences\n",
        "                                            # = 1,024 × 128 = 131,072 tokens per update\n",
        "    gradient_clip_norm: float = 0.5        # Max gradient norm (prevents explosions)\n",
        "\n",
        "    # ── Evaluation ──\n",
        "    eval_interval: int = 500          # Evaluate every N iterations\n",
        "    eval_iters: int = 50              # Number of batches for evaluation\n",
        "    save_every: int = 500             # Save checkpoint every N iterations\n",
        "\n",
        "    # ── Logging ──\n",
        "    log_interval: int = 10            # Print loss every N iterations\n",
        "\n",
        "\n",
        "# ── Instantiate configs ──\n",
        "model_config = Gemma3Config()\n",
        "train_config = TrainingConfig()\n",
        "\n",
        "# ── Verify ──\n",
        "print(\"🧠 GEMMA 3 270M — Model Configuration\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"  Vocab Size:        {model_config.vocab_size:,}\")\n",
        "print(f\"  Context Length:     {model_config.context_length:,}\")\n",
        "print(f\"  Embedding Dim:     {model_config.emb_dim}\")\n",
        "print(f\"  Layers:            {model_config.n_layers}\")\n",
        "print(f\"  Attention Heads:   {model_config.n_heads}\")\n",
        "print(f\"  Head Dim:          {model_config.head_dim}\")\n",
        "print(f\"  FFN Hidden Dim:    {model_config.hidden_dim}\")\n",
        "print(f\"  KV Groups (MQA):   {model_config.n_kv_groups}\")\n",
        "print(f\"  Sliding Window:    {model_config.sliding_window}\")\n",
        "print(f\"  RoPE Base (global):{model_config.rope_base:,.0f}\")\n",
        "print(f\"  RoPE Base (local): {model_config.rope_local_base:,.0f}\")\n",
        "print(f\"  Full Attn Layers:  [6, 12, 18]\")\n",
        "print(f\"  Sliding Layers:    15 of 18\")\n",
        "print(f\"  Dtype:             {model_config.dtype}\")\n",
        "\n",
        "print(f\"\\n⚡ TRAINING Configuration\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"  Max Iterations:    {train_config.max_iters:,}\")\n",
        "print(f\"  Batch Size:        {train_config.batch_size}\")\n",
        "print(f\"  Block Size:        {train_config.block_size}\")\n",
        "print(f\"  Grad Accum Steps:  {train_config.gradient_accumulation_steps}\")\n",
        "eff_batch = train_config.batch_size * train_config.gradient_accumulation_steps\n",
        "eff_tokens = eff_batch * train_config.block_size\n",
        "print(f\"  Effective Batch:   {eff_batch:,} sequences\")\n",
        "print(f\"  Tokens per Update: {eff_tokens:,}\")\n",
        "total_tokens = eff_tokens * train_config.max_iters\n",
        "print(f\"  Total Tokens Seen: {total_tokens:,.0f} (~{total_tokens/1e9:.1f}B)\")\n",
        "print(f\"  Learning Rate:     {train_config.learning_rate}\")\n",
        "print(f\"  Warmup Steps:      {train_config.warmup_steps:,}\")\n",
        "print(f\"  Gradient Clip:     {train_config.gradient_clip_norm}\")\n",
        "\n",
        "print(f\"\\n{'='*55}\")\n",
        "print(f\"  ✅ Cell 2 Done! Configs ready.\")\n",
        "print(f\"{'='*55}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgQO2DtH915p",
        "outputId": "de7c193c-95b8-4eef-f4a4-e6e822a2248f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 GEMMA 3 270M — Model Configuration\n",
            "=======================================================\n",
            "  Vocab Size:        50,257\n",
            "  Context Length:     32,768\n",
            "  Embedding Dim:     640\n",
            "  Layers:            18\n",
            "  Attention Heads:   4\n",
            "  Head Dim:          256\n",
            "  FFN Hidden Dim:    2048\n",
            "  KV Groups (MQA):   1\n",
            "  Sliding Window:    512\n",
            "  RoPE Base (global):1,000,000\n",
            "  RoPE Base (local): 10,000\n",
            "  Full Attn Layers:  [6, 12, 18]\n",
            "  Sliding Layers:    15 of 18\n",
            "  Dtype:             torch.bfloat16\n",
            "\n",
            "⚡ TRAINING Configuration\n",
            "=======================================================\n",
            "  Max Iterations:    60,000\n",
            "  Batch Size:        32\n",
            "  Block Size:        128\n",
            "  Grad Accum Steps:  32\n",
            "  Effective Batch:   1,024 sequences\n",
            "  Tokens per Update: 131,072\n",
            "  Total Tokens Seen: 7,864,320,000 (~7.9B)\n",
            "  Learning Rate:     0.0001\n",
            "  Warmup Steps:      1,000\n",
            "  Gradient Clip:     0.5\n",
            "\n",
            "=======================================================\n",
            "  ✅ Cell 2 Done! Configs ready.\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 3: Download TinyStories + Tokenize with GPT-2 BPE               ║\n",
        "# ║                                                                         ║\n",
        "# ║  Dataset: TinyStories by Microsoft Research (Ronen Eldan)               ║\n",
        "# ║  - 2+ million short stories (3-to-5 year old level)                     ║\n",
        "# ║  - ~2M train rows, ~22K validation rows                                ║\n",
        "# ║                                                                         ║\n",
        "# ║  Tokenizer: GPT-2 via tiktoken (Byte-Pair Encoding)                    ║\n",
        "# ║  - Vocab size: 50,257                                                   ║\n",
        "# ║  - Why not Gemma's 256K vocab? → 10x faster, saves 138M params         ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "# ── Step 1: Download TinyStories from HuggingFace ──\n",
        "print(\"📥 Downloading TinyStories dataset...\")\n",
        "print(\"   Source: https://huggingface.co/datasets/roneneldan/TinyStories\")\n",
        "print(\"   This may take 2-3 minutes...\\n\")\n",
        "\n",
        "ds = load_dataset(\"roneneldan/TinyStories\")\n",
        "\n",
        "n_train = len(ds[\"train\"])\n",
        "n_val = len(ds[\"validation\"])\n",
        "print(f\"✅ Dataset loaded!\")\n",
        "print(f\"   Train:      {n_train:,} stories\")\n",
        "print(f\"   Validation: {n_val:,} stories\")\n",
        "\n",
        "# ── Quick peek at the data ──\n",
        "print(f\"\\n📖 Sample story (first 300 chars):\")\n",
        "print(f\"   \\\"{ds['train'][0]['text'][:300]}...\\\"\")\n",
        "\n",
        "# ── Step 2: Initialize GPT-2 Tokenizer ──\n",
        "# tiktoken is OpenAI's fast BPE tokenizer (written in Rust → very fast)\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "print(f\"\\n🔤 Tokenizer: GPT-2 (BPE)\")\n",
        "print(f\"   Vocab size: {enc.n_vocab:,}\")\n",
        "\n",
        "# ── Demo: Show how tokenization works ──\n",
        "demo_text = \"Once upon a time, a little girl named Lily\"\n",
        "demo_tokens = enc.encode_ordinary(demo_text)\n",
        "print(f\"\\n   Example: \\\"{demo_text}\\\"\")\n",
        "print(f\"   Tokens:  {demo_tokens}\")\n",
        "print(f\"   Count:   {len(demo_tokens)} tokens\")\n",
        "print(f\"   Decoded: \\\"{enc.decode(demo_tokens)}\\\"\")\n",
        "\n",
        "# ── Step 3: Tokenize ENTIRE dataset ──\n",
        "# We tokenize all stories and flatten into one giant list of token IDs.\n",
        "# This is stored as a binary file for memory-efficient loading later.\n",
        "\n",
        "def tokenize_split(split_name):\n",
        "    \"\"\"\n",
        "    Tokenize all stories in a split → flat list of token IDs.\n",
        "\n",
        "    For 2M stories, this produces ~500M+ tokens.\n",
        "    We use encode_ordinary() which skips special tokens — just raw BPE.\n",
        "    \"\"\"\n",
        "    data = ds[split_name]\n",
        "    all_tokens = []\n",
        "    total_chars = 0\n",
        "\n",
        "    print(f\"\\n⏳ Tokenizing {split_name} ({len(data):,} stories)...\")\n",
        "\n",
        "    for i, example in enumerate(tqdm(data, desc=f\"   {split_name}\")):\n",
        "        text = example[\"text\"]\n",
        "        total_chars += len(text)\n",
        "        tokens = enc.encode_ordinary(text)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "    print(f\"   Total characters: {total_chars:,}\")\n",
        "    print(f\"   Total tokens:     {len(all_tokens):,}\")\n",
        "    print(f\"   Avg tokens/story: {len(all_tokens) // len(data):,}\")\n",
        "\n",
        "    return all_tokens\n",
        "\n",
        "train_tokens = tokenize_split(\"train\")\n",
        "val_tokens = tokenize_split(\"validation\")\n",
        "\n",
        "# ── Step 4: Save as memory-mapped binary files ──\n",
        "# Why .bin files?\n",
        "#   - np.memmap reads directly from disk → no RAM overhead\n",
        "#   - Can handle datasets larger than RAM\n",
        "#   - Lightning-fast random access for batching\n",
        "#   - dtype=uint16 because vocab_size=50,257 < 65,535 (max uint16)\n",
        "\n",
        "def save_tokens_to_bin(tokens, filepath):\n",
        "    \"\"\"Save token IDs as uint16 binary file using np.memmap.\"\"\"\n",
        "    tokens_np = np.array(tokens, dtype=np.uint16)\n",
        "    filepath = str(filepath)\n",
        "\n",
        "    # Create memory-mapped file and write tokens\n",
        "    arr = np.memmap(filepath, dtype=np.uint16, mode='w+', shape=(len(tokens_np),))\n",
        "    arr[:] = tokens_np[:]\n",
        "    arr.flush()\n",
        "    del arr  # Close the memmap\n",
        "\n",
        "    file_size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "    print(f\"   💾 Saved: {filepath}\")\n",
        "    print(f\"      Tokens: {len(tokens_np):,}\")\n",
        "    print(f\"      Size:   {file_size_mb:.1f} MB\")\n",
        "\n",
        "print(f\"\\n📦 Saving tokenized data as .bin files...\")\n",
        "save_tokens_to_bin(train_tokens, PROCESSED_DIR / \"train.bin\")\n",
        "save_tokens_to_bin(val_tokens, PROCESSED_DIR / \"val.bin\")\n",
        "\n",
        "# ── Verify files ──\n",
        "train_data = np.memmap(str(PROCESSED_DIR / \"train.bin\"), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(str(PROCESSED_DIR / \"val.bin\"), dtype=np.uint16, mode='r')\n",
        "\n",
        "print(f\"\\n✅ Verification:\")\n",
        "print(f\"   train.bin → {len(train_data):,} tokens\")\n",
        "print(f\"   val.bin   → {len(val_data):,} tokens\")\n",
        "print(f\"   First 10 train tokens: {list(train_data[:10])}\")\n",
        "print(f\"   Decoded: \\\"{enc.decode(list(train_data[:10]))}\\\"\")\n",
        "\n",
        "# ── Cleanup RAM ──\n",
        "del train_tokens, val_tokens\n",
        "import gc; gc.collect()\n",
        "\n",
        "print(f\"\\n{'='*55}\")\n",
        "print(f\"  ✅ Cell 3 Done! Data tokenized & saved.\")\n",
        "print(f\"{'='*55}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icgAuXsm-jFN",
        "outputId": "56e2b928-62e1-4a84-f37c-6c8ae8a93a6c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Downloading TinyStories dataset...\n",
            "   Source: https://huggingface.co/datasets/roneneldan/TinyStories\n",
            "   This may take 2-3 minutes...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset loaded!\n",
            "   Train:      2,119,719 stories\n",
            "   Validation: 21,990 stories\n",
            "\n",
            "📖 Sample story (first 300 chars):\n",
            "   \"One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
            "\n",
            "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and ...\"\n",
            "\n",
            "🔤 Tokenizer: GPT-2 (BPE)\n",
            "   Vocab size: 50,257\n",
            "\n",
            "   Example: \"Once upon a time, a little girl named Lily\"\n",
            "   Tokens:  [7454, 2402, 257, 640, 11, 257, 1310, 2576, 3706, 20037]\n",
            "   Count:   10 tokens\n",
            "   Decoded: \"Once upon a time, a little girl named Lily\"\n",
            "\n",
            "⏳ Tokenizing train (2,119,719 stories)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "   train: 100%|██████████| 2119719/2119719 [04:33<00:00, 7749.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Total characters: 1,899,973,203\n",
            "   Total tokens:     471,872,517\n",
            "   Avg tokens/story: 222\n",
            "\n",
            "⏳ Tokenizing validation (21,990 stories)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "   validation: 100%|██████████| 21990/21990 [00:02<00:00, 7869.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Total characters: 19,190,318\n",
            "   Total tokens:     4,743,928\n",
            "   Avg tokens/story: 215\n",
            "\n",
            "📦 Saving tokenized data as .bin files...\n",
            "   💾 Saved: /content/data/processed/train.bin\n",
            "      Tokens: 471,872,517\n",
            "      Size:   900.0 MB\n",
            "   💾 Saved: /content/data/processed/val.bin\n",
            "      Tokens: 4,743,928\n",
            "      Size:   9.0 MB\n",
            "\n",
            "✅ Verification:\n",
            "   train.bin → 471,872,517 tokens\n",
            "   val.bin   → 4,743,928 tokens\n",
            "   First 10 train tokens: [np.uint16(3198), np.uint16(1110), np.uint16(11), np.uint16(257), np.uint16(1310), np.uint16(2576), np.uint16(3706), np.uint16(20037), np.uint16(1043), np.uint16(257)]\n",
            "   Decoded: \"One day, a little girl named Lily found a\"\n",
            "\n",
            "=======================================================\n",
            "  ✅ Cell 3 Done! Data tokenized & saved.\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 4: DataLoader — Creating Input-Output Pairs & Batches            ║\n",
        "# ║                                                                         ║\n",
        "# ║  From your notes (page 3):                                              ║\n",
        "# ║  \"One day, a little girl named Lily found a needle in her...\"           ║\n",
        "# ║                                                                         ║\n",
        "# ║  Input:  [1, 11, 15, 24]    → \"one day a little\"                       ║\n",
        "# ║  Output: [11, 15, 24, 63]   → \"day a little girl\"                      ║\n",
        "# ║                                                                         ║\n",
        "# ║  The output is the input shifted RIGHT by 1 token.                      ║\n",
        "# ║  Each position is a next-token prediction task.                         ║\n",
        "# ║  4 tokens = 4 prediction tasks (= context_size).                        ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "# ── Load memory-mapped binary files ──\n",
        "# np.memmap reads directly from disk — no RAM spike even for 900MB file!\n",
        "train_data = np.memmap(str(PROCESSED_DIR / \"train.bin\"), dtype=np.uint16, mode='r')\n",
        "val_data   = np.memmap(str(PROCESSED_DIR / \"val.bin\"),   dtype=np.uint16, mode='r')\n",
        "\n",
        "print(f\"📂 Loaded memory-mapped data:\")\n",
        "print(f\"   train.bin → {len(train_data):,} tokens ({len(train_data)*2/1e6:.0f} MB on disk)\")\n",
        "print(f\"   val.bin   → {len(val_data):,} tokens ({len(val_data)*2/1e6:.0f} MB on disk)\")\n",
        "\n",
        "\n",
        "def get_batch(split: str):\n",
        "    \"\"\"\n",
        "    Generate one random batch of input-output pairs.\n",
        "\n",
        "    How it works (from your notes, page 3):\n",
        "    ─────────────────────────────────────────────────────────\n",
        "    1. Pick `batch_size` random starting positions in the data\n",
        "    2. For each position, grab `block_size + 1` consecutive tokens\n",
        "    3. Input  x = tokens[:-1]   → first block_size tokens\n",
        "       Target y = tokens[1:]    → shifted right by 1\n",
        "\n",
        "    Example (block_size=4):\n",
        "        tokens = [1, 11, 15, 24, 63]\n",
        "        x = [1, 11, 15, 24]     ← input\n",
        "        y = [11, 15, 24, 63]    ← target (shifted by 1)\n",
        "\n",
        "        Prediction tasks:\n",
        "        #1: [1]              → predict 11  (\"one\" → \"day\")\n",
        "        #2: [1, 11]          → predict 15  (\"one day\" → \"a\")\n",
        "        #3: [1, 11, 15]      → predict 24  (\"one day a\" → \"little\")\n",
        "        #4: [1, 11, 15, 24]  → predict 63  (\"one day a little\" → \"girl\")\n",
        "    ─────────────────────────────────────────────────────────\n",
        "\n",
        "    Returns:\n",
        "        x: (batch_size, block_size) — input token IDs\n",
        "        y: (batch_size, block_size) — target token IDs (x shifted right by 1)\n",
        "    \"\"\"\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    block_size = train_config.block_size\n",
        "\n",
        "    # Random starting indices (ensure we don't go past the end)\n",
        "    ix = torch.randint(len(data) - block_size - 1, (train_config.batch_size,))\n",
        "\n",
        "    # Grab sequences and split into input/target\n",
        "    x = torch.stack([torch.from_numpy(data[i   : i + block_size].astype(np.int64))     for i in ix])\n",
        "    y = torch.stack([torch.from_numpy(data[i+1 : i + block_size + 1].astype(np.int64)) for i in ix])\n",
        "\n",
        "    # ── Move to GPU with pin_memory optimization ──\n",
        "    # From your notes (page 4):\n",
        "    # pin_memory() locks tensor in RAM → faster CPU→GPU transfer\n",
        "    # non_blocking=True → CPU doesn't wait for GPU copy to finish\n",
        "    #                   → CPU can prepare next batch while GPU trains\n",
        "    if device_type == \"cuda\":\n",
        "        x = x.pin_memory().to(device, non_blocking=True)\n",
        "        y = y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# ── Test the batch function ──\n",
        "print(f\"\\n🧪 Testing get_batch()...\")\n",
        "x_test, y_test = get_batch(\"train\")\n",
        "\n",
        "print(f\"\\n   Batch shapes:\")\n",
        "print(f\"   x (input):  {x_test.shape}  → (batch_size={train_config.batch_size}, block_size={train_config.block_size})\")\n",
        "print(f\"   y (target): {y_test.shape}  → same shape, shifted right by 1\")\n",
        "print(f\"   Device:     {x_test.device}\")\n",
        "print(f\"   Dtype:      {x_test.dtype}\")\n",
        "\n",
        "# ── Show one input-output pair ──\n",
        "print(f\"\\n📖 Example input-output pair (first sequence, first 8 tokens):\")\n",
        "x_sample = x_test[0, :8].tolist()\n",
        "y_sample = y_test[0, :8].tolist()\n",
        "print(f\"   Input  IDs:  {x_sample}\")\n",
        "print(f\"   Target IDs:  {y_sample}\")\n",
        "print(f\"   Input  text:  \\\"{enc.decode(x_sample)}\\\"\")\n",
        "print(f\"   Target text:  \\\"{enc.decode(y_sample)}\\\"\")\n",
        "\n",
        "print(f\"\\n   Notice: target is input shifted RIGHT by 1 token ✓\")\n",
        "print(f\"   Each position = 1 next-token prediction task\")\n",
        "print(f\"   {train_config.block_size} tokens × {train_config.batch_size} batch = {train_config.block_size * train_config.batch_size:,} predictions per batch\")\n",
        "\n",
        "# ── Clean up test batch ──\n",
        "del x_test, y_test\n",
        "\n",
        "print(f\"\\n{'='*55}\")\n",
        "print(f\"  ✅ Cell 4 Done! DataLoader ready.\")\n",
        "print(f\"{'='*55}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9yIhFdl-x0K",
        "outputId": "dff78abd-6a99-408a-99e6-e398ff05ca0e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Loaded memory-mapped data:\n",
            "   train.bin → 471,872,517 tokens (944 MB on disk)\n",
            "   val.bin   → 4,743,928 tokens (9 MB on disk)\n",
            "\n",
            "🧪 Testing get_batch()...\n",
            "\n",
            "   Batch shapes:\n",
            "   x (input):  torch.Size([32, 128])  → (batch_size=32, block_size=128)\n",
            "   y (target): torch.Size([32, 128])  → same shape, shifted right by 1\n",
            "   Device:     cuda:0\n",
            "   Dtype:      torch.int64\n",
            "\n",
            "📖 Example input-output pair (first sequence, first 8 tokens):\n",
            "   Input  IDs:  [340, 790, 1110, 13, 198, 198, 3198, 1110]\n",
            "   Target IDs:  [790, 1110, 13, 198, 198, 3198, 1110, 11]\n",
            "   Input  text:  \" it every day.\n",
            "\n",
            "One day\"\n",
            "   Target text:  \" every day.\n",
            "\n",
            "One day,\"\n",
            "\n",
            "   Notice: target is input shifted RIGHT by 1 token ✓\n",
            "   Each position = 1 next-token prediction task\n",
            "   128 tokens × 32 batch = 4,096 predictions per batch\n",
            "\n",
            "=======================================================\n",
            "  ✅ Cell 4 Done! DataLoader ready.\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 5: RMS Normalization (Root Mean Square Normalization)             ║\n",
        "# ║                                                                         ║\n",
        "# ║  From your notes (pages 6-7):                                           ║\n",
        "# ║                                                                         ║\n",
        "# ║  Why RMSNorm instead of LayerNorm?                                      ║\n",
        "# ║  - Simpler: no mean subtraction, no shift/bias needed                   ║\n",
        "# ║  - Faster: fewer operations                                             ║\n",
        "# ║  - Used in Gemma3, LLaMA, Mistral, etc.                                ║\n",
        "# ║                                                                         ║\n",
        "# ║  Steps (from your notes page 6):                                        ║\n",
        "# ║  ┌───────────────────────────────────────────────────────┐              ║\n",
        "# ║  │ Step 1: var(x) = (1/d) × Σ xᵢ²     (mean of squares)│              ║\n",
        "# ║  │ Step 2: rms(x) = √(var(x) + ε)      (normalization)  │              ║\n",
        "# ║  │ Step 3: x̂ᵢ = xᵢ / rms(x)           (normalize)      │              ║\n",
        "# ║  │ Step 4: yᵢ = x̂ᵢ × (1 + scale)      (Gemma3 style!)  │              ║\n",
        "# ║  │ Step 5: yᵢ = yᵢ + shift             (optional bias)  │              ║\n",
        "# ║  └───────────────────────────────────────────────────────┘              ║\n",
        "# ║                                                                         ║\n",
        "# ║  Gemma3 special: scaling uses (1 + weight) instead of just weight.      ║\n",
        "# ║  This means weights initialize around 1.0, not 0.0 → more stable.      ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Root Mean Square Layer Normalization (Gemma3-style).\n",
        "\n",
        "    Standard LayerNorm:  y = (x - mean) / std * γ + β\n",
        "    RMSNorm:             y = x / rms(x) * (1 + γ)\n",
        "\n",
        "    Key difference from standard RMSNorm:\n",
        "    - Gemma3 uses (1 + weight) scaling instead of just weight\n",
        "    - Weights initialized to zeros → (1 + 0) = 1 at start\n",
        "    - This is more stable: identity transform at initialization\n",
        "\n",
        "    Numerical example from your notes (page 7):\n",
        "    ─────────────────────────────────────────────\n",
        "    x = [2, -6], d = 2\n",
        "    scale = [0.5, -0.2], shift = [1.0, -1.0]\n",
        "\n",
        "    Step 1: var(x) = (1/2)(2² + (-6)²) = (4 + 36)/2 = 20\n",
        "    Step 2: rms(x) = √(20 + ε) ≈ 4.472\n",
        "    Step 3: x̂ = [2/4.472, -6/4.472] = [0.447, -1.342]\n",
        "    Step 4: y = x̂ ⊙ (1 + scale) = [0.447×1.5, -1.342×0.8] = [0.671, -1.074]\n",
        "    Step 5: y = y + shift = [0.671+1.0, -1.074-1.0] = [1.671, -2.074]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emb_dim: int, eps: float = 1e-6):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            emb_dim: Dimension of input vectors (640 for Gemma3 270M)\n",
        "            eps:     Small constant to prevent division by zero\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        # Learnable scale parameter — initialized to zeros\n",
        "        # So (1 + weight) = 1.0 at start → identity scaling\n",
        "        self.weight = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (..., emb_dim)\n",
        "\n",
        "        Returns:\n",
        "            Normalized tensor of same shape\n",
        "        \"\"\"\n",
        "        # Step 1 & 2: Compute RMS\n",
        "        # x.float() → upcast to float32 for numerical stability\n",
        "        # (important when x is bfloat16!)\n",
        "        rms = torch.sqrt(x.float().pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
        "\n",
        "        # Step 3: Normalize\n",
        "        x_normed = x.float() / rms\n",
        "\n",
        "        # Step 4: Gemma3-style scaling → (1 + weight) instead of weight\n",
        "        # Cast back to original dtype (bfloat16)\n",
        "        return (x_normed * (1.0 + self.weight.float())).to(x.dtype)\n",
        "\n",
        "\n",
        "# ── Test RMSNorm with the exact example from your notes ──\n",
        "print(\"🧪 Testing RMSNorm with example from notes (page 7):\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Create a test instance with dim=2\n",
        "test_norm = RMSNorm(emb_dim=2)\n",
        "\n",
        "# Manually set weights to match notes example\n",
        "with torch.no_grad():\n",
        "    test_norm.weight.copy_(torch.tensor([0.5, -0.2]))\n",
        "\n",
        "x_test = torch.tensor([[2.0, -6.0]])\n",
        "\n",
        "print(f\"\\n   Input x:     {x_test.tolist()[0]}\")\n",
        "print(f\"   Scale:       {test_norm.weight.tolist()}\")\n",
        "\n",
        "# Step-by-step computation\n",
        "var_x = x_test.pow(2).mean(dim=-1)\n",
        "rms_x = torch.sqrt(var_x + 1e-6)\n",
        "x_norm = x_test / rms_x.unsqueeze(-1)\n",
        "y = x_norm * (1 + test_norm.weight)\n",
        "\n",
        "print(f\"\\n   Step 1: var(x) = {var_x.item():.1f}\")\n",
        "print(f\"   Step 2: rms(x) = {rms_x.item():.3f}\")\n",
        "print(f\"   Step 3: x̂ = [{x_norm[0,0].item():.3f}, {x_norm[0,1].item():.3f}]\")\n",
        "print(f\"   Step 4: y = x̂ ⊙ (1+scale) = [{y[0,0].item():.3f}, {y[0,1].item():.3f}]\")\n",
        "\n",
        "# Verify with actual forward pass\n",
        "y_actual = test_norm(x_test)\n",
        "print(f\"\\n   Forward pass output: [{y_actual[0,0].item():.3f}, {y_actual[0,1].item():.3f}]\")\n",
        "print(f\"   ✓ Matches manual computation!\")\n",
        "\n",
        "# ── Test with actual model dimensions ──\n",
        "print(f\"\\n\\n🔧 Testing with Gemma3 dimensions:\")\n",
        "print(\"=\" * 55)\n",
        "norm = RMSNorm(emb_dim=model_config.emb_dim)\n",
        "x_real = torch.randn(2, 10, model_config.emb_dim)  # (batch=2, seq=10, dim=640)\n",
        "y_real = norm(x_real)\n",
        "\n",
        "print(f\"   Input shape:  {x_real.shape}  (batch=2, seq_len=10, emb_dim=640)\")\n",
        "print(f\"   Output shape: {y_real.shape}  (same shape ✓)\")\n",
        "print(f\"   Input mean:   {x_real.mean().item():.4f}\")\n",
        "print(f\"   Output RMS:   {y_real.float().pow(2).mean(dim=-1).sqrt().mean().item():.4f}  (≈1.0 ✓)\")\n",
        "print(f\"   Parameters:   {sum(p.numel() for p in norm.parameters()):,} (just the scale vector)\")\n",
        "\n",
        "del test_norm, norm, x_test, x_real, y_real\n",
        "\n",
        "print(f\"\\n{'='*55}\")\n",
        "print(f\"  ✅ Cell 5 Done! RMSNorm ready.\")\n",
        "print(f\"{'='*55}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6fTw8LFATsr",
        "outputId": "86ce4316-c0df-4fba-8a70-1128ab348112"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Testing RMSNorm with example from notes (page 7):\n",
            "=======================================================\n",
            "\n",
            "   Input x:     [2.0, -6.0]\n",
            "   Scale:       [0.5, -0.20000000298023224]\n",
            "\n",
            "   Step 1: var(x) = 20.0\n",
            "   Step 2: rms(x) = 4.472\n",
            "   Step 3: x̂ = [0.447, -1.342]\n",
            "   Step 4: y = x̂ ⊙ (1+scale) = [0.671, -1.073]\n",
            "\n",
            "   Forward pass output: [0.671, -1.073]\n",
            "   ✓ Matches manual computation!\n",
            "\n",
            "\n",
            "🔧 Testing with Gemma3 dimensions:\n",
            "=======================================================\n",
            "   Input shape:  torch.Size([2, 10, 640])  (batch=2, seq_len=10, emb_dim=640)\n",
            "   Output shape: torch.Size([2, 10, 640])  (same shape ✓)\n",
            "   Input mean:   0.0085\n",
            "   Output RMS:   1.0000  (≈1.0 ✓)\n",
            "   Parameters:   640 (just the scale vector)\n",
            "\n",
            "=======================================================\n",
            "  ✅ Cell 5 Done! RMSNorm ready.\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 6: Rotary Positional Encoding (RoPE)                             ║\n",
        "# ║                                                                         ║\n",
        "# ║  From your notes (pages 12-13):                                         ║\n",
        "# ║                                                                         ║\n",
        "# ║  Problem: How does the model know word ORDER?                           ║\n",
        "# ║  \"The dog chased another dog\" — which \"dog\" is which?                   ║\n",
        "# ║                                                                         ║\n",
        "# ║  Old approach: Add position vectors to embeddings                       ║\n",
        "# ║  → But this POLLUTES the semantic meaning!                              ║\n",
        "# ║                                                                         ║\n",
        "# ║  RoPE approach: ROTATE query & key vectors instead                      ║\n",
        "# ║  → Magnitude stays same (preserves meaning)                             ║\n",
        "# ║  → Angle changes (encodes position)                                     ║\n",
        "# ║  → No vector added! Just rotation.                                      ║\n",
        "# ║                                                                         ║\n",
        "# ║  From your notes (page 13):                                             ║\n",
        "# ║  ┌                    ┐   ┌         ┐                                   ║\n",
        "# ║  │ x₁'│     ┌cos(θ)  sin(θ)┐ │ x₁ │                                   ║\n",
        "# ║  │    │  =  │               │ │    │   ← 2D Rotation Matrix             ║\n",
        "# ║  │ x₂'│     └-sin(θ) cos(θ)┘ │ x₂ │                                   ║\n",
        "# ║  └                    ┘   └         ┘                                   ║\n",
        "# ║                                                                         ║\n",
        "# ║  θ = ω × position,  ω = 1 / (base^(2i/d))                             ║\n",
        "# ║                                                                         ║\n",
        "# ║  Gemma3 uses DUAL bases:                                                ║\n",
        "# ║  - Local (sliding layers):  base = 10,000   → fast rotation             ║\n",
        "# ║  - Global (full attn layers): base = 1,000,000 → slow rotation          ║\n",
        "# ║                                                                         ║\n",
        "# ║  Why? (from your notes page 14):                                        ║\n",
        "# ║  - Lower index = fast oscillation → captures small position shifts      ║\n",
        "# ║  - Higher index = slow oscillation → captures long-range dependencies   ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "def precompute_rope_frequencies(\n",
        "    head_dim: int,\n",
        "    max_seq_len: int,\n",
        "    rope_base: float = 10_000.0,\n",
        "    device: torch.device = None\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Precompute the complex exponential frequencies for RoPE.\n",
        "\n",
        "    From your notes (page 13):\n",
        "    ω_i = 1 / (base^(2i/d))\n",
        "\n",
        "    For each position p and frequency index i:\n",
        "    θ = ω_i × p\n",
        "\n",
        "    We precompute all (position, frequency) pairs as complex numbers:\n",
        "    e^(iθ) = cos(θ) + i·sin(θ)\n",
        "\n",
        "    This is stored as a (max_seq_len, head_dim/2) complex tensor.\n",
        "\n",
        "    Args:\n",
        "        head_dim:    Dimension per attention head (256 for Gemma3)\n",
        "        max_seq_len: Maximum sequence length to precompute (32,768)\n",
        "        rope_base:   Base frequency (10K for local, 1M for global)\n",
        "\n",
        "    Returns:\n",
        "        freqs_cis: Complex tensor of shape (max_seq_len, head_dim // 2)\n",
        "    \"\"\"\n",
        "    assert head_dim % 2 == 0, \"head_dim must be even for RoPE (we pair dimensions)\"\n",
        "\n",
        "    # Step 1: Compute frequency for each dimension pair\n",
        "    # ω_i = 1 / base^(2i/d) for i = 0, 1, ..., d/2 - 1\n",
        "    # From your notes: ω_i = 1 / 10000^(2i/d)\n",
        "    i = torch.arange(0, head_dim, 2, device=device).float()  # [0, 2, 4, ..., d-2]\n",
        "    freqs = 1.0 / (rope_base ** (i / head_dim))              # shape: (head_dim/2,)\n",
        "\n",
        "    # Step 2: Compute angles for each position\n",
        "    # θ(pos, i) = pos × ω_i\n",
        "    positions = torch.arange(max_seq_len, device=device).float()  # [0, 1, 2, ..., L-1]\n",
        "    angles = torch.outer(positions, freqs)  # shape: (max_seq_len, head_dim/2)\n",
        "\n",
        "    # Step 3: Convert to complex form → e^(iθ) = cos(θ) + i·sin(θ)\n",
        "    # This is the rotation we'll apply to query and key vectors\n",
        "    freqs_cis = torch.polar(torch.ones_like(angles), angles)  # shape: (max_seq_len, head_dim/2)\n",
        "\n",
        "    return freqs_cis\n",
        "\n",
        "\n",
        "def apply_rope(\n",
        "    x: torch.Tensor,\n",
        "    freqs_cis: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Apply Rotary Positional Encoding to query or key vectors.\n",
        "\n",
        "    From your notes (page 13):\n",
        "    ─────────────────────────────────────────────────────\n",
        "    Take pairs of dimensions (x₁, x₂), (x₃, x₄), ...\n",
        "    Treat each pair as a 2D vector.\n",
        "    Rotate each pair by angle θ = ω × position.\n",
        "\n",
        "    [x₁']   [cos(θ)  -sin(θ)] [x₁]\n",
        "    [x₂'] = [sin(θ)   cos(θ)] [x₂]\n",
        "\n",
        "    \"We effectively rotate parts of the original query vector\n",
        "     and hence maintaining the magnitude, thus avoiding the\n",
        "     vector polluting issue which we saw with sinusoidal encodings.\"\n",
        "    ─────────────────────────────────────────────────────\n",
        "\n",
        "    Instead of matrix multiplication, we use complex number multiplication:\n",
        "    (x₁ + ix₂) × (cos θ + i sin θ) = (x₁cosθ - x₂sinθ) + i(x₁sinθ + x₂cosθ)\n",
        "\n",
        "    This is mathematically equivalent to the 2D rotation matrix!\n",
        "\n",
        "    Args:\n",
        "        x:         (batch, seq_len, n_heads, head_dim) — Q or K vectors\n",
        "        freqs_cis: (seq_len, head_dim//2) — precomputed complex frequencies\n",
        "\n",
        "    Returns:\n",
        "        Rotated tensor of same shape as x\n",
        "    \"\"\"\n",
        "    # Reshape x: pair consecutive dimensions → complex numbers\n",
        "    # (batch, seq, heads, dim) → (batch, seq, heads, dim/2, 2) → complex\n",
        "    x_complex = torch.view_as_complex(\n",
        "        x.float().reshape(*x.shape[:-1], -1, 2)\n",
        "    )\n",
        "    # x_complex shape: (batch, seq_len, n_heads, head_dim/2)\n",
        "\n",
        "    # Reshape freqs to broadcast: (seq_len, head_dim/2) → (1, seq_len, 1, head_dim/2)\n",
        "    freqs_cis = freqs_cis.unsqueeze(0).unsqueeze(2)\n",
        "\n",
        "    # Apply rotation via complex multiplication\n",
        "    x_rotated = x_complex * freqs_cis\n",
        "\n",
        "    # Convert back to real: complex → (batch, seq, heads, dim/2, 2) → (batch, seq, heads, dim)\n",
        "    x_out = torch.view_as_real(x_rotated).reshape(*x.shape)\n",
        "\n",
        "    return x_out.to(x.dtype)\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 🧪 TEST: Verify RoPE implementation\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"🌀 Testing Rotary Positional Encoding (RoPE)\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# ── Test 1: Precompute frequencies for both bases ──\n",
        "freqs_local  = precompute_rope_frequencies(\n",
        "    head_dim=model_config.head_dim,\n",
        "    max_seq_len=model_config.context_length,\n",
        "    rope_base=model_config.rope_local_base,   # 10,000 for sliding layers\n",
        "    device=\"cpu\"\n",
        ")\n",
        "freqs_global = precompute_rope_frequencies(\n",
        "    head_dim=model_config.head_dim,\n",
        "    max_seq_len=model_config.context_length,\n",
        "    rope_base=model_config.rope_base,          # 1,000,000 for full attention\n",
        "    device=\"cpu\"\n",
        ")\n",
        "\n",
        "print(f\"\\n   Local RoPE  (base=10,000):    shape={freqs_local.shape}\")\n",
        "print(f\"   Global RoPE (base=1,000,000): shape={freqs_global.shape}\")\n",
        "print(f\"   Each: ({model_config.context_length} positions × {model_config.head_dim//2} freq pairs)\")\n",
        "\n",
        "# ── Test 2: Verify rotation preserves magnitude ──\n",
        "print(f\"\\n🔍 Magnitude preservation test:\")\n",
        "batch, seq, heads, dim = 2, 16, 4, model_config.head_dim\n",
        "x_test = torch.randn(batch, seq, heads, dim)\n",
        "\n",
        "freqs_test = precompute_rope_frequencies(dim, seq, rope_base=10000.0)\n",
        "x_rotated = apply_rope(x_test, freqs_test)\n",
        "\n",
        "mag_before = x_test.float().norm(dim=-1).mean().item()\n",
        "mag_after  = x_rotated.float().norm(dim=-1).mean().item()\n",
        "print(f\"   Before rotation: magnitude = {mag_before:.4f}\")\n",
        "print(f\"   After rotation:  magnitude = {mag_after:.4f}\")\n",
        "print(f\"   Difference:      {abs(mag_before - mag_after):.6f}  (≈0 ✓)\")\n",
        "\n",
        "# ── Test 3: Show rotation angles for position 0 vs position 100 ──\n",
        "print(f\"\\n🔍 Frequency analysis (from your notes page 14):\")\n",
        "print(f\"   Lower indices → FAST oscillation → captures small shifts\")\n",
        "print(f\"   Higher indices → SLOW oscillation → captures long-range deps\")\n",
        "\n",
        "freqs_demo = precompute_rope_frequencies(256, 200, rope_base=10000.0)\n",
        "angles_pos0   = freqs_demo[0].angle()    # Angles at position 0\n",
        "angles_pos100 = freqs_demo[100].angle()  # Angles at position 100\n",
        "rotation = (angles_pos100 - angles_pos0)\n",
        "\n",
        "print(f\"\\n   Rotation from position 0 → 100:\")\n",
        "print(f\"   Dim pair 0 (lowest):   {rotation[0].item():.4f} rad  ← FAST (local patterns)\")\n",
        "print(f\"   Dim pair 63 (middle):  {rotation[63].item():.6f} rad\")\n",
        "print(f\"   Dim pair 127 (highest):{rotation[127].item():.8f} rad  ← SLOW (long-range)\")\n",
        "\n",
        "# ── Test 4: Verify dual base difference ──\n",
        "print(f\"\\n🔍 Dual base comparison (Gemma3 innovation):\")\n",
        "freqs_10k = precompute_rope_frequencies(256, 200, rope_base=10_000.0)\n",
        "freqs_1m  = precompute_rope_frequencies(256, 200, rope_base=1_000_000.0)\n",
        "\n",
        "rot_10k = (freqs_10k[100].angle() - freqs_10k[0].angle())[0].item()\n",
        "rot_1m  = (freqs_1m[100].angle()  - freqs_1m[0].angle())[0].item()\n",
        "\n",
        "print(f\"   Local  base (10K):   rotation at dim 0 = {rot_10k:.4f} rad  (fast)\")\n",
        "print(f\"   Global base (1M):    rotation at dim 0 = {rot_1m:.4f} rad  (slower)\")\n",
        "print(f\"   Ratio: {rot_10k/rot_1m:.1f}× faster rotation with local base\")\n",
        "print(f\"   → Local base = nearby tokens matter more (sliding window)\")\n",
        "print(f\"   → Global base = distant tokens still distinguishable (full attn)\")\n",
        "\n",
        "del x_test, x_rotated, freqs_test, freqs_demo, freqs_10k, freqs_1m\n",
        "\n",
        "print(f\"\\n{'='*55}\")\n",
        "print(f\"  ✅ Cell 6 Done! RoPE ready.\")\n",
        "print(f\"{'='*55}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvnggc6AAm-y",
        "outputId": "4c1e965e-446a-4da0-baf5-58464f631e6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌀 Testing Rotary Positional Encoding (RoPE)\n",
            "=======================================================\n",
            "\n",
            "   Local RoPE  (base=10,000):    shape=torch.Size([32768, 128])\n",
            "   Global RoPE (base=1,000,000): shape=torch.Size([32768, 128])\n",
            "   Each: (32768 positions × 128 freq pairs)\n",
            "\n",
            "🔍 Magnitude preservation test:\n",
            "   Before rotation: magnitude = 16.1595\n",
            "   After rotation:  magnitude = 16.1595\n",
            "   Difference:      0.000002  (≈0 ✓)\n",
            "\n",
            "🔍 Frequency analysis (from your notes page 14):\n",
            "   Lower indices → FAST oscillation → captures small shifts\n",
            "   Higher indices → SLOW oscillation → captures long-range deps\n",
            "\n",
            "   Rotation from position 0 → 100:\n",
            "   Dim pair 0 (lowest):   -0.5310 rad  ← FAST (local patterns)\n",
            "   Dim pair 63 (middle):  1.074608 rad\n",
            "   Dim pair 127 (highest):0.01074608 rad  ← SLOW (long-range)\n",
            "\n",
            "🔍 Dual base comparison (Gemma3 innovation):\n",
            "   Local  base (10K):   rotation at dim 0 = -0.5310 rad  (fast)\n",
            "   Global base (1M):    rotation at dim 0 = -0.5310 rad  (slower)\n",
            "   Ratio: 1.0× faster rotation with local base\n",
            "   → Local base = nearby tokens matter more (sliding window)\n",
            "   → Global base = distant tokens still distinguishable (full attn)\n",
            "\n",
            "=======================================================\n",
            "  ✅ Cell 6 Done! RoPE ready.\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 7: Multi-Query Attention + Sliding Window Attention               ║\n",
        "# ║                                                                         ║\n",
        "# ║  From your notes (pages 8-11):                                          ║\n",
        "# ║                                                                         ║\n",
        "# ║  (a) Input Embedding Matrix (11×8): words don't know their neighbors    ║\n",
        "# ║  (b) Multiply with Wq, Wk, Wv → Query, Key, Value vectors              ║\n",
        "# ║  (c) Multiple heads: each head gets d_out/n_heads dimensions            ║\n",
        "# ║  (d) Attention scores: Q × K^T for each head                            ║\n",
        "# ║  (e) Attention weights: scaling + softmax + causal mask + dropout        ║\n",
        "# ║  (f) Context vectors: weights × V                                       ║\n",
        "# ║  (g) Merge heads: concatenate → final context matrix                    ║\n",
        "# ║                                                                         ║\n",
        "# ║  Multi-Query Attention (MQA) — page 10:                                 ║\n",
        "# ║  \"To save computational cost, same content is shared across heads\"      ║\n",
        "# ║  → All query heads share 1 key head and 1 value head                    ║\n",
        "# ║  → Reduces KV cache size and trainable parameters                       ║\n",
        "# ║                                                                         ║\n",
        "# ║  Sliding Window — page 11:                                              ║\n",
        "# ║  \"Cannot peek into future AND not too far back into past\"               ║\n",
        "# ║  → Each token only attends to w=512 nearest tokens                      ║\n",
        "# ║  → For n=32,768: 64× cheaper than full attention                        ║\n",
        "# ║                                                                         ║\n",
        "# ║  Causal Mask — page 11:                                                 ║\n",
        "# ║  \"Cannot peek into future\" — lower triangular mask                      ║\n",
        "# ║  Sliding Window mask combines causal + window constraint                ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "class MultiQueryAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Query Attention with optional Sliding Window.\n",
        "\n",
        "    From your notes (page 8):\n",
        "    ─────────────────────────────────────────────────────\n",
        "    X (11,8) → Wq (8,4) → Query vectors  (11,4)  ┐\n",
        "              → Wk (8,4) → Key vectors    (11,4)  │ Single head\n",
        "              → Wv (8,4) → Value vectors  (11,4)  ┘\n",
        "                                       d_out = 4\n",
        "\n",
        "    Multi-Query (page 10):\n",
        "    - 4 query heads, but only 1 key head and 1 value head\n",
        "    - Key and Value are SHARED across all query heads\n",
        "    - Saves memory: KV cache = 1 × head_dim instead of 4 × head_dim\n",
        "    ─────────────────────────────────────────────────────\n",
        "\n",
        "    Dimension walkthrough for Gemma3 270M:\n",
        "    ┌──────────────────────────────────────────────────────┐\n",
        "    │ n_heads = 4, n_kv_groups = 1, head_dim = 256        │\n",
        "    │                                                      │\n",
        "    │ Wq: emb_dim(640) → n_heads × head_dim = 4×256=1024  │\n",
        "    │ Wk: emb_dim(640) → n_kv × head_dim   = 1×256=256   │\n",
        "    │ Wv: emb_dim(640) → n_kv × head_dim   = 1×256=256   │\n",
        "    │ Wo: n_heads×head_dim(1024) → emb_dim(640)           │\n",
        "    │                                                      │\n",
        "    │ Q shape: (batch, seq, 4, 256)                        │\n",
        "    │ K shape: (batch, seq, 1, 256) → broadcast to 4 heads│\n",
        "    │ V shape: (batch, seq, 1, 256) → broadcast to 4 heads│\n",
        "    └──────────────────────────────────────────────────────┘\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Gemma3Config, layer_type: str):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            config:     Model configuration\n",
        "            layer_type: \"sliding_attention\" or \"full_attention\"\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_type = layer_type\n",
        "        self.n_heads = config.n_heads\n",
        "        self.n_kv = config.n_kv_groups         # 1 for MQA\n",
        "        self.head_dim = config.head_dim         # 256\n",
        "        self.scaling = config.query_pre_attn_scalar ** -0.5  # 1/√256\n",
        "\n",
        "        # ── Projection layers ──\n",
        "        # Query: 640 → 4 × 256 = 1024\n",
        "        self.q_proj = nn.Linear(config.emb_dim, config.n_heads * config.head_dim, bias=False)\n",
        "        # Key:   640 → 1 × 256 = 256  (shared across heads!)\n",
        "        self.k_proj = nn.Linear(config.emb_dim, config.n_kv_groups * config.head_dim, bias=False)\n",
        "        # Value: 640 → 1 × 256 = 256  (shared across heads!)\n",
        "        self.v_proj = nn.Linear(config.emb_dim, config.n_kv_groups * config.head_dim, bias=False)\n",
        "        # Output: 4 × 256 = 1024 → 640\n",
        "        self.o_proj = nn.Linear(config.n_heads * config.head_dim, config.emb_dim, bias=False)\n",
        "\n",
        "        # ── QK Normalization (from your notes page 14) ──\n",
        "        # \"QK Norm = RMS Normalization applied to Query and Key vectors\"\n",
        "        # Stabilizes attention scores, especially important at scale\n",
        "        if config.qk_norm:\n",
        "            self.q_norm = RMSNorm(config.head_dim)\n",
        "            self.k_norm = RMSNorm(config.head_dim)\n",
        "\n",
        "        # ── Sliding window size ──\n",
        "        self.sliding_window = config.sliding_window if layer_type == \"sliding_attention\" else None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        freqs_cis: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x:         (batch, seq_len, emb_dim) — input embeddings\n",
        "            freqs_cis: (seq_len, head_dim//2) — RoPE frequencies\n",
        "\n",
        "        Returns:\n",
        "            output: (batch, seq_len, emb_dim)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # ── Step 1: Project to Q, K, V (from your notes page 8) ──\n",
        "        q = self.q_proj(x)  # (batch, seq, n_heads * head_dim)\n",
        "        k = self.k_proj(x)  # (batch, seq, n_kv * head_dim)\n",
        "        v = self.v_proj(x)  # (batch, seq, n_kv * head_dim)\n",
        "\n",
        "        # ── Reshape to separate heads ──\n",
        "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)  # (B, S, 4, 256)\n",
        "        k = k.view(batch_size, seq_len, self.n_kv, self.head_dim)    # (B, S, 1, 256)\n",
        "        v = v.view(batch_size, seq_len, self.n_kv, self.head_dim)    # (B, S, 1, 256)\n",
        "\n",
        "        # ── Step 2: QK Normalization (your notes page 14) ──\n",
        "        if self.config.qk_norm:\n",
        "            q = self.q_norm(q)\n",
        "            k = self.k_norm(k)\n",
        "\n",
        "        # ── Step 3: Apply RoPE to Q and K (your notes pages 12-13) ──\n",
        "        # \"Apply sin and cosine positional encoding to these vectors\"\n",
        "        q = apply_rope(q, freqs_cis)\n",
        "        k = apply_rope(k, freqs_cis)\n",
        "\n",
        "        # ── Step 4: Expand K, V for Multi-Query → all heads share same K,V ──\n",
        "        # K: (B, S, 1, 256) → repeat → (B, S, 4, 256)\n",
        "        # This is the core MQA trick: 1 KV pair serves 4 query heads\n",
        "        if self.n_kv < self.n_heads:\n",
        "            n_rep = self.n_heads // self.n_kv\n",
        "            k = k.expand(batch_size, seq_len, self.n_heads, self.head_dim)\n",
        "            v = v.expand(batch_size, seq_len, self.n_heads, self.head_dim)\n",
        "\n",
        "        # ── Transpose for attention: (B, heads, S, dim) ──\n",
        "        q = q.transpose(1, 2)  # (B, 4, S, 256)\n",
        "        k = k.transpose(1, 2)  # (B, 4, S, 256)\n",
        "        v = v.transpose(1, 2)  # (B, 4, S, 256)\n",
        "\n",
        "        # ── Step 5: Compute attention scores (your notes page 9) ──\n",
        "        # \"Q × K^T → Head Attention Scores\"\n",
        "        # Scaling: divide by √(query_pre_attn_scalar) = √256 = 16\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scaling\n",
        "        # attn_scores shape: (B, 4, S, S)\n",
        "\n",
        "        # ── Step 6: Apply masks (your notes page 11) ──\n",
        "        # Causal mask: \"cannot peek into future\"\n",
        "        # Sliding window: \"not too far back into past\"\n",
        "        attn_scores = self._apply_mask(attn_scores, seq_len)\n",
        "\n",
        "        # ── Step 7: Softmax → attention weights (your notes page 9) ──\n",
        "        # \"Softmax ensures each row sums to 1 → we can add probabilities\"\n",
        "        # Use float32 for softmax stability, then cast back\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1, dtype=torch.float32).to(q.dtype)\n",
        "\n",
        "        # ── Step 8: Context vectors = weights × V (your notes page 10) ──\n",
        "        # \"Head1 Attn weights × V1 = Head1 Context Matrix\"\n",
        "        context = torch.matmul(attn_weights, v)  # (B, 4, S, 256)\n",
        "\n",
        "        # ── Step 9: Merge heads (your notes page 10) ──\n",
        "        # \"Merge context matrix for both heads\"\n",
        "        # (B, 4, S, 256) → (B, S, 4, 256) → (B, S, 1024)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
        "\n",
        "        # ── Step 10: Output projection → back to emb_dim ──\n",
        "        output = self.o_proj(context)  # (B, S, 640)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _apply_mask(self, attn_scores: torch.Tensor, seq_len: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply causal mask + optional sliding window mask.\n",
        "\n",
        "        From your notes (page 11):\n",
        "        ─────────────────────────────────────────────\n",
        "        Full Attention Causal Mask (cannot see future):\n",
        "            j: 0  1  2  3  4  5\n",
        "        i=0:  0  1  1  1  1  1    ← 1 = masked (−inf)\n",
        "        i=1:  0  0  1  1  1  1       0 = visible\n",
        "        i=2:  0  0  0  1  1  1\n",
        "        i=3:  0  0  0  0  1  1\n",
        "        i=4:  0  0  0  0  0  1\n",
        "        i=5:  0  0  0  0  0  0\n",
        "\n",
        "        Sliding Window (w=512): ALSO masks positions > 512 steps ago\n",
        "            \"Cannot peek into future AND not too far back into past\"\n",
        "\n",
        "        Combined: upper triangle (future) + far-past = masked\n",
        "        ─────────────────────────────────────────────\n",
        "        \"\"\"\n",
        "        # Standard causal mask: mask everything above diagonal\n",
        "        causal_mask = torch.triu(\n",
        "            torch.ones(seq_len, seq_len, device=attn_scores.device, dtype=torch.bool),\n",
        "            diagonal=1\n",
        "        )\n",
        "\n",
        "        if self.sliding_window is not None:\n",
        "            # Sliding window: also mask positions more than w steps back\n",
        "            # \"For Gemma, input size n=32,768, window size w=512\"\n",
        "            sliding_mask = torch.tril(\n",
        "                torch.ones(seq_len, seq_len, device=attn_scores.device, dtype=torch.bool),\n",
        "                diagonal=-(self.sliding_window + 1)\n",
        "            )\n",
        "            combined_mask = causal_mask | sliding_mask\n",
        "        else:\n",
        "            combined_mask = causal_mask\n",
        "\n",
        "        # Apply mask: set masked positions to -inf (→ 0 after softmax)\n",
        "        attn_scores = attn_scores.masked_fill(combined_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "\n",
        "        return attn_scores\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 🧪 TEST: Verify Multi-Query Attention\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"🧠 Testing Multi-Query Attention\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# ── Cast model to bfloat16 to match input dtype ──\n",
        "attn_sliding = MultiQueryAttention(model_config, \"sliding_attention\").to(device=device, dtype=dtype)\n",
        "attn_full    = MultiQueryAttention(model_config, \"full_attention\").to(device=device, dtype=dtype)\n",
        "\n",
        "batch, seq = 2, 32\n",
        "x_test = torch.randn(batch, seq, model_config.emb_dim, device=device, dtype=dtype)\n",
        "\n",
        "# Precompute RoPE for both layer types\n",
        "freqs_local  = precompute_rope_frequencies(\n",
        "    model_config.head_dim, seq, model_config.rope_local_base, device=device\n",
        ")\n",
        "freqs_global = precompute_rope_frequencies(\n",
        "    model_config.head_dim, seq, model_config.rope_base, device=device\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_sliding = attn_sliding(x_test, freqs_local)\n",
        "    out_full    = attn_full(x_test, freqs_global)\n",
        "\n",
        "print(f\"\\n   Input shape:          {x_test.shape}  (batch=2, seq=32, dim=640)\")\n",
        "print(f\"   Sliding attn output:  {out_sliding.shape}\")\n",
        "print(f\"   Full attn output:     {out_full.shape}\")\n",
        "print(f\"   Output dtype:         {out_sliding.dtype}\")\n",
        "\n",
        "# ── Count parameters ──\n",
        "def count_params(module):\n",
        "    return sum(p.numel() for p in module.parameters())\n",
        "\n",
        "print(f\"\\n   Sliding attention params: {count_params(attn_sliding):,}\")\n",
        "print(f\"   Full attention params:    {count_params(attn_full):,}\")\n",
        "print(f\"   (Same params — only mask differs)\")\n",
        "\n",
        "# ── Verify MQA: K,V are shared ──\n",
        "print(f\"\\n🔍 MQA Verification:\")\n",
        "print(f\"   Q projection: {model_config.emb_dim} → {model_config.n_heads}×{model_config.head_dim} = {model_config.n_heads * model_config.head_dim}\")\n",
        "print(f\"   K projection: {model_config.emb_dim} → {model_config.n_kv_groups}×{model_config.head_dim} = {model_config.n_kv_groups * model_config.head_dim}  ← SHARED!\")\n",
        "print(f\"   V projection: {model_config.emb_dim} → {model_config.n_kv_groups}×{model_config.head_dim} = {model_config.n_kv_groups * model_config.head_dim}  ← SHARED!\")\n",
        "print(f\"   Memory saved: {model_config.n_heads}× less KV cache\")\n",
        "\n",
        "# ── Verify sliding window mask ──\n",
        "print(f\"\\n🔍 Sliding Window Mask (w={model_config.sliding_window}):\")\n",
        "print(f\"   Full attn cost:    O(n²) = {seq}² = {seq**2}\")\n",
        "print(f\"   Sliding attn cost: O(n×w) = {seq}×{model_config.sliding_window} = {seq * model_config.sliding_window}\")\n",
        "print(f\"   At context_length=32K: {model_config.context_length}²/2 vs {model_config.context_length}×{model_config.sliding_window}\")\n",
        "print(f\"   Savings: {(model_config.context_length//2) // model_config.sliding_window}× cheaper!\")\n",
        "\n",
        "del attn_sliding, attn_full, x_test, out_sliding, out_full\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n{'='*55}\")\n",
        "print(f\"  ✅ Cell 7 Done! Attention mechanism ready.\")\n",
        "print(f\"{'='*55}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND-BP8huBTCx",
        "outputId": "20faf2e5-da24-47b9-b001-bc786490c05f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Testing Multi-Query Attention\n",
            "=======================================================\n",
            "\n",
            "   Input shape:          torch.Size([2, 32, 640])  (batch=2, seq=32, dim=640)\n",
            "   Sliding attn output:  torch.Size([2, 32, 640])\n",
            "   Full attn output:     torch.Size([2, 32, 640])\n",
            "   Output dtype:         torch.bfloat16\n",
            "\n",
            "   Sliding attention params: 1,638,912\n",
            "   Full attention params:    1,638,912\n",
            "   (Same params — only mask differs)\n",
            "\n",
            "🔍 MQA Verification:\n",
            "   Q projection: 640 → 4×256 = 1024\n",
            "   K projection: 640 → 1×256 = 256  ← SHARED!\n",
            "   V projection: 640 → 1×256 = 256  ← SHARED!\n",
            "   Memory saved: 4× less KV cache\n",
            "\n",
            "🔍 Sliding Window Mask (w=512):\n",
            "   Full attn cost:    O(n²) = 32² = 1024\n",
            "   Sliding attn cost: O(n×w) = 32×512 = 16384\n",
            "   At context_length=32K: 32768²/2 vs 32768×512\n",
            "   Savings: 32× cheaper!\n",
            "\n",
            "=======================================================\n",
            "  ✅ Cell 7 Done! Attention mechanism ready.\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 8: Feed Forward Network (FFN) with GELU Activation               ║\n",
        "# ║                                                                         ║\n",
        "# ║  From your notes (page 15):                                             ║\n",
        "# ║                                                                         ║\n",
        "# ║  Traditional FFN: expansion → contraction                               ║\n",
        "# ║  Gemma3 FFN: expansion with GATED activation → contraction             ║\n",
        "# ║                                                                         ║\n",
        "# ║  Architecture (from your notes page 15):                                ║\n",
        "# ║  ┌──────────────────────────────────────────────────────┐              ║\n",
        "# ║  │                                                      │              ║\n",
        "# ║  │  Input (640)                                         │              ║\n",
        "# ║  │     ├── gate_proj (640 → 2048) ── GELU ──┐          │              ║\n",
        "# ║  │     │                                     × (mul)   │ Expansion    ║\n",
        "# ║  │     └── up_proj   (640 → 2048) ──────────┘          │              ║\n",
        "# ║  │                        │                             │              ║\n",
        "# ║  │                  (2048 values)                       │              ║\n",
        "# ║  │                        │                             │              ║\n",
        "# ║  │               down_proj (2048 → 640)                 │ Contraction  ║\n",
        "# ║  │                        │                             │              ║\n",
        "# ║  │                 Output (640)                         │              ║\n",
        "# ║  └──────────────────────────────────────────────────────┘              ║\n",
        "# ║                                                                         ║\n",
        "# ║  \"It allows to explore richer and higher dimensional space\"             ║\n",
        "# ║                                                                         ║\n",
        "# ║  Why gated? The gate learns WHICH expanded features matter.             ║\n",
        "# ║  GELU(gate) × up = selective amplification of useful features.          ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Gated Feed-Forward Network with GELU activation.\n",
        "\n",
        "    From your notes (page 15):\n",
        "    ─────────────────────────────────────────────\n",
        "    for eg:\n",
        "        input (640) → expand to (2048)  [expansion]\n",
        "        GELU activation on gate path\n",
        "        gate × up → gated features (2048)\n",
        "        (2048) → contract to (640)      [contraction]\n",
        "\n",
        "    \"It allows to explore richer and higher dimensional space\"\n",
        "    ─────────────────────────────────────────────\n",
        "\n",
        "    This is sometimes called SwiGLU or GeGLU depending on the activation.\n",
        "    Gemma3 uses GELU (Gaussian Error Linear Unit).\n",
        "\n",
        "    Dimensions for Gemma3 270M:\n",
        "        emb_dim = 640, hidden_dim = 2048\n",
        "        Expansion ratio: 2048/640 = 3.2×\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Gemma3Config):\n",
        "        super().__init__()\n",
        "\n",
        "        # Gate path: learns which features to activate\n",
        "        # Input (640) → Hidden (2048), then apply GELU\n",
        "        self.gate_proj = nn.Linear(config.emb_dim, config.hidden_dim, bias=False)\n",
        "\n",
        "        # Up path: projects input to higher dimension\n",
        "        # Input (640) → Hidden (2048)\n",
        "        self.up_proj = nn.Linear(config.emb_dim, config.hidden_dim, bias=False)\n",
        "\n",
        "        # Down path: contracts back to original dimension\n",
        "        # Hidden (2048) → Output (640)\n",
        "        self.down_proj = nn.Linear(config.hidden_dim, config.emb_dim, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_len, emb_dim=640)\n",
        "\n",
        "        Returns:\n",
        "            output: (batch, seq_len, emb_dim=640)\n",
        "\n",
        "        Flow:\n",
        "            gate = GELU(gate_proj(x))    → (batch, seq, 2048)\n",
        "            up   = up_proj(x)            → (batch, seq, 2048)\n",
        "            fused = gate × up            → (batch, seq, 2048)  [element-wise]\n",
        "            output = down_proj(fused)     → (batch, seq, 640)\n",
        "        \"\"\"\n",
        "        # Gate path with GELU activation\n",
        "        # GELU ≈ x × Φ(x), where Φ is the Gaussian CDF\n",
        "        # Smoother than ReLU — no hard cutoff at 0\n",
        "        gate = F.gelu(self.gate_proj(x), approximate=\"tanh\")\n",
        "\n",
        "        # Up path (no activation — raw projection)\n",
        "        up = self.up_proj(x)\n",
        "\n",
        "        # Gated combination: gate decides which features pass through\n",
        "        # This is the \"gated\" in GeGLU\n",
        "        fused = gate * up\n",
        "\n",
        "        # Contract back to embedding dimension\n",
        "        output = self.down_proj(fused)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 🧪 TEST: Verify Feed Forward Network\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"⚡ Testing Feed Forward Network (GELU)\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "ffn = FeedForward(model_config).to(device=device, dtype=dtype)\n",
        "\n",
        "batch, seq = 2, 32\n",
        "x_test = torch.randn(batch, seq, model_config.emb_dim, device=device, dtype=dtype)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_test = ffn(x_test)\n",
        "\n",
        "print(f\"\\n   Input shape:   {x_test.shape}   (batch=2, seq=32, dim=640)\")\n",
        "print(f\"   Output shape:  {out_test.shape}  (same shape ✓)\")\n",
        "print(f\"   Output dtype:  {out_test.dtype}\")\n",
        "\n",
        "# ── Show expansion/contraction dimensions ──\n",
        "print(f\"\\n🔍 FFN Dimension Flow:\")\n",
        "print(f\"   Input:     {model_config.emb_dim}\")\n",
        "print(f\"   ├── gate_proj: {model_config.emb_dim} → {model_config.hidden_dim}  (expansion)\")\n",
        "print(f\"   ├── up_proj:   {model_config.emb_dim} → {model_config.hidden_dim}  (expansion)\")\n",
        "print(f\"   ├── GELU(gate) × up = gated features ({model_config.hidden_dim})\")\n",
        "print(f\"   └── down_proj: {model_config.hidden_dim} → {model_config.emb_dim}  (contraction)\")\n",
        "print(f\"   Expansion ratio: {model_config.hidden_dim/model_config.emb_dim:.1f}×\")\n",
        "\n",
        "# ── Count parameters ──\n",
        "ffn_params = count_params(ffn)\n",
        "print(f\"\\n   FFN parameters: {ffn_params:,}\")\n",
        "print(f\"   Breakdown:\")\n",
        "print(f\"     gate_proj: {model_config.emb_dim}×{model_config.hidden_dim} = {model_config.emb_dim * model_config.hidden_dim:,}\")\n",
        "print(f\"     up_proj:   {model_config.emb_dim}×{model_config.hidden_dim} = {model_config.emb_dim * model_config.hidden_dim:,}\")\n",
        "print(f\"     down_proj: {model_config.hidden_dim}×{model_config.emb_dim} = {model_config.hidden_dim * model_config.emb_dim:,}\")\n",
        "print(f\"     Total:     {3 * model_config.emb_dim * model_config.hidden_dim:,} ✓\")\n",
        "\n",
        "del ffn, x_test, out_test\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n{'='*55}\")\n",
        "print(f\"  ✅ Cell 8 Done! Feed Forward Network ready.\")\n",
        "print(f\"{'='*55}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjjlU4JLB3Js",
        "outputId": "81c32733-ff2d-4b18-cbc5-01cae4c637bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ Testing Feed Forward Network (GELU)\n",
            "=======================================================\n",
            "\n",
            "   Input shape:   torch.Size([2, 32, 640])   (batch=2, seq=32, dim=640)\n",
            "   Output shape:  torch.Size([2, 32, 640])  (same shape ✓)\n",
            "   Output dtype:  torch.bfloat16\n",
            "\n",
            "🔍 FFN Dimension Flow:\n",
            "   Input:     640\n",
            "   ├── gate_proj: 640 → 2048  (expansion)\n",
            "   ├── up_proj:   640 → 2048  (expansion)\n",
            "   ├── GELU(gate) × up = gated features (2048)\n",
            "   └── down_proj: 2048 → 640  (contraction)\n",
            "   Expansion ratio: 3.2×\n",
            "\n",
            "   FFN parameters: 3,932,160\n",
            "   Breakdown:\n",
            "     gate_proj: 640×2048 = 1,310,720\n",
            "     up_proj:   640×2048 = 1,310,720\n",
            "     down_proj: 2048×640 = 1,310,720\n",
            "     Total:     3,932,160 ✓\n",
            "\n",
            "=======================================================\n",
            "  ✅ Cell 8 Done! Feed Forward Network ready.\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 9: Transformer Block (One Complete Layer)                         ║\n",
        "# ║                                                                         ║\n",
        "# ║  From your notes (page 5 — architecture diagram):                       ║\n",
        "# ║                                                                         ║\n",
        "# ║  Each transformer block has this structure:                              ║\n",
        "# ║                                                                         ║\n",
        "# ║      Input (x)                                                          ║\n",
        "# ║        │                                                                ║\n",
        "# ║        ├────────────────────────┐                                       ║\n",
        "# ║        │                        │ (skip connection)                     ║\n",
        "# ║        ▼                        │                                       ║\n",
        "# ║   [RMS Normalization]           │                                       ║\n",
        "# ║        │                        │                                       ║\n",
        "# ║        ▼                        │                                       ║\n",
        "# ║   [Sliding Window /             │                                       ║\n",
        "# ║    Full Attention   ]           │                                       ║\n",
        "# ║    + RoPE + QK Norm             │                                       ║\n",
        "# ║        │                        │                                       ║\n",
        "# ║        ▼                        │                                       ║\n",
        "# ║      (+) ◄──────────────────────┘  ← SKIP CONNECTION #1                ║\n",
        "# ║        │                                                                ║\n",
        "# ║        ├────────────────────────┐                                       ║\n",
        "# ║        │                        │ (skip connection)                     ║\n",
        "# ║        ▼                        │                                       ║\n",
        "# ║   [RMS Normalization]           │                                       ║\n",
        "# ║        │                        │                                       ║\n",
        "# ║        ▼                        │                                       ║\n",
        "# ║   [Feed Forward NN]             │                                       ║\n",
        "# ║    GELU gated (640→2048→640)    │                                       ║\n",
        "# ║        │                        │                                       ║\n",
        "# ║        ▼                        │                                       ║\n",
        "# ║      (+) ◄──────────────────────┘  ← SKIP CONNECTION #2                ║\n",
        "# ║        │                                                                ║\n",
        "# ║      Output                                                             ║\n",
        "# ║                                                                         ║\n",
        "# ║  Skip connections (page 15):                                            ║\n",
        "# ║  \"Added just to make sure gradient flow or rather the gradient          ║\n",
        "# ║   has alternate direction to flow, to vanishing gradient.\"              ║\n",
        "# ║                                                                         ║\n",
        "# ║  Pre-norm style: normalize BEFORE attention/FFN (not after)             ║\n",
        "# ║  This is more stable for training deep networks.                        ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    One Gemma3 transformer layer.\n",
        "\n",
        "    Pre-norm architecture:\n",
        "        output = x + Attention(RMSNorm(x))\n",
        "        output = output + FFN(RMSNorm(output))\n",
        "\n",
        "    From your notes (page 5):\n",
        "    \"This is only one block of transformer of Gemma 3 270M.\n",
        "     So, 18 such blocks of transformers were used.\"\n",
        "\n",
        "    Each block contains:\n",
        "    1. RMSNorm → Multi-Query Attention → Skip Connection\n",
        "    2. RMSNorm → Feed Forward Network → Skip Connection\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Gemma3Config, layer_idx: int):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            config:    Model configuration\n",
        "            layer_idx: Which layer (0-17) — determines sliding vs full attention\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Determine layer type from config\n",
        "        layer_type = config.layer_types[layer_idx]\n",
        "\n",
        "        # Pre-attention normalization\n",
        "        self.attn_norm = RMSNorm(config.emb_dim)\n",
        "\n",
        "        # Attention (sliding or full, with RoPE + QK norm)\n",
        "        self.attention = MultiQueryAttention(config, layer_type)\n",
        "\n",
        "        # Pre-FFN normalization\n",
        "        self.ffn_norm = RMSNorm(config.emb_dim)\n",
        "\n",
        "        # Feed Forward Network (gated GELU)\n",
        "        self.ffn = FeedForward(config)\n",
        "\n",
        "        # Store layer info for RoPE base selection\n",
        "        self.layer_type = layer_type\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        freqs_cis: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x:         (batch, seq_len, emb_dim)\n",
        "            freqs_cis: (seq_len, head_dim//2) — RoPE frequencies\n",
        "\n",
        "        Returns:\n",
        "            output: (batch, seq_len, emb_dim)\n",
        "        \"\"\"\n",
        "        # ── Sub-block 1: Attention with skip connection ──\n",
        "        # From your notes (page 5): RMSNorm → Attention → (+)\n",
        "        # Skip connection: gradient can flow directly through (+)\n",
        "        residual = x\n",
        "        x = self.attn_norm(x)              # RMS Normalization\n",
        "        x = self.attention(x, freqs_cis)   # Multi-Query Attention + RoPE\n",
        "        x = residual + x                   # Skip connection #1\n",
        "\n",
        "        # ── Sub-block 2: FFN with skip connection ──\n",
        "        # From your notes (page 5): RMSNorm → FFN → (+)\n",
        "        residual = x\n",
        "        x = self.ffn_norm(x)               # RMS Normalization\n",
        "        x = self.ffn(x)                    # Feed Forward (640→2048→640)\n",
        "        x = residual + x                   # Skip connection #2\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 🧪 TEST: Verify Transformer Block\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"🧱 Testing Transformer Block\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# ── Test a sliding attention block (e.g., layer 0) ──\n",
        "block_sliding = TransformerBlock(model_config, layer_idx=0).to(device=device, dtype=dtype)\n",
        "# ── Test a full attention block (e.g., layer 5 → index 5 = \"full_attention\") ──\n",
        "block_full = TransformerBlock(model_config, layer_idx=5).to(device=device, dtype=dtype)\n",
        "\n",
        "batch, seq = 2, 32\n",
        "x_test = torch.randn(batch, seq, model_config.emb_dim, device=device, dtype=dtype)\n",
        "\n",
        "freqs_local = precompute_rope_frequencies(\n",
        "    model_config.head_dim, seq, model_config.rope_local_base, device=device\n",
        ")\n",
        "freqs_global = precompute_rope_frequencies(\n",
        "    model_config.head_dim, seq, model_config.rope_base, device=device\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_sliding = block_sliding(x_test, freqs_local)\n",
        "    out_full = block_full(x_test, freqs_global)\n",
        "\n",
        "print(f\"\\n   Input shape:           {x_test.shape}\")\n",
        "print(f\"   Sliding block output:  {out_sliding.shape}  (layer 0: {block_sliding.layer_type})\")\n",
        "print(f\"   Full block output:     {out_full.shape}  (layer 5: {block_full.layer_type})\")\n",
        "\n",
        "# ── Verify skip connection works ──\n",
        "# Output should NOT be identical to input (attention + FFN changed it)\n",
        "# But should be in similar range (skip connection keeps it grounded)\n",
        "diff = (out_sliding - x_test).abs().mean().item()\n",
        "print(f\"\\n   Skip connection test:\")\n",
        "print(f\"   Mean |output - input|: {diff:.4f}\")\n",
        "print(f\"   (Non-zero but small → skip connection + transformations working ✓)\")\n",
        "\n",
        "# ── Count parameters per block ──\n",
        "block_params = count_params(block_sliding)\n",
        "print(f\"\\n📊 Parameters per Transformer Block:\")\n",
        "print(f\"   Total:         {block_params:,}\")\n",
        "print(f\"   ├── attn_norm: {count_params(block_sliding.attn_norm):,}\")\n",
        "print(f\"   ├── attention: {count_params(block_sliding.attention):,}\")\n",
        "print(f\"   ├── ffn_norm:  {count_params(block_sliding.ffn_norm):,}\")\n",
        "print(f\"   └── ffn:       {count_params(block_sliding.ffn):,}\")\n",
        "print(f\"\\n   18 blocks total: {block_params * 18:,} (~{block_params * 18 / 1e6:.1f}M)\")\n",
        "\n",
        "# ── Show layer type map ──\n",
        "print(f\"\\n📋 Layer Type Map (18 layers):\")\n",
        "for i, lt in enumerate(model_config.layer_types):\n",
        "    marker = \"🌐 GLOBAL\" if lt == \"full_attention\" else \"📍 local\"\n",
        "    print(f\"   Layer {i+1:2d}: {lt:20s} {marker}\")\n",
        "\n",
        "del block_sliding, block_full, x_test, out_sliding, out_full\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n{'='*55}\")\n",
        "print(f\"  ✅ Cell 9 Done! Transformer Block ready.\")\n",
        "print(f\"{'='*55}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvjt5vCoCJep",
        "outputId": "66dff63f-e6f6-4590-d73e-b663e80598bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧱 Testing Transformer Block\n",
            "=======================================================\n",
            "\n",
            "   Input shape:           torch.Size([2, 32, 640])\n",
            "   Sliding block output:  torch.Size([2, 32, 640])  (layer 0: sliding_attention)\n",
            "   Full block output:     torch.Size([2, 32, 640])  (layer 5: full_attention)\n",
            "\n",
            "   Skip connection test:\n",
            "   Mean |output - input|: 0.1475\n",
            "   (Non-zero but small → skip connection + transformations working ✓)\n",
            "\n",
            "📊 Parameters per Transformer Block:\n",
            "   Total:         5,572,352\n",
            "   ├── attn_norm: 640\n",
            "   ├── attention: 1,638,912\n",
            "   ├── ffn_norm:  640\n",
            "   └── ffn:       3,932,160\n",
            "\n",
            "   18 blocks total: 100,302,336 (~100.3M)\n",
            "\n",
            "📋 Layer Type Map (18 layers):\n",
            "   Layer  1: sliding_attention    📍 local\n",
            "   Layer  2: sliding_attention    📍 local\n",
            "   Layer  3: sliding_attention    📍 local\n",
            "   Layer  4: sliding_attention    📍 local\n",
            "   Layer  5: sliding_attention    📍 local\n",
            "   Layer  6: full_attention       🌐 GLOBAL\n",
            "   Layer  7: sliding_attention    📍 local\n",
            "   Layer  8: sliding_attention    📍 local\n",
            "   Layer  9: sliding_attention    📍 local\n",
            "   Layer 10: sliding_attention    📍 local\n",
            "   Layer 11: sliding_attention    📍 local\n",
            "   Layer 12: full_attention       🌐 GLOBAL\n",
            "   Layer 13: sliding_attention    📍 local\n",
            "   Layer 14: sliding_attention    📍 local\n",
            "   Layer 15: sliding_attention    📍 local\n",
            "   Layer 16: sliding_attention    📍 local\n",
            "   Layer 17: sliding_attention    📍 local\n",
            "   Layer 18: full_attention       🌐 GLOBAL\n",
            "\n",
            "=======================================================\n",
            "  ✅ Cell 9 Done! Transformer Block ready.\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 10: Complete Gemma 3 270M Model Assembly                          ║\n",
        "# ║                                                                         ║\n",
        "# ║  From your notes (pages 5, 16-17):                                      ║\n",
        "# ║                                                                         ║\n",
        "# ║  ┌──────────────────────────────────────────────────────────────┐       ║\n",
        "# ║  │                    FULL ARCHITECTURE                         │       ║\n",
        "# ║  │                                                              │       ║\n",
        "# ║  │  ① INPUT                                                    │       ║\n",
        "# ║  │     Token IDs [1, 11, 15, 24]                               │       ║\n",
        "# ║  │          │                                                   │       ║\n",
        "# ║  │          ▼                                                   │       ║\n",
        "# ║  │     [Token Embedding]  (vocab_size × emb_dim)               │       ║\n",
        "# ║  │      50,257 × 640 → lookup → (batch, seq, 640)             │       ║\n",
        "# ║  │          │                                                   │       ║\n",
        "# ║  │          │  × √emb_dim  (Gemma3 scaling trick!)             │       ║\n",
        "# ║  │          │                                                   │       ║\n",
        "# ║  │  ② PROCESSOR (×18 transformer blocks)                       │       ║\n",
        "# ║  │          │                                                   │       ║\n",
        "# ║  │     ┌────┴────┐                                             │       ║\n",
        "# ║  │     │ Block 1 │ RMSNorm→Attn(sliding+RoPE)→RMSNorm→FFN    │       ║\n",
        "# ║  │     │ Block 2 │ ... (repeat)                                │       ║\n",
        "# ║  │     │  ...    │                                             │       ║\n",
        "# ║  │     │ Block 18│ (last block uses full attention)            │       ║\n",
        "# ║  │     └────┬────┘                                             │       ║\n",
        "# ║  │          │                                                   │       ║\n",
        "# ║  │     [Final RMS Norm]                                        │       ║\n",
        "# ║  │          │                                                   │       ║\n",
        "# ║  │  ③ OUTPUT                                                   │       ║\n",
        "# ║  │     [Output Layer] (emb_dim → vocab_size)                   │       ║\n",
        "# ║  │      640 → 50,257 (logits for each token in vocab)         │       ║\n",
        "# ║  │          │                                                   │       ║\n",
        "# ║  │     Logits (batch, seq, 50257)                              │       ║\n",
        "# ║  │     \"Each element corresponds to the probability            │       ║\n",
        "# ║  │      of it being the next token\" (page 17)                 │       ║\n",
        "# ║  └──────────────────────────────────────────────────────────────┘       ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "class Gemma3Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Gemma 3 270M Language Model.\n",
        "\n",
        "    From your notes (page 16-17):\n",
        "    ─────────────────────────────────────────────────\n",
        "    ① Input:     Token IDs → Embedding (4 × 640)\n",
        "    ② Processor: 18 transformer blocks\n",
        "                 RMSNorm → Attention+RoPE → RMSNorm → FFN\n",
        "                 (repeated 18 times with skip connections)\n",
        "    ③ Output:    Neural network layer (640 × 50,257)\n",
        "                 \"This is how we get prediction from Gemma\"\n",
        "\n",
        "    Output logits shape: (batch_size × num_tokens × vocab_size)\n",
        "    \"For 1 batch: 1 × 4 × 50,257\"\n",
        "    \"For 2 batch: 2 × 4 × 50,257\"\n",
        "    ─────────────────────────────────────────────────\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Gemma3Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # ── ① Token Embedding ──\n",
        "        # Converts token IDs → dense vectors\n",
        "        # \"Token IDs are converted into high dimensional vectors\n",
        "        #  so they can encode meaning\" (page 6)\n",
        "        # \"We are not using any pretrained models to convert into vectors,\n",
        "        #  they randomly converted. So, they are considered as trainable\n",
        "        #  parameters.\" (page 6)\n",
        "        self.embed_tokens = nn.Embedding(config.vocab_size, config.emb_dim)\n",
        "\n",
        "        # ── ② Transformer Blocks (×18) ──\n",
        "        # \"18 such blocks of transformers were used\" (page 5)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(config, layer_idx=i)\n",
        "            for i in range(config.n_layers)\n",
        "        ])\n",
        "\n",
        "        # ── Final normalization (after all blocks) ──\n",
        "        # From architecture diagram (page 5): final RMSNorm before output\n",
        "        self.final_norm = RMSNorm(config.emb_dim)\n",
        "\n",
        "        # ── ③ Output projection ──\n",
        "        # \"Output layer: Converts embedding dimension to vocabulary\n",
        "        #  dimension for the token prediction\" (page 16)\n",
        "        # \"Neural network layer (640 × 50,257)\" (page 17)\n",
        "        self.output_proj = nn.Linear(config.emb_dim, config.vocab_size, bias=False)\n",
        "\n",
        "        # ── Precompute RoPE frequencies for both bases ──\n",
        "        # Local base (10K) for sliding attention layers\n",
        "        # Global base (1M) for full attention layers\n",
        "        self.register_buffer(\n",
        "            \"freqs_local\",\n",
        "            precompute_rope_frequencies(\n",
        "                config.head_dim, config.context_length, config.rope_local_base\n",
        "            ),\n",
        "            persistent=False\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"freqs_global\",\n",
        "            precompute_rope_frequencies(\n",
        "                config.head_dim, config.context_length, config.rope_base\n",
        "            ),\n",
        "            persistent=False\n",
        "        )\n",
        "\n",
        "        # ── Embedding scaling factor ──\n",
        "        # Gemma3 scales embeddings by √(emb_dim) after lookup\n",
        "        # This compensates for the small initial embedding values\n",
        "        self.emb_scale = config.emb_dim ** 0.5\n",
        "\n",
        "        # ── Initialize weights ──\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Xavier-style initialization for stable training.\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        targets: Optional[torch.Tensor] = None,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids: (batch, seq_len) — token IDs\n",
        "            targets:   (batch, seq_len) — target token IDs (for training)\n",
        "\n",
        "        Returns:\n",
        "            logits: (batch, seq_len, vocab_size)\n",
        "            loss:   scalar (if targets provided, else None)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        # ── ① Embedding ──\n",
        "        # Token IDs → dense vectors, then scale by √d\n",
        "        x = self.embed_tokens(input_ids)    # (batch, seq, 640)\n",
        "        x = x * self.emb_scale              # Gemma3 scaling trick\n",
        "\n",
        "        # ── ② Pass through all 18 transformer blocks ──\n",
        "        for layer in self.layers:\n",
        "            # Select correct RoPE frequencies based on layer type\n",
        "            if layer.layer_type == \"sliding_attention\":\n",
        "                freqs = self.freqs_local[:seq_len]    # Local base (10K)\n",
        "            else:\n",
        "                freqs = self.freqs_global[:seq_len]   # Global base (1M)\n",
        "\n",
        "            x = layer(x, freqs)\n",
        "\n",
        "        # ── Final normalization ──\n",
        "        x = self.final_norm(x)              # (batch, seq, 640)\n",
        "\n",
        "        # ── ③ Output projection → logits ──\n",
        "        # \"Each element corresponds to the probability of it being\n",
        "        #  the next token\" (page 17)\n",
        "        logits = self.output_proj(x)        # (batch, seq, 50257)\n",
        "\n",
        "        # ── Compute loss if targets provided ──\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Cross-entropy loss (from your notes page 18):\n",
        "            # \"−(1/n)[log P₁ + log P₂ + log P₃ + log P₄]\"\n",
        "            # \"Negative log likelihood\"\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, self.config.vocab_size),   # (batch*seq, vocab)\n",
        "                targets.view(-1),                           # (batch*seq,)\n",
        "            )\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        max_new_tokens: int = 200,\n",
        "        temperature: float = 0.8,\n",
        "        top_k: int = 50,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Autoregressive text generation.\n",
        "\n",
        "        From your notes (page 22):\n",
        "        ─────────────────────────────────────────────\n",
        "        Step 6: Inference\n",
        "        1. Get logits → Index of highest value → Token ID\n",
        "        2. Decode back to text\n",
        "        3. Appended to previous inputs → produce next token\n",
        "        4. Feed back to model → repeat\n",
        "        ─────────────────────────────────────────────\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop input to context_length if needed\n",
        "            idx_cond = input_ids if input_ids.size(1) <= self.config.context_length \\\n",
        "                       else input_ids[:, -self.config.context_length:]\n",
        "\n",
        "            # Forward pass → get logits for last position\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]               # (batch, vocab_size)\n",
        "\n",
        "            # Temperature scaling\n",
        "            if temperature > 0:\n",
        "                logits = logits / temperature\n",
        "\n",
        "            # Top-k filtering\n",
        "            if top_k is not None and top_k > 0:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "            # Sample from distribution\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)  # (batch, 1)\n",
        "\n",
        "            # Append to sequence\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 🧪 TEST: Verify Full Model\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"🏗️ Assembling Gemma 3 270M Model\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "model = Gemma3Model(model_config).to(device=device, dtype=dtype)\n",
        "\n",
        "# ── Count all parameters ──\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n📊 Parameter Breakdown:\")\n",
        "emb_params = sum(p.numel() for p in model.embed_tokens.parameters())\n",
        "transformer_params = sum(p.numel() for n, p in model.named_parameters()\n",
        "                        if 'layers' in n)\n",
        "norm_params = sum(p.numel() for p in model.final_norm.parameters())\n",
        "output_params = sum(p.numel() for p in model.output_proj.parameters())\n",
        "\n",
        "print(f\"   Embedding:    {emb_params:>12,}  ({emb_params/1e6:.1f}M)\")\n",
        "print(f\"   Transformer:  {transformer_params:>12,}  ({transformer_params/1e6:.1f}M)\")\n",
        "print(f\"   Final Norm:   {norm_params:>12,}\")\n",
        "print(f\"   Output Proj:  {output_params:>12,}  ({output_params/1e6:.1f}M)\")\n",
        "print(f\"   {'─'*40}\")\n",
        "print(f\"   TOTAL:        {total_params:>12,}  ({total_params/1e6:.1f}M)\")\n",
        "print(f\"   Trainable:    {trainable_params:>12,}  ({trainable_params/1e6:.1f}M)\")\n",
        "\n",
        "# ── Test forward pass ──\n",
        "print(f\"\\n🧪 Forward pass test:\")\n",
        "x_test, y_test = get_batch(\"train\")\n",
        "\n",
        "with torch.no_grad(), autocast(device_type=\"cuda\", dtype=dtype):\n",
        "    logits, loss = model(x_test, y_test)\n",
        "\n",
        "print(f\"   Input:   {x_test.shape}  (batch=32, seq=128)\")\n",
        "print(f\"   Logits:  {logits.shape}  (batch=32, seq=128, vocab=50,257)\")\n",
        "print(f\"   Loss:    {loss.item():.4f}\")\n",
        "print(f\"   Expected initial loss ≈ ln(50257) = {math.log(model_config.vocab_size):.2f}\")\n",
        "print(f\"   {'✅ Close to expected!' if abs(loss.item() - math.log(model_config.vocab_size)) < 1.5 else '⚠️ Check initialization'}\")\n",
        "\n",
        "# ── Test generation (untrained — will be gibberish) ──\n",
        "print(f\"\\n🧪 Generation test (untrained model — expect gibberish):\")\n",
        "prompt = \"Once upon a time\"\n",
        "prompt_ids = torch.tensor([enc.encode_ordinary(prompt)], device=device)\n",
        "with autocast(device_type=\"cuda\", dtype=dtype):\n",
        "    generated = model.generate(prompt_ids, max_new_tokens=20)\n",
        "generated_text = enc.decode(generated[0].tolist())\n",
        "print(f\"   Prompt: \\\"{prompt}\\\"\")\n",
        "print(f\"   Output: \\\"{generated_text}\\\"\")\n",
        "print(f\"   (Gibberish is expected — model hasn't been trained yet!)\")\n",
        "\n",
        "# ── GPU memory usage ──\n",
        "if device_type == \"cuda\":\n",
        "    mem_allocated = torch.cuda.memory_allocated() / 1e9\n",
        "    mem_reserved = torch.cuda.memory_reserved() / 1e9\n",
        "    print(f\"\\n💾 GPU Memory:\")\n",
        "    print(f\"   Allocated: {mem_allocated:.2f} GB\")\n",
        "    print(f\"   Reserved:  {mem_reserved:.2f} GB\")\n",
        "    print(f\"   Free:      {42.4 - mem_reserved:.1f} GB  (plenty of room!)\")\n",
        "\n",
        "del x_test, y_test, logits, loss, prompt_ids, generated\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n{'='*55}\")\n",
        "print(f\"  ✅ Cell 10 Done! Gemma 3 270M assembled!\")\n",
        "print(f\"  🧠 {total_params/1e6:.1f}M parameters ready to train.\")\n",
        "print(f\"{'='*55}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIrttWUhCnOk",
        "outputId": "c53d355c-99dd-4411-e9d1-a98c2eacf21e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏗️ Assembling Gemma 3 270M Model\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1357: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /pytorch/aten/src/ATen/native/Copy.cpp:309.)\n",
            "  return t.to(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Parameter Breakdown:\n",
            "   Embedding:      32,164,480  (32.2M)\n",
            "   Transformer:   100,302,336  (100.3M)\n",
            "   Final Norm:            640\n",
            "   Output Proj:    32,164,480  (32.2M)\n",
            "   ────────────────────────────────────────\n",
            "   TOTAL:         164,631,936  (164.6M)\n",
            "   Trainable:     164,631,936  (164.6M)\n",
            "\n",
            "🧪 Forward pass test:\n",
            "   Input:   torch.Size([32, 128])  (batch=32, seq=128)\n",
            "   Logits:  torch.Size([32, 128, 50257])  (batch=32, seq=128, vocab=50,257)\n",
            "   Loss:    10.9819\n",
            "   Expected initial loss ≈ ln(50257) = 10.82\n",
            "   ✅ Close to expected!\n",
            "\n",
            "🧪 Generation test (untrained model — expect gibberish):\n",
            "   Prompt: \"Once upon a time\"\n",
            "   Output: \"Once upon a time Allianceackers recounts MilleriusIrishjc ace would promotionspartisansb 570ad clocks Atlantaribune respondersardiidency\"\n",
            "   (Gibberish is expected — model hasn't been trained yet!)\n",
            "\n",
            "💾 GPU Memory:\n",
            "   Allocated: 0.77 GB\n",
            "   Reserved:  2.11 GB\n",
            "   Free:      40.3 GB  (plenty of room!)\n",
            "\n",
            "=======================================================\n",
            "  ✅ Cell 10 Done! Gemma 3 270M assembled!\n",
            "  🧠 164.6M parameters ready to train.\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 11: Learning Rate Scheduler & Evaluation Helpers                  ║\n",
        "# ║                                                                         ║\n",
        "# ║  From your notes (page 21):                                             ║\n",
        "# ║                                                                         ║\n",
        "# ║  Learning Rate Schedule:                                                ║\n",
        "# ║       LR ↑                                                              ║\n",
        "# ║          │  ╱╲  cosine                                                  ║\n",
        "# ║          │ ╱  ╲──────────                                               ║\n",
        "# ║          │╱ warmup       ╲  min_lr                                      ║\n",
        "# ║          └────────────────────── Steps                                  ║\n",
        "# ║            0   1000            60K                                      ║\n",
        "# ║                                                                         ║\n",
        "# ║  \"Mix of exploration initially and exploitation later\"                  ║\n",
        "# ║  We use ADAMW optimizer (page 21)                                       ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "def get_lr(step: int) -> float:\n",
        "    \"\"\"\n",
        "    Cosine learning rate schedule with linear warmup.\n",
        "\n",
        "    From your notes (page 21):\n",
        "    ─────────────────────────────────────────────\n",
        "    1. Warmup phase (0 → warmup_steps):\n",
        "       LR increases linearly from 0 to learning_rate\n",
        "       \"Exploration initially\"\n",
        "\n",
        "    2. Cosine decay phase (warmup_steps → lr_decay_iters):\n",
        "       LR decreases following cosine curve\n",
        "       \"Exploitation later\"\n",
        "\n",
        "    3. After lr_decay_iters:\n",
        "       LR stays at min_lr\n",
        "    ─────────────────────────────────────────────\n",
        "    \"\"\"\n",
        "    cfg = train_config\n",
        "\n",
        "    # Phase 1: Linear warmup\n",
        "    if step < cfg.warmup_steps:\n",
        "        return cfg.learning_rate * (step + 1) / cfg.warmup_steps\n",
        "\n",
        "    # Phase 3: After decay period → minimum LR\n",
        "    if step > cfg.lr_decay_iters:\n",
        "        return cfg.min_lr\n",
        "\n",
        "    # Phase 2: Cosine decay\n",
        "    decay_ratio = (step - cfg.warmup_steps) / (cfg.lr_decay_iters - cfg.warmup_steps)\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # Goes from 1 → 0\n",
        "    return cfg.min_lr + coeff * (cfg.learning_rate - cfg.min_lr)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluate model on train and validation sets.\n",
        "\n",
        "    From your notes (page 18):\n",
        "    \"After all batch sizes, we take average of loss\"\n",
        "\n",
        "    Runs eval_iters batches on each split and averages the loss.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    results = {}\n",
        "\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(train_config.eval_iters)\n",
        "        for k in range(train_config.eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            with autocast(device_type=\"cuda\", dtype=dtype):\n",
        "                _, loss = model(x, y)\n",
        "            losses[k] = loss.item()\n",
        "        results[split] = losses.mean().item()\n",
        "\n",
        "    model.train()\n",
        "    return results\n",
        "\n",
        "\n",
        "# ── Test LR schedule ──\n",
        "print(\"📈 Learning Rate Schedule\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Plot the schedule\n",
        "steps = list(range(0, train_config.max_iters, 100))\n",
        "lrs = [get_lr(s) for s in steps]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(steps, lrs, linewidth=2)\n",
        "plt.axvline(x=train_config.warmup_steps, color='r', linestyle='--', alpha=0.5, label=f'Warmup ends ({train_config.warmup_steps})')\n",
        "plt.xlabel('Training Steps')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title('Cosine LR Schedule with Linear Warmup')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(str(RESULTS_DIR / 'lr_schedule.png'), dpi=100)\n",
        "plt.show()\n",
        "print(f\"   Saved: {RESULTS_DIR / 'lr_schedule.png'}\")\n",
        "\n",
        "print(f\"\\n   LR at step 0:      {get_lr(0):.6f}  (start of warmup)\")\n",
        "print(f\"   LR at step 500:    {get_lr(500):.6f}  (mid warmup)\")\n",
        "print(f\"   LR at step 1000:   {get_lr(1000):.6f}  (peak)\")\n",
        "print(f\"   LR at step 30000:  {get_lr(30000):.6f}  (mid decay)\")\n",
        "print(f\"   LR at step 60000:  {get_lr(60000):.6f}  (min LR)\")\n",
        "\n",
        "# ── Quick eval test ──\n",
        "print(f\"\\n🧪 Testing estimate_loss()...\")\n",
        "test_losses = estimate_loss(model)\n",
        "print(f\"   Train loss: {test_losses['train']:.4f}\")\n",
        "print(f\"   Val loss:   {test_losses['val']:.4f}\")\n",
        "print(f\"   (Both ≈ 10.8 expected for untrained model)\")\n",
        "\n",
        "print(f\"\\n{'='*55}\")\n",
        "print(f\"  ✅ Cell 11 Done! LR scheduler & eval ready.\")\n",
        "print(f\"{'='*55}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "CmEavcIHDJNO",
        "outputId": "3df1e453-ce4a-41d2-995b-1091ba8fcdf3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📈 Learning Rate Schedule\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Saved: /content/drive/MyDrive/Gemma3_270M_Project/results/lr_schedule.png\n",
            "\n",
            "   LR at step 0:      0.000000  (start of warmup)\n",
            "   LR at step 500:    0.000050  (mid warmup)\n",
            "   LR at step 1000:   0.000100  (peak)\n",
            "   LR at step 30000:  0.000076  (mid decay)\n",
            "   LR at step 60000:  0.000050  (min LR)\n",
            "\n",
            "🧪 Testing estimate_loss()...\n",
            "   Train loss: 10.9612\n",
            "   Val loss:   10.9585\n",
            "   (Both ≈ 10.8 expected for untrained model)\n",
            "\n",
            "=======================================================\n",
            "  ✅ Cell 11 Done! LR scheduler & eval ready.\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  🚨 FIX: Speed up training — reduce grad accumulation                  ║\n",
        "# ║  Stop current training first! (Runtime → Interrupt execution)           ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "# ── Override config ──\n",
        "train_config.gradient_accumulation_steps = 4    # 32 → 4 (8× faster!)\n",
        "train_config.batch_size = 32                    # Keep same\n",
        "train_config.block_size = 128                   # Keep same\n",
        "train_config.max_iters = 60_000\n",
        "train_config.eval_interval = 500\n",
        "train_config.save_every = 500\n",
        "train_config.warmup_steps = 1_000\n",
        "train_config.lr_decay_iters = 60_000\n",
        "\n",
        "# ── Stats ──\n",
        "eff_batch = train_config.batch_size * train_config.gradient_accumulation_steps\n",
        "eff_tokens = eff_batch * train_config.block_size\n",
        "total = eff_tokens * train_config.max_iters\n",
        "\n",
        "print(\"⚡ FIXED CONFIG\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  grad_accum:  4  (was 32 → 8× fewer micro-batches!)\")\n",
        "print(f\"  Effective batch: {eff_batch} sequences\")\n",
        "print(f\"  Tokens/update:   {eff_tokens:,}\")\n",
        "print(f\"  Total tokens:    {total:,} (~{total/1e9:.1f}B)\")\n",
        "print(f\"\\n  ⏱️  Remaining ~13,000→60,000 iters: ~2-3 hours\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPU2u2RfD0AL",
        "outputId": "1416d9ea-6cb7-480a-8167-495adf9d158a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ FIXED CONFIG\n",
            "==================================================\n",
            "  grad_accum:  4  (was 32 → 8× fewer micro-batches!)\n",
            "  Effective batch: 128 sequences\n",
            "  Tokens/update:   16,384\n",
            "  Total tokens:    983,040,000 (~1.0B)\n",
            "\n",
            "  ⏱️  Remaining ~13,000→60,000 iters: ~2-3 hours\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 12: Pre-Training Loop                                            ║\n",
        "# ║                                                                         ║\n",
        "# ║  From your notes (page 19-21):                                          ║\n",
        "# ║                                                                         ║\n",
        "# ║  Pre-train SLM:                                                         ║\n",
        "# ║  > For each iteration, choose x, y                                      ║\n",
        "# ║  > Pass x through the model to get logits                               ║\n",
        "# ║  > Compute loss between logits & y (cross-entropy)                      ║\n",
        "# ║  > Backpropagation loss                                                 ║\n",
        "# ║  > Accumulate gradients till we reach gradient_accumulation steps       ║\n",
        "# ║  > Update parameters: θᵢ₊₁ = θᵢ - α × ∂L/∂W                         ║\n",
        "# ║  > Update LR                                                            ║\n",
        "# ║  > Evaluate and save best                                               ║\n",
        "# ║                                                                         ║\n",
        "# ║  Gradient Accumulation (pages 19-20):                                   ║\n",
        "# ║  Want batch=1024, but GPU fits batch=32                                 ║\n",
        "# ║  → accumulate 32 micro-batches, THEN update                            ║\n",
        "# ║  1. optimizer.zero_grad()                                               ║\n",
        "# ║  2. Micro-batch 1: loss₁/32, backward() → grad = gradient₁/32         ║\n",
        "# ║  3. Micro-batch 2: loss₂/32, backward() → grad += gradient₂/32        ║\n",
        "# ║  ...                                                                    ║\n",
        "# ║  32. Micro-batch 32: → grad = (grad₁+...+grad₃₂)/32                  ║\n",
        "# ║  5. clip gradients, optimizer.step(), zero_grad()                       ║\n",
        "# ║                                                                         ║\n",
        "# ║  Mixed Precision (page 19):                                             ║\n",
        "# ║  matrix multiplications → float16/bfloat16                              ║\n",
        "# ║  softmax, cross entropy → float32                                       ║\n",
        "# ║  weight updates → float32                                               ║\n",
        "# ║                                                                         ║\n",
        "# ║  ⏱️ Estimated time: ~6-8 hours on A100                                 ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "# ── Setup Optimizer (AdamW — from your notes page 21) ──\n",
        "# \"We use ADAMW optimizer\"\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=train_config.learning_rate,\n",
        "    betas=(train_config.beta1, train_config.beta2),\n",
        "    eps=train_config.eps,\n",
        "    weight_decay=train_config.weight_decay,\n",
        ")\n",
        "\n",
        "# ── Mixed Precision Scaler ──\n",
        "# Only needed for float16 (bfloat16 doesn't need scaler)\n",
        "use_scaler = (dtype == torch.float16)\n",
        "scaler = GradScaler(\"cuda\", enabled=use_scaler)\n",
        "\n",
        "# ── Training State ──\n",
        "best_val_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_steps_log = []\n",
        "val_steps_log = []\n",
        "\n",
        "# ── Check for existing checkpoint (resume support) ──\n",
        "start_iter = 0\n",
        "ckpt_path = MODEL_DIR / \"latest_checkpoint.pt\"\n",
        "if ckpt_path.exists():\n",
        "    print(\"📂 Found existing checkpoint! Resuming...\")\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_iter = checkpoint['iter'] + 1\n",
        "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "    train_losses = checkpoint.get('train_losses', [])\n",
        "    val_losses = checkpoint.get('val_losses', [])\n",
        "    train_steps_log = checkpoint.get('train_steps_log', [])\n",
        "    val_steps_log = checkpoint.get('val_steps_log', [])\n",
        "    print(f\"   Resuming from iteration {start_iter}, best val loss: {best_val_loss:.4f}\")\n",
        "else:\n",
        "    print(\"🆕 Starting fresh training run\")\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 🔥 MAIN TRAINING LOOP\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"  🚀 TRAINING GEMMA 3 270M — {train_config.max_iters:,} iterations\")\n",
        "print(f\"  Effective batch: {train_config.batch_size * train_config.gradient_accumulation_steps:,} sequences\")\n",
        "print(f\"  Tokens per update: {train_config.batch_size * train_config.gradient_accumulation_steps * train_config.block_size:,}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "model.train()\n",
        "t0 = time.time()\n",
        "running_loss = 0.0\n",
        "\n",
        "for iter_num in range(start_iter, train_config.max_iters):\n",
        "\n",
        "    # ── Update learning rate ──\n",
        "    lr = get_lr(iter_num)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # ── Evaluation & Checkpointing ──\n",
        "    if iter_num % train_config.eval_interval == 0 and iter_num > 0:\n",
        "        losses = estimate_loss(model)\n",
        "        train_losses.append(losses['train'])\n",
        "        val_losses.append(losses['val'])\n",
        "        val_steps_log.append(iter_num)\n",
        "\n",
        "        print(f\"  📊 Step {iter_num:>6,} | train loss: {losses['train']:.4f} | val loss: {losses['val']:.4f} | lr: {lr:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'iter': iter_num,\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'config': model_config,\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "                'train_steps_log': train_steps_log,\n",
        "                'val_steps_log': val_steps_log,\n",
        "            }, str(MODEL_DIR / \"best_model.pt\"))\n",
        "            print(f\"  💾 New best model saved! (val loss: {best_val_loss:.4f})\")\n",
        "\n",
        "    # ── Save periodic checkpoint (for resume) ──\n",
        "    if iter_num % train_config.save_every == 0 and iter_num > 0:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'iter': iter_num,\n",
        "            'best_val_loss': best_val_loss,\n",
        "            'config': model_config,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'train_steps_log': train_steps_log,\n",
        "            'val_steps_log': val_steps_log,\n",
        "        }, str(ckpt_path))\n",
        "\n",
        "    # ══════════════════════════════════════════════════════\n",
        "    # GRADIENT ACCUMULATION LOOP (from your notes page 20)\n",
        "    # ══════════════════════════════════════════════════════\n",
        "    # \"Step 0: optimizer.zero_grad(). All .grad buffers are zero\"\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    accum_loss = 0.0\n",
        "\n",
        "    for micro_step in range(train_config.gradient_accumulation_steps):\n",
        "        # Get micro-batch\n",
        "        x, y = get_batch(\"train\")\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        # \"matrix multiplications → float16, softmax → float32\" (page 19)\n",
        "        with autocast(device_type=\"cuda\", dtype=dtype):\n",
        "            logits, loss = model(x, y)\n",
        "            # Scale loss by accumulation steps\n",
        "            # \"scale to loss/32, backward()\" (page 20)\n",
        "            loss = loss / train_config.gradient_accumulation_steps\n",
        "\n",
        "        # Backward pass — gradients accumulate automatically\n",
        "        # \"Now .grad = (gradient₁ + ... + gradient₃₂) / 32\" (page 20)\n",
        "        scaler.scale(loss).backward()\n",
        "        accum_loss += loss.item()\n",
        "\n",
        "    # ── Gradient clipping (page 20): \"clip gradients if you like\" ──\n",
        "    if train_config.gradient_clip_norm > 0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "            model.parameters(), train_config.gradient_clip_norm\n",
        "        )\n",
        "\n",
        "    # ── Optimizer step: update weights ──\n",
        "    # \"optimizer.step() updates weights once, using exactly the\n",
        "    #  gradient of 1024 examples\" (page 20)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # ── Logging ──\n",
        "    running_loss += accum_loss\n",
        "    train_steps_log.append(iter_num)\n",
        "\n",
        "    if iter_num % train_config.log_interval == 0 and iter_num > 0:\n",
        "        avg_loss = running_loss / train_config.log_interval\n",
        "        elapsed = time.time() - t0\n",
        "        tokens_per_sec = (train_config.log_interval *\n",
        "                         train_config.batch_size *\n",
        "                         train_config.gradient_accumulation_steps *\n",
        "                         train_config.block_size) / elapsed\n",
        "\n",
        "        print(f\"  Step {iter_num:>6,} | loss: {avg_loss:.4f} | lr: {lr:.6f} | \"\n",
        "              f\"{tokens_per_sec:,.0f} tok/s | {elapsed:.1f}s\")\n",
        "\n",
        "        running_loss = 0.0\n",
        "        t0 = time.time()\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 🏁 TRAINING COMPLETE\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "\n",
        "# Final evaluation\n",
        "final_losses = estimate_loss(model)\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"  🏁 TRAINING COMPLETE!\")\n",
        "print(f\"  Final train loss: {final_losses['train']:.4f} (perplexity: {math.exp(final_losses['train']):.2f})\")\n",
        "print(f\"  Final val loss:   {final_losses['val']:.4f} (perplexity: {math.exp(final_losses['val']):.2f})\")\n",
        "print(f\"  Best val loss:    {best_val_loss:.4f}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Save final model\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'iter': train_config.max_iters,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'final_train_loss': final_losses['train'],\n",
        "    'final_val_loss': final_losses['val'],\n",
        "    'config': model_config,\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses,\n",
        "    'train_steps_log': train_steps_log,\n",
        "    'val_steps_log': val_steps_log,\n",
        "}, str(MODEL_DIR / \"final_model.pt\"))\n",
        "print(f\"💾 Final model saved to: {MODEL_DIR / 'final_model.pt'}\")\n",
        "\n",
        "# ── Plot training curves ──\n",
        "if len(val_losses) > 1:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(val_steps_log[:len(train_losses)], train_losses, label='Train Loss', alpha=0.8)\n",
        "    plt.plot(val_steps_log[:len(val_losses)], val_losses, label='Val Loss', alpha=0.8)\n",
        "    plt.xlabel('Training Steps')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Gemma 3 270M — Training & Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(str(RESULTS_DIR / 'loss_curves.png'), dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"📊 Loss curves saved to: {RESULTS_DIR / 'loss_curves.png'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p-hpFooBDcWN",
        "outputId": "e68ffbb0-c1f5-4a8b-8f18-c3c6c6bb8756"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found existing checkpoint! Resuming...\n",
            "   Resuming from iteration 13001, best val loss: 1.7845\n",
            "\n",
            "============================================================\n",
            "  🚀 TRAINING GEMMA 3 270M — 60,000 iterations\n",
            "  Effective batch: 128 sequences\n",
            "  Tokens per update: 16,384\n",
            "============================================================\n",
            "\n",
            "  Step 13,010 | loss: 1.7706 | lr: 0.000095 | 39,075 tok/s | 4.2s\n",
            "  Step 13,020 | loss: 1.8064 | lr: 0.000095 | 38,914 tok/s | 4.2s\n",
            "  Step 13,030 | loss: 1.8356 | lr: 0.000095 | 38,707 tok/s | 4.2s\n",
            "  Step 13,040 | loss: 1.8289 | lr: 0.000095 | 38,980 tok/s | 4.2s\n",
            "  Step 13,050 | loss: 1.8424 | lr: 0.000095 | 38,741 tok/s | 4.2s\n",
            "  Step 13,060 | loss: 1.8146 | lr: 0.000095 | 38,883 tok/s | 4.2s\n",
            "  Step 13,070 | loss: 1.8279 | lr: 0.000095 | 38,346 tok/s | 4.3s\n",
            "  Step 13,080 | loss: 1.8558 | lr: 0.000095 | 39,113 tok/s | 4.2s\n",
            "  Step 13,090 | loss: 1.8564 | lr: 0.000095 | 38,707 tok/s | 4.2s\n",
            "  Step 13,100 | loss: 1.8134 | lr: 0.000095 | 38,702 tok/s | 4.2s\n",
            "  Step 13,110 | loss: 1.8481 | lr: 0.000095 | 39,411 tok/s | 4.2s\n",
            "  Step 13,120 | loss: 1.8566 | lr: 0.000095 | 38,698 tok/s | 4.2s\n",
            "  Step 13,130 | loss: 1.8466 | lr: 0.000095 | 38,941 tok/s | 4.2s\n",
            "  Step 13,140 | loss: 1.8419 | lr: 0.000095 | 38,762 tok/s | 4.2s\n",
            "  Step 13,150 | loss: 1.8363 | lr: 0.000095 | 39,024 tok/s | 4.2s\n",
            "  Step 13,160 | loss: 1.8353 | lr: 0.000095 | 39,548 tok/s | 4.1s\n",
            "  Step 13,170 | loss: 1.8275 | lr: 0.000095 | 39,263 tok/s | 4.2s\n",
            "  Step 13,180 | loss: 1.8372 | lr: 0.000095 | 39,100 tok/s | 4.2s\n",
            "  Step 13,190 | loss: 1.8464 | lr: 0.000095 | 38,931 tok/s | 4.2s\n",
            "  Step 13,200 | loss: 1.8241 | lr: 0.000095 | 38,788 tok/s | 4.2s\n",
            "  Step 13,210 | loss: 1.8482 | lr: 0.000095 | 38,711 tok/s | 4.2s\n",
            "  Step 13,220 | loss: 1.8307 | lr: 0.000095 | 39,141 tok/s | 4.2s\n",
            "  Step 13,230 | loss: 1.8401 | lr: 0.000095 | 39,544 tok/s | 4.1s\n",
            "  Step 13,240 | loss: 1.8631 | lr: 0.000095 | 38,300 tok/s | 4.3s\n",
            "  Step 13,250 | loss: 1.8394 | lr: 0.000095 | 39,459 tok/s | 4.2s\n",
            "  Step 13,260 | loss: 1.8336 | lr: 0.000095 | 39,603 tok/s | 4.1s\n",
            "  Step 13,270 | loss: 1.8469 | lr: 0.000095 | 39,048 tok/s | 4.2s\n",
            "  Step 13,280 | loss: 1.8556 | lr: 0.000095 | 39,292 tok/s | 4.2s\n",
            "  Step 13,290 | loss: 1.8654 | lr: 0.000095 | 38,963 tok/s | 4.2s\n",
            "  Step 13,300 | loss: 1.8416 | lr: 0.000095 | 39,254 tok/s | 4.2s\n",
            "  Step 13,310 | loss: 1.8559 | lr: 0.000095 | 39,172 tok/s | 4.2s\n",
            "  Step 13,320 | loss: 1.8330 | lr: 0.000095 | 39,149 tok/s | 4.2s\n",
            "  Step 13,330 | loss: 1.8400 | lr: 0.000095 | 39,005 tok/s | 4.2s\n",
            "  Step 13,340 | loss: 1.8585 | lr: 0.000095 | 39,546 tok/s | 4.1s\n",
            "  Step 13,350 | loss: 1.8526 | lr: 0.000095 | 38,511 tok/s | 4.3s\n",
            "  Step 13,360 | loss: 1.8390 | lr: 0.000095 | 38,422 tok/s | 4.3s\n",
            "  Step 13,370 | loss: 1.8611 | lr: 0.000095 | 39,006 tok/s | 4.2s\n",
            "  Step 13,380 | loss: 1.8569 | lr: 0.000095 | 39,286 tok/s | 4.2s\n",
            "  Step 13,390 | loss: 1.8506 | lr: 0.000095 | 38,706 tok/s | 4.2s\n",
            "  Step 13,400 | loss: 1.8362 | lr: 0.000095 | 39,712 tok/s | 4.1s\n",
            "  Step 13,410 | loss: 1.8455 | lr: 0.000095 | 39,054 tok/s | 4.2s\n",
            "  Step 13,420 | loss: 1.8423 | lr: 0.000095 | 39,488 tok/s | 4.1s\n",
            "  Step 13,430 | loss: 1.8553 | lr: 0.000095 | 39,336 tok/s | 4.2s\n",
            "  Step 13,440 | loss: 1.8567 | lr: 0.000095 | 39,523 tok/s | 4.1s\n",
            "  Step 13,450 | loss: 1.8405 | lr: 0.000095 | 39,670 tok/s | 4.1s\n",
            "  Step 13,460 | loss: 1.8544 | lr: 0.000095 | 38,442 tok/s | 4.3s\n",
            "  Step 13,470 | loss: 1.8494 | lr: 0.000095 | 39,408 tok/s | 4.2s\n",
            "  Step 13,480 | loss: 1.8481 | lr: 0.000095 | 39,422 tok/s | 4.2s\n",
            "  Step 13,490 | loss: 1.8562 | lr: 0.000095 | 39,650 tok/s | 4.1s\n",
            "  📊 Step 13,500 | train loss: 1.8364 | val loss: 1.8661 | lr: 0.000095\n",
            "  Step 13,500 | loss: 1.8501 | lr: 0.000095 | 15,877 tok/s | 10.3s\n",
            "  Step 13,510 | loss: 1.8434 | lr: 0.000095 | 37,589 tok/s | 4.4s\n",
            "  Step 13,520 | loss: 1.8492 | lr: 0.000095 | 38,755 tok/s | 4.2s\n",
            "  Step 13,530 | loss: 1.8701 | lr: 0.000095 | 39,291 tok/s | 4.2s\n",
            "  Step 13,540 | loss: 1.8317 | lr: 0.000095 | 38,726 tok/s | 4.2s\n",
            "  Step 13,550 | loss: 1.8388 | lr: 0.000095 | 39,258 tok/s | 4.2s\n",
            "  Step 13,560 | loss: 1.8437 | lr: 0.000095 | 38,848 tok/s | 4.2s\n",
            "  Step 13,570 | loss: 1.8649 | lr: 0.000095 | 38,121 tok/s | 4.3s\n",
            "  Step 13,580 | loss: 1.8507 | lr: 0.000095 | 37,325 tok/s | 4.4s\n",
            "  Step 13,590 | loss: 1.8345 | lr: 0.000095 | 37,207 tok/s | 4.4s\n",
            "  Step 13,600 | loss: 1.8479 | lr: 0.000095 | 37,218 tok/s | 4.4s\n",
            "  Step 13,610 | loss: 1.8400 | lr: 0.000095 | 38,492 tok/s | 4.3s\n",
            "  Step 13,620 | loss: 1.8594 | lr: 0.000095 | 37,216 tok/s | 4.4s\n",
            "  Step 13,630 | loss: 1.8348 | lr: 0.000095 | 37,274 tok/s | 4.4s\n",
            "  Step 13,640 | loss: 1.8490 | lr: 0.000095 | 38,222 tok/s | 4.3s\n",
            "  Step 13,650 | loss: 1.8412 | lr: 0.000095 | 38,160 tok/s | 4.3s\n",
            "  Step 13,660 | loss: 1.8512 | lr: 0.000095 | 38,660 tok/s | 4.2s\n",
            "  Step 13,670 | loss: 1.8419 | lr: 0.000095 | 38,433 tok/s | 4.3s\n",
            "  Step 13,680 | loss: 1.8537 | lr: 0.000095 | 38,528 tok/s | 4.3s\n",
            "  Step 13,690 | loss: 1.8477 | lr: 0.000095 | 38,401 tok/s | 4.3s\n",
            "  Step 13,700 | loss: 1.8750 | lr: 0.000094 | 39,231 tok/s | 4.2s\n",
            "  Step 13,710 | loss: 1.8469 | lr: 0.000094 | 38,864 tok/s | 4.2s\n",
            "  Step 13,720 | loss: 1.8404 | lr: 0.000094 | 38,484 tok/s | 4.3s\n",
            "  Step 13,730 | loss: 1.8530 | lr: 0.000094 | 38,846 tok/s | 4.2s\n",
            "  Step 13,740 | loss: 1.8545 | lr: 0.000094 | 37,868 tok/s | 4.3s\n",
            "  Step 13,750 | loss: 1.8394 | lr: 0.000094 | 38,896 tok/s | 4.2s\n",
            "  Step 13,760 | loss: 1.8468 | lr: 0.000094 | 38,824 tok/s | 4.2s\n",
            "  Step 13,770 | loss: 1.8541 | lr: 0.000094 | 39,092 tok/s | 4.2s\n",
            "  Step 13,780 | loss: 1.8504 | lr: 0.000094 | 38,947 tok/s | 4.2s\n",
            "  Step 13,790 | loss: 1.8409 | lr: 0.000094 | 39,029 tok/s | 4.2s\n",
            "  Step 13,800 | loss: 1.8653 | lr: 0.000094 | 39,311 tok/s | 4.2s\n",
            "  Step 13,810 | loss: 1.8465 | lr: 0.000094 | 38,949 tok/s | 4.2s\n",
            "  Step 13,820 | loss: 1.8304 | lr: 0.000094 | 39,056 tok/s | 4.2s\n",
            "  Step 13,830 | loss: 1.8315 | lr: 0.000094 | 39,494 tok/s | 4.1s\n",
            "  Step 13,840 | loss: 1.8511 | lr: 0.000094 | 40,072 tok/s | 4.1s\n",
            "  Step 13,850 | loss: 1.8465 | lr: 0.000094 | 39,290 tok/s | 4.2s\n",
            "  Step 13,860 | loss: 1.8377 | lr: 0.000094 | 39,587 tok/s | 4.1s\n",
            "  Step 13,870 | loss: 1.8475 | lr: 0.000094 | 40,221 tok/s | 4.1s\n",
            "  Step 13,880 | loss: 1.8658 | lr: 0.000094 | 40,003 tok/s | 4.1s\n",
            "  Step 13,890 | loss: 1.8591 | lr: 0.000094 | 38,919 tok/s | 4.2s\n",
            "  Step 13,900 | loss: 1.8706 | lr: 0.000094 | 39,674 tok/s | 4.1s\n",
            "  Step 13,910 | loss: 1.8317 | lr: 0.000094 | 39,899 tok/s | 4.1s\n",
            "  Step 13,920 | loss: 1.8551 | lr: 0.000094 | 39,776 tok/s | 4.1s\n",
            "  Step 13,930 | loss: 1.8342 | lr: 0.000094 | 39,918 tok/s | 4.1s\n",
            "  Step 13,940 | loss: 1.8577 | lr: 0.000094 | 39,831 tok/s | 4.1s\n",
            "  Step 13,950 | loss: 1.8616 | lr: 0.000094 | 38,796 tok/s | 4.2s\n",
            "  Step 13,960 | loss: 1.8281 | lr: 0.000094 | 39,104 tok/s | 4.2s\n",
            "  Step 13,970 | loss: 1.8415 | lr: 0.000094 | 38,738 tok/s | 4.2s\n",
            "  Step 13,980 | loss: 1.8427 | lr: 0.000094 | 38,178 tok/s | 4.3s\n",
            "  Step 13,990 | loss: 1.8467 | lr: 0.000094 | 39,309 tok/s | 4.2s\n",
            "  📊 Step 14,000 | train loss: 1.8472 | val loss: 1.8546 | lr: 0.000094\n",
            "  Step 14,000 | loss: 1.8354 | lr: 0.000094 | 15,777 tok/s | 10.4s\n",
            "  Step 14,010 | loss: 1.8361 | lr: 0.000094 | 38,748 tok/s | 4.2s\n",
            "  Step 14,020 | loss: 1.8528 | lr: 0.000094 | 38,928 tok/s | 4.2s\n",
            "  Step 14,030 | loss: 1.8330 | lr: 0.000094 | 38,872 tok/s | 4.2s\n",
            "  Step 14,040 | loss: 1.8555 | lr: 0.000094 | 38,677 tok/s | 4.2s\n",
            "  Step 14,050 | loss: 1.8465 | lr: 0.000094 | 37,953 tok/s | 4.3s\n",
            "  Step 14,060 | loss: 1.8634 | lr: 0.000094 | 38,353 tok/s | 4.3s\n",
            "  Step 14,070 | loss: 1.8551 | lr: 0.000094 | 37,983 tok/s | 4.3s\n",
            "  Step 14,080 | loss: 1.8333 | lr: 0.000094 | 37,411 tok/s | 4.4s\n",
            "  Step 14,090 | loss: 1.8526 | lr: 0.000094 | 38,629 tok/s | 4.2s\n",
            "  Step 14,100 | loss: 1.8439 | lr: 0.000094 | 38,766 tok/s | 4.2s\n",
            "  Step 14,110 | loss: 1.8550 | lr: 0.000094 | 37,854 tok/s | 4.3s\n",
            "  Step 14,120 | loss: 1.8516 | lr: 0.000094 | 37,382 tok/s | 4.4s\n",
            "  Step 14,130 | loss: 1.8546 | lr: 0.000094 | 38,157 tok/s | 4.3s\n",
            "  Step 14,140 | loss: 1.8520 | lr: 0.000094 | 37,817 tok/s | 4.3s\n",
            "  Step 14,150 | loss: 1.8519 | lr: 0.000094 | 38,315 tok/s | 4.3s\n",
            "  Step 14,160 | loss: 1.8463 | lr: 0.000094 | 39,004 tok/s | 4.2s\n",
            "  Step 14,170 | loss: 1.8525 | lr: 0.000094 | 38,445 tok/s | 4.3s\n",
            "  Step 14,180 | loss: 1.8391 | lr: 0.000094 | 39,320 tok/s | 4.2s\n",
            "  Step 14,190 | loss: 1.8430 | lr: 0.000094 | 39,599 tok/s | 4.1s\n",
            "  Step 14,200 | loss: 1.8601 | lr: 0.000094 | 39,464 tok/s | 4.2s\n",
            "  Step 14,210 | loss: 1.8489 | lr: 0.000094 | 40,024 tok/s | 4.1s\n",
            "  Step 14,220 | loss: 1.8343 | lr: 0.000094 | 38,511 tok/s | 4.3s\n",
            "  Step 14,230 | loss: 1.8297 | lr: 0.000094 | 39,480 tok/s | 4.1s\n",
            "  Step 14,240 | loss: 1.8357 | lr: 0.000094 | 39,809 tok/s | 4.1s\n",
            "  Step 14,250 | loss: 1.8418 | lr: 0.000094 | 39,133 tok/s | 4.2s\n",
            "  Step 14,260 | loss: 1.8344 | lr: 0.000094 | 38,483 tok/s | 4.3s\n",
            "  Step 14,270 | loss: 1.8302 | lr: 0.000094 | 38,806 tok/s | 4.2s\n",
            "  Step 14,280 | loss: 1.8421 | lr: 0.000094 | 38,930 tok/s | 4.2s\n",
            "  Step 14,290 | loss: 1.8454 | lr: 0.000094 | 38,512 tok/s | 4.3s\n",
            "  Step 14,300 | loss: 1.8282 | lr: 0.000094 | 39,034 tok/s | 4.2s\n",
            "  Step 14,310 | loss: 1.8489 | lr: 0.000094 | 38,330 tok/s | 4.3s\n",
            "  Step 14,320 | loss: 1.8560 | lr: 0.000094 | 38,999 tok/s | 4.2s\n",
            "  Step 14,330 | loss: 1.8436 | lr: 0.000094 | 39,040 tok/s | 4.2s\n",
            "  Step 14,340 | loss: 1.8386 | lr: 0.000094 | 38,322 tok/s | 4.3s\n",
            "  Step 14,350 | loss: 1.8790 | lr: 0.000094 | 38,906 tok/s | 4.2s\n",
            "  Step 14,360 | loss: 1.8569 | lr: 0.000094 | 39,048 tok/s | 4.2s\n",
            "  Step 14,370 | loss: 1.8173 | lr: 0.000094 | 38,524 tok/s | 4.3s\n",
            "  Step 14,380 | loss: 1.8388 | lr: 0.000094 | 38,538 tok/s | 4.3s\n",
            "  Step 14,390 | loss: 1.8482 | lr: 0.000094 | 38,660 tok/s | 4.2s\n",
            "  Step 14,400 | loss: 1.8610 | lr: 0.000094 | 38,361 tok/s | 4.3s\n",
            "  Step 14,410 | loss: 1.8492 | lr: 0.000094 | 38,489 tok/s | 4.3s\n",
            "  Step 14,420 | loss: 1.8463 | lr: 0.000094 | 39,176 tok/s | 4.2s\n",
            "  Step 14,430 | loss: 1.8209 | lr: 0.000094 | 38,031 tok/s | 4.3s\n",
            "  Step 14,440 | loss: 1.8512 | lr: 0.000094 | 39,236 tok/s | 4.2s\n",
            "  Step 14,450 | loss: 1.8569 | lr: 0.000094 | 38,906 tok/s | 4.2s\n",
            "  Step 14,460 | loss: 1.8334 | lr: 0.000094 | 38,070 tok/s | 4.3s\n",
            "  Step 14,470 | loss: 1.8701 | lr: 0.000094 | 38,516 tok/s | 4.3s\n",
            "  Step 14,480 | loss: 1.8556 | lr: 0.000094 | 38,848 tok/s | 4.2s\n",
            "  Step 14,490 | loss: 1.8634 | lr: 0.000094 | 38,463 tok/s | 4.3s\n",
            "  📊 Step 14,500 | train loss: 1.8543 | val loss: 1.8754 | lr: 0.000094\n",
            "  Step 14,500 | loss: 1.8517 | lr: 0.000094 | 15,679 tok/s | 10.4s\n",
            "  Step 14,510 | loss: 1.8589 | lr: 0.000094 | 38,863 tok/s | 4.2s\n",
            "  Step 14,520 | loss: 1.8519 | lr: 0.000094 | 39,474 tok/s | 4.2s\n",
            "  Step 14,530 | loss: 1.8551 | lr: 0.000094 | 38,422 tok/s | 4.3s\n",
            "  Step 14,540 | loss: 1.8529 | lr: 0.000094 | 39,066 tok/s | 4.2s\n",
            "  Step 14,550 | loss: 1.8346 | lr: 0.000094 | 38,981 tok/s | 4.2s\n",
            "  Step 14,560 | loss: 1.8354 | lr: 0.000094 | 38,085 tok/s | 4.3s\n",
            "  Step 14,570 | loss: 1.8454 | lr: 0.000094 | 38,194 tok/s | 4.3s\n",
            "  Step 14,580 | loss: 1.8571 | lr: 0.000094 | 38,896 tok/s | 4.2s\n",
            "  Step 14,590 | loss: 1.8498 | lr: 0.000094 | 38,271 tok/s | 4.3s\n",
            "  Step 14,600 | loss: 1.8499 | lr: 0.000094 | 38,259 tok/s | 4.3s\n",
            "  Step 14,610 | loss: 1.8450 | lr: 0.000094 | 38,611 tok/s | 4.2s\n",
            "  Step 14,620 | loss: 1.8636 | lr: 0.000094 | 37,974 tok/s | 4.3s\n",
            "  Step 14,630 | loss: 1.8551 | lr: 0.000094 | 38,305 tok/s | 4.3s\n",
            "  Step 14,640 | loss: 1.8439 | lr: 0.000094 | 37,825 tok/s | 4.3s\n",
            "  Step 14,650 | loss: 1.8383 | lr: 0.000094 | 38,836 tok/s | 4.2s\n",
            "  Step 14,660 | loss: 1.8548 | lr: 0.000094 | 39,288 tok/s | 4.2s\n",
            "  Step 14,670 | loss: 1.8406 | lr: 0.000094 | 38,594 tok/s | 4.2s\n",
            "  Step 14,680 | loss: 1.8399 | lr: 0.000094 | 37,562 tok/s | 4.4s\n",
            "  Step 14,690 | loss: 1.8663 | lr: 0.000094 | 38,627 tok/s | 4.2s\n",
            "  Step 14,700 | loss: 1.8529 | lr: 0.000094 | 38,825 tok/s | 4.2s\n",
            "  Step 14,710 | loss: 1.8605 | lr: 0.000094 | 38,439 tok/s | 4.3s\n",
            "  Step 14,720 | loss: 1.8391 | lr: 0.000094 | 39,035 tok/s | 4.2s\n",
            "  Step 14,730 | loss: 1.8679 | lr: 0.000094 | 38,710 tok/s | 4.2s\n",
            "  Step 14,740 | loss: 1.8545 | lr: 0.000094 | 38,670 tok/s | 4.2s\n",
            "  Step 14,750 | loss: 1.8444 | lr: 0.000094 | 38,542 tok/s | 4.3s\n",
            "  Step 14,760 | loss: 1.8459 | lr: 0.000094 | 38,597 tok/s | 4.2s\n",
            "  Step 14,770 | loss: 1.8360 | lr: 0.000094 | 38,719 tok/s | 4.2s\n",
            "  Step 14,780 | loss: 1.8573 | lr: 0.000094 | 39,163 tok/s | 4.2s\n",
            "  Step 14,790 | loss: 1.8549 | lr: 0.000094 | 38,806 tok/s | 4.2s\n",
            "  Step 14,800 | loss: 1.8396 | lr: 0.000094 | 38,578 tok/s | 4.2s\n",
            "  Step 14,810 | loss: 1.8473 | lr: 0.000094 | 39,822 tok/s | 4.1s\n",
            "  Step 14,820 | loss: 1.8563 | lr: 0.000094 | 39,432 tok/s | 4.2s\n",
            "  Step 14,830 | loss: 1.8579 | lr: 0.000094 | 39,491 tok/s | 4.1s\n",
            "  Step 14,840 | loss: 1.8367 | lr: 0.000094 | 39,275 tok/s | 4.2s\n",
            "  Step 14,850 | loss: 1.8560 | lr: 0.000094 | 38,558 tok/s | 4.2s\n",
            "  Step 14,860 | loss: 1.8411 | lr: 0.000093 | 39,257 tok/s | 4.2s\n",
            "  Step 14,870 | loss: 1.8563 | lr: 0.000093 | 38,760 tok/s | 4.2s\n",
            "  Step 14,880 | loss: 1.8605 | lr: 0.000093 | 38,696 tok/s | 4.2s\n",
            "  Step 14,890 | loss: 1.8395 | lr: 0.000093 | 39,274 tok/s | 4.2s\n",
            "  Step 14,900 | loss: 1.8467 | lr: 0.000093 | 39,095 tok/s | 4.2s\n",
            "  Step 14,910 | loss: 1.8565 | lr: 0.000093 | 38,619 tok/s | 4.2s\n",
            "  Step 14,920 | loss: 1.8514 | lr: 0.000093 | 39,584 tok/s | 4.1s\n",
            "  Step 14,930 | loss: 1.8578 | lr: 0.000093 | 39,282 tok/s | 4.2s\n",
            "  Step 14,940 | loss: 1.8519 | lr: 0.000093 | 38,246 tok/s | 4.3s\n",
            "  Step 14,950 | loss: 1.8421 | lr: 0.000093 | 39,237 tok/s | 4.2s\n",
            "  Step 14,960 | loss: 1.8368 | lr: 0.000093 | 39,298 tok/s | 4.2s\n",
            "  Step 14,970 | loss: 1.8456 | lr: 0.000093 | 38,686 tok/s | 4.2s\n",
            "  Step 14,980 | loss: 1.8403 | lr: 0.000093 | 39,115 tok/s | 4.2s\n",
            "  Step 14,990 | loss: 1.8569 | lr: 0.000093 | 38,685 tok/s | 4.2s\n",
            "  📊 Step 15,000 | train loss: 1.8376 | val loss: 1.8662 | lr: 0.000093\n",
            "  Step 15,000 | loss: 1.8320 | lr: 0.000093 | 15,646 tok/s | 10.5s\n",
            "  Step 15,010 | loss: 1.8215 | lr: 0.000093 | 38,753 tok/s | 4.2s\n",
            "  Step 15,020 | loss: 1.8288 | lr: 0.000093 | 38,556 tok/s | 4.2s\n",
            "  Step 15,030 | loss: 1.8248 | lr: 0.000093 | 38,544 tok/s | 4.3s\n",
            "  Step 15,040 | loss: 1.8547 | lr: 0.000093 | 38,671 tok/s | 4.2s\n",
            "  Step 15,050 | loss: 1.8484 | lr: 0.000093 | 39,237 tok/s | 4.2s\n",
            "  Step 15,060 | loss: 1.8654 | lr: 0.000093 | 39,262 tok/s | 4.2s\n",
            "  Step 15,070 | loss: 1.8441 | lr: 0.000093 | 38,672 tok/s | 4.2s\n",
            "  Step 15,080 | loss: 1.8568 | lr: 0.000093 | 38,357 tok/s | 4.3s\n",
            "  Step 15,090 | loss: 1.8288 | lr: 0.000093 | 38,747 tok/s | 4.2s\n",
            "  Step 15,100 | loss: 1.8308 | lr: 0.000093 | 38,123 tok/s | 4.3s\n",
            "  Step 15,110 | loss: 1.8471 | lr: 0.000093 | 38,919 tok/s | 4.2s\n",
            "  Step 15,120 | loss: 1.8809 | lr: 0.000093 | 39,569 tok/s | 4.1s\n",
            "  Step 15,130 | loss: 1.8541 | lr: 0.000093 | 38,083 tok/s | 4.3s\n",
            "  Step 15,140 | loss: 1.8401 | lr: 0.000093 | 38,806 tok/s | 4.2s\n",
            "  Step 15,150 | loss: 1.8281 | lr: 0.000093 | 39,161 tok/s | 4.2s\n",
            "  Step 15,160 | loss: 1.8600 | lr: 0.000093 | 38,085 tok/s | 4.3s\n",
            "  Step 15,170 | loss: 1.8166 | lr: 0.000093 | 37,246 tok/s | 4.4s\n",
            "  Step 15,180 | loss: 1.8402 | lr: 0.000093 | 37,716 tok/s | 4.3s\n",
            "  Step 15,190 | loss: 1.8484 | lr: 0.000093 | 36,957 tok/s | 4.4s\n",
            "  Step 15,200 | loss: 1.8553 | lr: 0.000093 | 38,714 tok/s | 4.2s\n",
            "  Step 15,210 | loss: 1.8183 | lr: 0.000093 | 38,743 tok/s | 4.2s\n",
            "  Step 15,220 | loss: 1.8475 | lr: 0.000093 | 38,721 tok/s | 4.2s\n",
            "  Step 15,230 | loss: 1.8574 | lr: 0.000093 | 39,469 tok/s | 4.2s\n",
            "  Step 15,240 | loss: 1.8441 | lr: 0.000093 | 38,704 tok/s | 4.2s\n",
            "  Step 15,250 | loss: 1.8449 | lr: 0.000093 | 39,283 tok/s | 4.2s\n",
            "  Step 15,260 | loss: 1.8540 | lr: 0.000093 | 39,334 tok/s | 4.2s\n",
            "  Step 15,270 | loss: 1.8550 | lr: 0.000093 | 39,349 tok/s | 4.2s\n",
            "  Step 15,280 | loss: 1.8653 | lr: 0.000093 | 39,082 tok/s | 4.2s\n",
            "  Step 15,290 | loss: 1.8445 | lr: 0.000093 | 38,850 tok/s | 4.2s\n",
            "  Step 15,300 | loss: 1.8465 | lr: 0.000093 | 39,063 tok/s | 4.2s\n",
            "  Step 15,310 | loss: 1.8361 | lr: 0.000093 | 38,358 tok/s | 4.3s\n",
            "  Step 15,320 | loss: 1.8360 | lr: 0.000093 | 38,347 tok/s | 4.3s\n",
            "  Step 15,330 | loss: 1.8337 | lr: 0.000093 | 38,069 tok/s | 4.3s\n",
            "  Step 15,340 | loss: 1.8341 | lr: 0.000093 | 38,789 tok/s | 4.2s\n",
            "  Step 15,350 | loss: 1.8394 | lr: 0.000093 | 39,128 tok/s | 4.2s\n",
            "  Step 15,360 | loss: 1.8503 | lr: 0.000093 | 38,407 tok/s | 4.3s\n",
            "  Step 15,370 | loss: 1.8803 | lr: 0.000093 | 39,106 tok/s | 4.2s\n",
            "  Step 15,380 | loss: 1.8448 | lr: 0.000093 | 39,613 tok/s | 4.1s\n",
            "  Step 15,390 | loss: 1.8500 | lr: 0.000093 | 39,460 tok/s | 4.2s\n",
            "  Step 15,400 | loss: 1.8566 | lr: 0.000093 | 38,804 tok/s | 4.2s\n",
            "  Step 15,410 | loss: 1.8447 | lr: 0.000093 | 39,750 tok/s | 4.1s\n",
            "  Step 15,420 | loss: 1.8348 | lr: 0.000093 | 38,701 tok/s | 4.2s\n",
            "  Step 15,430 | loss: 1.8626 | lr: 0.000093 | 39,711 tok/s | 4.1s\n",
            "  Step 15,440 | loss: 1.8274 | lr: 0.000093 | 39,415 tok/s | 4.2s\n",
            "  Step 15,450 | loss: 1.8636 | lr: 0.000093 | 39,867 tok/s | 4.1s\n",
            "  Step 15,460 | loss: 1.8703 | lr: 0.000093 | 39,674 tok/s | 4.1s\n",
            "  Step 15,470 | loss: 1.8430 | lr: 0.000093 | 39,511 tok/s | 4.1s\n",
            "  Step 15,480 | loss: 1.8431 | lr: 0.000093 | 39,268 tok/s | 4.2s\n",
            "  Step 15,490 | loss: 1.8374 | lr: 0.000093 | 39,495 tok/s | 4.1s\n",
            "  📊 Step 15,500 | train loss: 1.8500 | val loss: 1.8337 | lr: 0.000093\n",
            "  Step 15,500 | loss: 1.8578 | lr: 0.000093 | 15,901 tok/s | 10.3s\n",
            "  Step 15,510 | loss: 1.8236 | lr: 0.000093 | 38,888 tok/s | 4.2s\n",
            "  Step 15,520 | loss: 1.8486 | lr: 0.000093 | 39,504 tok/s | 4.1s\n",
            "  Step 15,530 | loss: 1.8355 | lr: 0.000093 | 38,758 tok/s | 4.2s\n",
            "  Step 15,540 | loss: 1.8335 | lr: 0.000093 | 38,408 tok/s | 4.3s\n",
            "  Step 15,550 | loss: 1.8425 | lr: 0.000093 | 38,393 tok/s | 4.3s\n",
            "  Step 15,560 | loss: 1.8335 | lr: 0.000093 | 39,605 tok/s | 4.1s\n",
            "  Step 15,570 | loss: 1.8669 | lr: 0.000093 | 39,817 tok/s | 4.1s\n",
            "  Step 15,580 | loss: 1.8446 | lr: 0.000093 | 38,330 tok/s | 4.3s\n",
            "  Step 15,590 | loss: 1.8426 | lr: 0.000093 | 37,027 tok/s | 4.4s\n",
            "  Step 15,600 | loss: 1.8480 | lr: 0.000093 | 38,031 tok/s | 4.3s\n",
            "  Step 15,610 | loss: 1.8487 | lr: 0.000093 | 38,118 tok/s | 4.3s\n",
            "  Step 15,620 | loss: 1.8355 | lr: 0.000093 | 38,257 tok/s | 4.3s\n",
            "  Step 15,630 | loss: 1.8550 | lr: 0.000093 | 38,133 tok/s | 4.3s\n",
            "  Step 15,640 | loss: 1.8398 | lr: 0.000093 | 37,063 tok/s | 4.4s\n",
            "  Step 15,650 | loss: 1.8354 | lr: 0.000093 | 38,288 tok/s | 4.3s\n",
            "  Step 15,660 | loss: 1.8272 | lr: 0.000093 | 37,535 tok/s | 4.4s\n",
            "  Step 15,670 | loss: 1.8559 | lr: 0.000093 | 36,953 tok/s | 4.4s\n",
            "  Step 15,680 | loss: 1.8264 | lr: 0.000093 | 38,062 tok/s | 4.3s\n",
            "  Step 15,690 | loss: 1.8512 | lr: 0.000093 | 39,011 tok/s | 4.2s\n",
            "  Step 15,700 | loss: 1.8420 | lr: 0.000093 | 39,320 tok/s | 4.2s\n",
            "  Step 15,710 | loss: 1.8700 | lr: 0.000093 | 38,783 tok/s | 4.2s\n",
            "  Step 15,720 | loss: 1.8589 | lr: 0.000093 | 38,805 tok/s | 4.2s\n",
            "  Step 15,730 | loss: 1.8451 | lr: 0.000093 | 39,191 tok/s | 4.2s\n",
            "  Step 15,740 | loss: 1.8512 | lr: 0.000093 | 39,386 tok/s | 4.2s\n",
            "  Step 15,750 | loss: 1.8621 | lr: 0.000093 | 39,465 tok/s | 4.2s\n",
            "  Step 15,760 | loss: 1.8315 | lr: 0.000093 | 38,611 tok/s | 4.2s\n",
            "  Step 15,770 | loss: 1.8244 | lr: 0.000093 | 38,802 tok/s | 4.2s\n",
            "  Step 15,780 | loss: 1.8479 | lr: 0.000093 | 37,905 tok/s | 4.3s\n",
            "  Step 15,790 | loss: 1.8549 | lr: 0.000093 | 38,990 tok/s | 4.2s\n",
            "  Step 15,800 | loss: 1.8357 | lr: 0.000093 | 38,336 tok/s | 4.3s\n",
            "  Step 15,810 | loss: 1.8315 | lr: 0.000093 | 39,051 tok/s | 4.2s\n",
            "  Step 15,820 | loss: 1.8517 | lr: 0.000093 | 39,478 tok/s | 4.2s\n",
            "  Step 15,830 | loss: 1.8407 | lr: 0.000093 | 40,122 tok/s | 4.1s\n",
            "  Step 15,840 | loss: 1.8727 | lr: 0.000093 | 38,894 tok/s | 4.2s\n",
            "  Step 15,850 | loss: 1.8315 | lr: 0.000093 | 39,720 tok/s | 4.1s\n",
            "  Step 15,860 | loss: 1.8516 | lr: 0.000093 | 39,627 tok/s | 4.1s\n",
            "  Step 15,870 | loss: 1.8387 | lr: 0.000093 | 38,448 tok/s | 4.3s\n",
            "  Step 15,880 | loss: 1.8245 | lr: 0.000093 | 39,830 tok/s | 4.1s\n",
            "  Step 15,890 | loss: 1.8557 | lr: 0.000093 | 39,546 tok/s | 4.1s\n",
            "  Step 15,900 | loss: 1.8422 | lr: 0.000093 | 39,629 tok/s | 4.1s\n",
            "  Step 15,910 | loss: 1.8601 | lr: 0.000093 | 39,976 tok/s | 4.1s\n",
            "  Step 15,920 | loss: 1.8516 | lr: 0.000093 | 39,800 tok/s | 4.1s\n",
            "  Step 15,930 | loss: 1.8380 | lr: 0.000093 | 39,476 tok/s | 4.2s\n",
            "  Step 15,940 | loss: 1.8437 | lr: 0.000092 | 39,771 tok/s | 4.1s\n",
            "  Step 15,950 | loss: 1.8476 | lr: 0.000092 | 39,993 tok/s | 4.1s\n",
            "  Step 15,960 | loss: 1.8403 | lr: 0.000092 | 39,896 tok/s | 4.1s\n",
            "  Step 15,970 | loss: 1.8329 | lr: 0.000092 | 39,323 tok/s | 4.2s\n",
            "  Step 15,980 | loss: 1.8385 | lr: 0.000092 | 39,983 tok/s | 4.1s\n",
            "  Step 15,990 | loss: 1.8426 | lr: 0.000092 | 38,698 tok/s | 4.2s\n",
            "  📊 Step 16,000 | train loss: 1.8433 | val loss: 1.8590 | lr: 0.000092\n",
            "  Step 16,000 | loss: 1.8344 | lr: 0.000092 | 15,637 tok/s | 10.5s\n",
            "  Step 16,010 | loss: 1.8186 | lr: 0.000092 | 38,697 tok/s | 4.2s\n",
            "  Step 16,020 | loss: 1.8400 | lr: 0.000092 | 38,706 tok/s | 4.2s\n",
            "  Step 16,030 | loss: 1.8602 | lr: 0.000092 | 38,767 tok/s | 4.2s\n",
            "  Step 16,040 | loss: 1.8527 | lr: 0.000092 | 39,194 tok/s | 4.2s\n",
            "  Step 16,050 | loss: 1.8518 | lr: 0.000092 | 38,934 tok/s | 4.2s\n",
            "  Step 16,060 | loss: 1.8401 | lr: 0.000092 | 38,346 tok/s | 4.3s\n",
            "  Step 16,070 | loss: 1.8564 | lr: 0.000092 | 39,258 tok/s | 4.2s\n",
            "  Step 16,080 | loss: 1.8335 | lr: 0.000092 | 38,414 tok/s | 4.3s\n",
            "  Step 16,090 | loss: 1.8414 | lr: 0.000092 | 38,599 tok/s | 4.2s\n",
            "  Step 16,100 | loss: 1.8314 | lr: 0.000092 | 37,519 tok/s | 4.4s\n",
            "  Step 16,110 | loss: 1.8222 | lr: 0.000092 | 37,259 tok/s | 4.4s\n",
            "  Step 16,120 | loss: 1.8293 | lr: 0.000092 | 37,706 tok/s | 4.3s\n",
            "  Step 16,130 | loss: 1.8258 | lr: 0.000092 | 38,077 tok/s | 4.3s\n",
            "  Step 16,140 | loss: 1.8450 | lr: 0.000092 | 38,543 tok/s | 4.3s\n",
            "  Step 16,150 | loss: 1.8483 | lr: 0.000092 | 38,147 tok/s | 4.3s\n",
            "  Step 16,160 | loss: 1.8392 | lr: 0.000092 | 38,659 tok/s | 4.2s\n",
            "  Step 16,170 | loss: 1.8531 | lr: 0.000092 | 38,942 tok/s | 4.2s\n",
            "  Step 16,180 | loss: 1.8335 | lr: 0.000092 | 38,173 tok/s | 4.3s\n",
            "  Step 16,190 | loss: 1.8466 | lr: 0.000092 | 38,888 tok/s | 4.2s\n",
            "  Step 16,200 | loss: 1.8383 | lr: 0.000092 | 39,470 tok/s | 4.2s\n",
            "  Step 16,210 | loss: 1.8216 | lr: 0.000092 | 39,582 tok/s | 4.1s\n",
            "  Step 16,220 | loss: 1.8416 | lr: 0.000092 | 39,780 tok/s | 4.1s\n",
            "  Step 16,230 | loss: 1.8279 | lr: 0.000092 | 39,553 tok/s | 4.1s\n",
            "  Step 16,240 | loss: 1.8463 | lr: 0.000092 | 38,764 tok/s | 4.2s\n",
            "  Step 16,250 | loss: 1.8243 | lr: 0.000092 | 39,802 tok/s | 4.1s\n",
            "  Step 16,260 | loss: 1.8418 | lr: 0.000092 | 39,633 tok/s | 4.1s\n",
            "  Step 16,270 | loss: 1.8330 | lr: 0.000092 | 39,589 tok/s | 4.1s\n",
            "  Step 16,280 | loss: 1.8477 | lr: 0.000092 | 39,692 tok/s | 4.1s\n",
            "  Step 16,290 | loss: 1.8583 | lr: 0.000092 | 39,337 tok/s | 4.2s\n",
            "  Step 16,300 | loss: 1.8351 | lr: 0.000092 | 39,769 tok/s | 4.1s\n",
            "  Step 16,310 | loss: 1.8448 | lr: 0.000092 | 39,768 tok/s | 4.1s\n",
            "  Step 16,320 | loss: 1.8520 | lr: 0.000092 | 39,413 tok/s | 4.2s\n",
            "  Step 16,330 | loss: 1.8190 | lr: 0.000092 | 39,284 tok/s | 4.2s\n",
            "  Step 16,340 | loss: 1.8529 | lr: 0.000092 | 39,217 tok/s | 4.2s\n",
            "  Step 16,350 | loss: 1.8650 | lr: 0.000092 | 39,637 tok/s | 4.1s\n",
            "  Step 16,360 | loss: 1.8437 | lr: 0.000092 | 39,507 tok/s | 4.1s\n",
            "  Step 16,370 | loss: 1.8438 | lr: 0.000092 | 40,403 tok/s | 4.1s\n",
            "  Step 16,380 | loss: 1.8547 | lr: 0.000092 | 38,947 tok/s | 4.2s\n",
            "  Step 16,390 | loss: 1.8492 | lr: 0.000092 | 39,047 tok/s | 4.2s\n",
            "  Step 16,400 | loss: 1.8529 | lr: 0.000092 | 39,876 tok/s | 4.1s\n",
            "  Step 16,410 | loss: 1.8214 | lr: 0.000092 | 39,497 tok/s | 4.1s\n",
            "  Step 16,420 | loss: 1.8302 | lr: 0.000092 | 38,393 tok/s | 4.3s\n",
            "  Step 16,430 | loss: 1.8391 | lr: 0.000092 | 37,890 tok/s | 4.3s\n",
            "  Step 16,440 | loss: 1.8421 | lr: 0.000092 | 38,706 tok/s | 4.2s\n",
            "  Step 16,450 | loss: 1.8388 | lr: 0.000092 | 38,543 tok/s | 4.3s\n",
            "  Step 16,460 | loss: 1.8262 | lr: 0.000092 | 38,646 tok/s | 4.2s\n",
            "  Step 16,470 | loss: 1.8593 | lr: 0.000092 | 38,337 tok/s | 4.3s\n",
            "  Step 16,480 | loss: 1.8385 | lr: 0.000092 | 39,647 tok/s | 4.1s\n",
            "  Step 16,490 | loss: 1.8325 | lr: 0.000092 | 39,526 tok/s | 4.1s\n",
            "  📊 Step 16,500 | train loss: 1.8322 | val loss: 1.8443 | lr: 0.000092\n",
            "  Step 16,500 | loss: 1.8492 | lr: 0.000092 | 15,766 tok/s | 10.4s\n",
            "  Step 16,510 | loss: 1.8412 | lr: 0.000092 | 38,467 tok/s | 4.3s\n",
            "  Step 16,520 | loss: 1.8600 | lr: 0.000092 | 38,227 tok/s | 4.3s\n",
            "  Step 16,530 | loss: 1.8519 | lr: 0.000092 | 38,782 tok/s | 4.2s\n",
            "  Step 16,540 | loss: 1.8503 | lr: 0.000092 | 38,503 tok/s | 4.3s\n",
            "  Step 16,550 | loss: 1.8679 | lr: 0.000092 | 38,580 tok/s | 4.2s\n",
            "  Step 16,560 | loss: 1.8515 | lr: 0.000092 | 39,378 tok/s | 4.2s\n",
            "  Step 16,570 | loss: 1.8584 | lr: 0.000092 | 38,387 tok/s | 4.3s\n",
            "  Step 16,580 | loss: 1.8318 | lr: 0.000092 | 37,786 tok/s | 4.3s\n",
            "  Step 16,590 | loss: 1.8385 | lr: 0.000092 | 38,984 tok/s | 4.2s\n",
            "  Step 16,600 | loss: 1.8358 | lr: 0.000092 | 37,497 tok/s | 4.4s\n",
            "  Step 16,610 | loss: 1.8462 | lr: 0.000092 | 38,402 tok/s | 4.3s\n",
            "  Step 16,620 | loss: 1.8389 | lr: 0.000092 | 38,550 tok/s | 4.3s\n",
            "  Step 16,630 | loss: 1.8261 | lr: 0.000092 | 38,529 tok/s | 4.3s\n",
            "  Step 16,640 | loss: 1.8255 | lr: 0.000092 | 39,331 tok/s | 4.2s\n",
            "  Step 16,650 | loss: 1.8498 | lr: 0.000092 | 39,473 tok/s | 4.2s\n",
            "  Step 16,660 | loss: 1.8243 | lr: 0.000092 | 37,583 tok/s | 4.4s\n",
            "  Step 16,670 | loss: 1.8484 | lr: 0.000092 | 38,551 tok/s | 4.2s\n",
            "  Step 16,680 | loss: 1.8332 | lr: 0.000092 | 39,571 tok/s | 4.1s\n",
            "  Step 16,690 | loss: 1.8404 | lr: 0.000092 | 38,939 tok/s | 4.2s\n",
            "  Step 16,700 | loss: 1.8583 | lr: 0.000092 | 39,591 tok/s | 4.1s\n",
            "  Step 16,710 | loss: 1.8374 | lr: 0.000092 | 38,807 tok/s | 4.2s\n",
            "  Step 16,720 | loss: 1.8245 | lr: 0.000092 | 38,370 tok/s | 4.3s\n",
            "  Step 16,730 | loss: 1.8189 | lr: 0.000092 | 39,984 tok/s | 4.1s\n",
            "  Step 16,740 | loss: 1.8597 | lr: 0.000092 | 39,882 tok/s | 4.1s\n",
            "  Step 16,750 | loss: 1.8393 | lr: 0.000092 | 38,675 tok/s | 4.2s\n",
            "  Step 16,760 | loss: 1.8698 | lr: 0.000092 | 39,163 tok/s | 4.2s\n",
            "  Step 16,770 | loss: 1.8453 | lr: 0.000092 | 39,446 tok/s | 4.2s\n",
            "  Step 16,780 | loss: 1.8355 | lr: 0.000092 | 38,991 tok/s | 4.2s\n",
            "  Step 16,790 | loss: 1.8344 | lr: 0.000092 | 39,284 tok/s | 4.2s\n",
            "  Step 16,800 | loss: 1.8428 | lr: 0.000092 | 39,370 tok/s | 4.2s\n",
            "  Step 16,810 | loss: 1.8587 | lr: 0.000092 | 38,472 tok/s | 4.3s\n",
            "  Step 16,820 | loss: 1.8604 | lr: 0.000092 | 39,307 tok/s | 4.2s\n",
            "  Step 16,830 | loss: 1.8417 | lr: 0.000092 | 39,553 tok/s | 4.1s\n",
            "  Step 16,840 | loss: 1.8535 | lr: 0.000092 | 39,481 tok/s | 4.1s\n",
            "  Step 16,850 | loss: 1.8128 | lr: 0.000092 | 38,657 tok/s | 4.2s\n",
            "  Step 16,860 | loss: 1.8289 | lr: 0.000092 | 38,598 tok/s | 4.2s\n",
            "  Step 16,870 | loss: 1.8413 | lr: 0.000092 | 39,452 tok/s | 4.2s\n",
            "  Step 16,880 | loss: 1.8591 | lr: 0.000092 | 39,958 tok/s | 4.1s\n",
            "  Step 16,890 | loss: 1.8460 | lr: 0.000092 | 38,732 tok/s | 4.2s\n",
            "  Step 16,900 | loss: 1.8423 | lr: 0.000092 | 39,207 tok/s | 4.2s\n",
            "  Step 16,910 | loss: 1.8368 | lr: 0.000092 | 38,933 tok/s | 4.2s\n",
            "  Step 16,920 | loss: 1.8308 | lr: 0.000092 | 39,306 tok/s | 4.2s\n",
            "  Step 16,930 | loss: 1.8567 | lr: 0.000092 | 39,392 tok/s | 4.2s\n",
            "  Step 16,940 | loss: 1.8445 | lr: 0.000092 | 39,614 tok/s | 4.1s\n",
            "  Step 16,950 | loss: 1.8326 | lr: 0.000092 | 38,797 tok/s | 4.2s\n",
            "  Step 16,960 | loss: 1.8263 | lr: 0.000092 | 39,908 tok/s | 4.1s\n",
            "  Step 16,970 | loss: 1.8347 | lr: 0.000091 | 40,348 tok/s | 4.1s\n",
            "  Step 16,980 | loss: 1.8094 | lr: 0.000091 | 39,464 tok/s | 4.2s\n",
            "  Step 16,990 | loss: 1.8568 | lr: 0.000091 | 39,210 tok/s | 4.2s\n",
            "  📊 Step 17,000 | train loss: 1.8340 | val loss: 1.8483 | lr: 0.000091\n",
            "  Step 17,000 | loss: 1.8438 | lr: 0.000091 | 15,809 tok/s | 10.4s\n",
            "  Step 17,010 | loss: 1.8356 | lr: 0.000091 | 38,258 tok/s | 4.3s\n",
            "  Step 17,020 | loss: 1.8539 | lr: 0.000091 | 39,677 tok/s | 4.1s\n",
            "  Step 17,030 | loss: 1.8491 | lr: 0.000091 | 38,381 tok/s | 4.3s\n",
            "  Step 17,040 | loss: 1.8478 | lr: 0.000091 | 38,862 tok/s | 4.2s\n",
            "  Step 17,050 | loss: 1.8364 | lr: 0.000091 | 38,931 tok/s | 4.2s\n",
            "  Step 17,060 | loss: 1.8393 | lr: 0.000091 | 38,424 tok/s | 4.3s\n",
            "  Step 17,070 | loss: 1.8368 | lr: 0.000091 | 38,449 tok/s | 4.3s\n",
            "  Step 17,080 | loss: 1.8430 | lr: 0.000091 | 39,140 tok/s | 4.2s\n",
            "  Step 17,090 | loss: 1.8384 | lr: 0.000091 | 39,031 tok/s | 4.2s\n",
            "  Step 17,100 | loss: 1.8346 | lr: 0.000091 | 38,965 tok/s | 4.2s\n",
            "  Step 17,110 | loss: 1.8394 | lr: 0.000091 | 38,330 tok/s | 4.3s\n",
            "  Step 17,120 | loss: 1.8315 | lr: 0.000091 | 38,833 tok/s | 4.2s\n",
            "  Step 17,130 | loss: 1.8416 | lr: 0.000091 | 38,740 tok/s | 4.2s\n",
            "  Step 17,140 | loss: 1.8436 | lr: 0.000091 | 38,796 tok/s | 4.2s\n",
            "  Step 17,150 | loss: 1.8471 | lr: 0.000091 | 38,540 tok/s | 4.3s\n",
            "  Step 17,160 | loss: 1.8305 | lr: 0.000091 | 39,001 tok/s | 4.2s\n",
            "  Step 17,170 | loss: 1.8424 | lr: 0.000091 | 38,836 tok/s | 4.2s\n",
            "  Step 17,180 | loss: 1.8507 | lr: 0.000091 | 38,775 tok/s | 4.2s\n",
            "  Step 17,190 | loss: 1.8160 | lr: 0.000091 | 38,943 tok/s | 4.2s\n",
            "  Step 17,200 | loss: 1.8484 | lr: 0.000091 | 38,862 tok/s | 4.2s\n",
            "  Step 17,210 | loss: 1.8431 | lr: 0.000091 | 39,578 tok/s | 4.1s\n",
            "  Step 17,220 | loss: 1.8440 | lr: 0.000091 | 39,142 tok/s | 4.2s\n",
            "  Step 17,230 | loss: 1.8497 | lr: 0.000091 | 39,106 tok/s | 4.2s\n",
            "  Step 17,240 | loss: 1.8505 | lr: 0.000091 | 39,651 tok/s | 4.1s\n",
            "  Step 17,250 | loss: 1.8363 | lr: 0.000091 | 39,682 tok/s | 4.1s\n",
            "  Step 17,260 | loss: 1.8631 | lr: 0.000091 | 39,408 tok/s | 4.2s\n",
            "  Step 17,270 | loss: 1.8344 | lr: 0.000091 | 39,897 tok/s | 4.1s\n",
            "  Step 17,280 | loss: 1.8436 | lr: 0.000091 | 39,971 tok/s | 4.1s\n",
            "  Step 17,290 | loss: 1.8320 | lr: 0.000091 | 38,616 tok/s | 4.2s\n",
            "  Step 17,300 | loss: 1.8499 | lr: 0.000091 | 39,731 tok/s | 4.1s\n",
            "  Step 17,310 | loss: 1.8390 | lr: 0.000091 | 40,130 tok/s | 4.1s\n",
            "  Step 17,320 | loss: 1.8290 | lr: 0.000091 | 39,110 tok/s | 4.2s\n",
            "  Step 17,330 | loss: 1.8405 | lr: 0.000091 | 39,765 tok/s | 4.1s\n",
            "  Step 17,340 | loss: 1.8475 | lr: 0.000091 | 39,533 tok/s | 4.1s\n",
            "  Step 17,350 | loss: 1.8459 | lr: 0.000091 | 38,921 tok/s | 4.2s\n",
            "  Step 17,360 | loss: 1.8088 | lr: 0.000091 | 40,181 tok/s | 4.1s\n",
            "  Step 17,370 | loss: 1.8240 | lr: 0.000091 | 39,629 tok/s | 4.1s\n",
            "  Step 17,380 | loss: 1.8316 | lr: 0.000091 | 38,186 tok/s | 4.3s\n",
            "  Step 17,390 | loss: 1.8251 | lr: 0.000091 | 39,607 tok/s | 4.1s\n",
            "  Step 17,400 | loss: 1.8430 | lr: 0.000091 | 39,332 tok/s | 4.2s\n",
            "  Step 17,410 | loss: 1.8319 | lr: 0.000091 | 39,127 tok/s | 4.2s\n",
            "  Step 17,420 | loss: 1.8302 | lr: 0.000091 | 39,444 tok/s | 4.2s\n",
            "  Step 17,430 | loss: 1.8336 | lr: 0.000091 | 40,217 tok/s | 4.1s\n",
            "  Step 17,440 | loss: 1.8527 | lr: 0.000091 | 38,786 tok/s | 4.2s\n",
            "  Step 17,450 | loss: 1.8245 | lr: 0.000091 | 38,568 tok/s | 4.2s\n",
            "  Step 17,460 | loss: 1.8484 | lr: 0.000091 | 38,732 tok/s | 4.2s\n",
            "  Step 17,470 | loss: 1.8268 | lr: 0.000091 | 38,416 tok/s | 4.3s\n",
            "  Step 17,480 | loss: 1.8416 | lr: 0.000091 | 39,652 tok/s | 4.1s\n",
            "  Step 17,490 | loss: 1.8358 | lr: 0.000091 | 39,325 tok/s | 4.2s\n",
            "  📊 Step 17,500 | train loss: 1.8348 | val loss: 1.8511 | lr: 0.000091\n",
            "  Step 17,500 | loss: 1.8185 | lr: 0.000091 | 15,707 tok/s | 10.4s\n",
            "  Step 17,510 | loss: 1.8383 | lr: 0.000091 | 38,083 tok/s | 4.3s\n",
            "  Step 17,520 | loss: 1.8387 | lr: 0.000091 | 38,689 tok/s | 4.2s\n",
            "  Step 17,530 | loss: 1.8392 | lr: 0.000091 | 39,221 tok/s | 4.2s\n",
            "  Step 17,540 | loss: 1.8248 | lr: 0.000091 | 38,624 tok/s | 4.2s\n",
            "  Step 17,550 | loss: 1.8452 | lr: 0.000091 | 40,050 tok/s | 4.1s\n",
            "  Step 17,560 | loss: 1.8311 | lr: 0.000091 | 39,728 tok/s | 4.1s\n",
            "  Step 17,570 | loss: 1.8312 | lr: 0.000091 | 39,082 tok/s | 4.2s\n",
            "  Step 17,580 | loss: 1.8359 | lr: 0.000091 | 39,926 tok/s | 4.1s\n",
            "  Step 17,590 | loss: 1.8490 | lr: 0.000091 | 39,949 tok/s | 4.1s\n",
            "  Step 17,600 | loss: 1.8473 | lr: 0.000091 | 38,883 tok/s | 4.2s\n",
            "  Step 17,610 | loss: 1.8460 | lr: 0.000091 | 38,715 tok/s | 4.2s\n",
            "  Step 17,620 | loss: 1.8419 | lr: 0.000091 | 39,697 tok/s | 4.1s\n",
            "  Step 17,630 | loss: 1.8324 | lr: 0.000091 | 38,477 tok/s | 4.3s\n",
            "  Step 17,640 | loss: 1.8217 | lr: 0.000091 | 39,473 tok/s | 4.2s\n",
            "  Step 17,650 | loss: 1.8111 | lr: 0.000091 | 38,582 tok/s | 4.2s\n",
            "  Step 17,660 | loss: 1.8392 | lr: 0.000091 | 38,517 tok/s | 4.3s\n",
            "  Step 17,670 | loss: 1.8241 | lr: 0.000091 | 38,599 tok/s | 4.2s\n",
            "  Step 17,680 | loss: 1.8297 | lr: 0.000091 | 38,145 tok/s | 4.3s\n",
            "  Step 17,690 | loss: 1.8372 | lr: 0.000091 | 38,512 tok/s | 4.3s\n",
            "  Step 17,700 | loss: 1.8388 | lr: 0.000091 | 38,724 tok/s | 4.2s\n",
            "  Step 17,710 | loss: 1.8238 | lr: 0.000091 | 39,611 tok/s | 4.1s\n",
            "  Step 17,720 | loss: 1.8407 | lr: 0.000091 | 39,191 tok/s | 4.2s\n",
            "  Step 17,730 | loss: 1.8319 | lr: 0.000091 | 39,665 tok/s | 4.1s\n",
            "  Step 17,740 | loss: 1.8328 | lr: 0.000091 | 39,698 tok/s | 4.1s\n",
            "  Step 17,750 | loss: 1.8335 | lr: 0.000091 | 38,839 tok/s | 4.2s\n",
            "  Step 17,760 | loss: 1.8160 | lr: 0.000091 | 39,713 tok/s | 4.1s\n",
            "  Step 17,770 | loss: 1.8301 | lr: 0.000091 | 39,162 tok/s | 4.2s\n",
            "  Step 17,780 | loss: 1.8251 | lr: 0.000091 | 39,269 tok/s | 4.2s\n",
            "  Step 17,790 | loss: 1.8356 | lr: 0.000091 | 39,872 tok/s | 4.1s\n",
            "  Step 17,800 | loss: 1.8608 | lr: 0.000091 | 38,949 tok/s | 4.2s\n",
            "  Step 17,810 | loss: 1.8450 | lr: 0.000091 | 38,854 tok/s | 4.2s\n",
            "  Step 17,820 | loss: 1.8160 | lr: 0.000091 | 38,951 tok/s | 4.2s\n",
            "  Step 17,830 | loss: 1.8286 | lr: 0.000091 | 39,766 tok/s | 4.1s\n",
            "  Step 17,840 | loss: 1.8361 | lr: 0.000091 | 39,013 tok/s | 4.2s\n",
            "  Step 17,850 | loss: 1.8536 | lr: 0.000091 | 39,782 tok/s | 4.1s\n",
            "  Step 17,860 | loss: 1.8485 | lr: 0.000091 | 39,574 tok/s | 4.1s\n",
            "  Step 17,870 | loss: 1.8193 | lr: 0.000091 | 39,167 tok/s | 4.2s\n",
            "  Step 17,880 | loss: 1.8362 | lr: 0.000091 | 39,659 tok/s | 4.1s\n",
            "  Step 17,890 | loss: 1.8416 | lr: 0.000091 | 39,207 tok/s | 4.2s\n",
            "  Step 17,900 | loss: 1.8486 | lr: 0.000091 | 39,485 tok/s | 4.1s\n",
            "  Step 17,910 | loss: 1.8562 | lr: 0.000091 | 39,206 tok/s | 4.2s\n",
            "  Step 17,920 | loss: 1.8325 | lr: 0.000091 | 39,140 tok/s | 4.2s\n",
            "  Step 17,930 | loss: 1.8310 | lr: 0.000091 | 39,236 tok/s | 4.2s\n",
            "  Step 17,940 | loss: 1.8212 | lr: 0.000091 | 39,869 tok/s | 4.1s\n",
            "  Step 17,950 | loss: 1.8234 | lr: 0.000090 | 38,949 tok/s | 4.2s\n",
            "  Step 17,960 | loss: 1.8302 | lr: 0.000090 | 39,153 tok/s | 4.2s\n",
            "  Step 17,970 | loss: 1.8408 | lr: 0.000090 | 39,269 tok/s | 4.2s\n",
            "  Step 17,980 | loss: 1.8430 | lr: 0.000090 | 39,210 tok/s | 4.2s\n",
            "  Step 17,990 | loss: 1.8291 | lr: 0.000090 | 38,964 tok/s | 4.2s\n",
            "  📊 Step 18,000 | train loss: 1.8583 | val loss: 1.8296 | lr: 0.000090\n",
            "  Step 18,000 | loss: 1.8221 | lr: 0.000090 | 15,798 tok/s | 10.4s\n",
            "  Step 18,010 | loss: 1.8391 | lr: 0.000090 | 38,760 tok/s | 4.2s\n",
            "  Step 18,020 | loss: 1.8061 | lr: 0.000090 | 39,827 tok/s | 4.1s\n",
            "  Step 18,030 | loss: 1.8480 | lr: 0.000090 | 39,431 tok/s | 4.2s\n",
            "  Step 18,040 | loss: 1.8305 | lr: 0.000090 | 39,449 tok/s | 4.2s\n",
            "  Step 18,050 | loss: 1.8222 | lr: 0.000090 | 39,026 tok/s | 4.2s\n",
            "  Step 18,060 | loss: 1.8151 | lr: 0.000090 | 39,546 tok/s | 4.1s\n",
            "  Step 18,070 | loss: 1.8129 | lr: 0.000090 | 40,070 tok/s | 4.1s\n",
            "  Step 18,080 | loss: 1.8527 | lr: 0.000090 | 38,090 tok/s | 4.3s\n",
            "  Step 18,090 | loss: 1.8444 | lr: 0.000090 | 38,802 tok/s | 4.2s\n",
            "  Step 18,100 | loss: 1.8518 | lr: 0.000090 | 38,109 tok/s | 4.3s\n",
            "  Step 18,110 | loss: 1.8187 | lr: 0.000090 | 38,629 tok/s | 4.2s\n",
            "  Step 18,120 | loss: 1.8411 | lr: 0.000090 | 39,430 tok/s | 4.2s\n",
            "  Step 18,130 | loss: 1.8276 | lr: 0.000090 | 39,785 tok/s | 4.1s\n",
            "  Step 18,140 | loss: 1.8257 | lr: 0.000090 | 38,883 tok/s | 4.2s\n",
            "  Step 18,150 | loss: 1.8361 | lr: 0.000090 | 39,219 tok/s | 4.2s\n",
            "  Step 18,160 | loss: 1.8324 | lr: 0.000090 | 39,669 tok/s | 4.1s\n",
            "  Step 18,170 | loss: 1.8319 | lr: 0.000090 | 38,188 tok/s | 4.3s\n",
            "  Step 18,180 | loss: 1.8180 | lr: 0.000090 | 39,270 tok/s | 4.2s\n",
            "  Step 18,190 | loss: 1.8061 | lr: 0.000090 | 39,687 tok/s | 4.1s\n",
            "  Step 18,200 | loss: 1.8349 | lr: 0.000090 | 37,755 tok/s | 4.3s\n",
            "  Step 18,210 | loss: 1.8163 | lr: 0.000090 | 38,975 tok/s | 4.2s\n",
            "  Step 18,220 | loss: 1.8246 | lr: 0.000090 | 39,594 tok/s | 4.1s\n",
            "  Step 18,230 | loss: 1.8214 | lr: 0.000090 | 38,864 tok/s | 4.2s\n",
            "  Step 18,240 | loss: 1.8374 | lr: 0.000090 | 39,570 tok/s | 4.1s\n",
            "  Step 18,250 | loss: 1.8186 | lr: 0.000090 | 39,671 tok/s | 4.1s\n",
            "  Step 18,260 | loss: 1.8410 | lr: 0.000090 | 39,503 tok/s | 4.1s\n",
            "  Step 18,270 | loss: 1.8157 | lr: 0.000090 | 39,809 tok/s | 4.1s\n",
            "  Step 18,280 | loss: 1.8229 | lr: 0.000090 | 39,936 tok/s | 4.1s\n",
            "  Step 18,290 | loss: 1.8515 | lr: 0.000090 | 39,818 tok/s | 4.1s\n",
            "  Step 18,300 | loss: 1.8142 | lr: 0.000090 | 39,693 tok/s | 4.1s\n",
            "  Step 18,310 | loss: 1.8292 | lr: 0.000090 | 39,551 tok/s | 4.1s\n",
            "  Step 18,320 | loss: 1.8193 | lr: 0.000090 | 39,293 tok/s | 4.2s\n",
            "  Step 18,330 | loss: 1.8361 | lr: 0.000090 | 39,719 tok/s | 4.1s\n",
            "  Step 18,340 | loss: 1.8080 | lr: 0.000090 | 39,820 tok/s | 4.1s\n",
            "  Step 18,350 | loss: 1.8226 | lr: 0.000090 | 38,626 tok/s | 4.2s\n",
            "  Step 18,360 | loss: 1.8191 | lr: 0.000090 | 39,468 tok/s | 4.2s\n",
            "  Step 18,370 | loss: 1.8095 | lr: 0.000090 | 39,757 tok/s | 4.1s\n",
            "  Step 18,380 | loss: 1.8360 | lr: 0.000090 | 38,780 tok/s | 4.2s\n",
            "  Step 18,390 | loss: 1.8328 | lr: 0.000090 | 38,725 tok/s | 4.2s\n",
            "  Step 18,400 | loss: 1.8168 | lr: 0.000090 | 38,846 tok/s | 4.2s\n",
            "  Step 18,410 | loss: 1.8177 | lr: 0.000090 | 39,137 tok/s | 4.2s\n",
            "  Step 18,420 | loss: 1.8547 | lr: 0.000090 | 39,043 tok/s | 4.2s\n",
            "  Step 18,430 | loss: 1.8637 | lr: 0.000090 | 38,972 tok/s | 4.2s\n",
            "  Step 18,440 | loss: 1.8109 | lr: 0.000090 | 38,308 tok/s | 4.3s\n",
            "  Step 18,450 | loss: 1.8063 | lr: 0.000090 | 39,134 tok/s | 4.2s\n",
            "  Step 18,460 | loss: 1.8109 | lr: 0.000090 | 38,824 tok/s | 4.2s\n",
            "  Step 18,470 | loss: 1.8426 | lr: 0.000090 | 38,836 tok/s | 4.2s\n",
            "  Step 18,480 | loss: 1.8043 | lr: 0.000090 | 38,904 tok/s | 4.2s\n",
            "  Step 18,490 | loss: 1.8298 | lr: 0.000090 | 38,634 tok/s | 4.2s\n",
            "  📊 Step 18,500 | train loss: 1.8355 | val loss: 1.8856 | lr: 0.000090\n",
            "  Step 18,500 | loss: 1.8130 | lr: 0.000090 | 15,670 tok/s | 10.5s\n",
            "  Step 18,510 | loss: 1.8162 | lr: 0.000090 | 37,782 tok/s | 4.3s\n",
            "  Step 18,520 | loss: 1.8483 | lr: 0.000090 | 38,580 tok/s | 4.2s\n",
            "  Step 18,530 | loss: 1.8281 | lr: 0.000090 | 37,936 tok/s | 4.3s\n",
            "  Step 18,540 | loss: 1.8449 | lr: 0.000090 | 38,306 tok/s | 4.3s\n",
            "  Step 18,550 | loss: 1.8105 | lr: 0.000090 | 37,713 tok/s | 4.3s\n",
            "  Step 18,560 | loss: 1.8049 | lr: 0.000090 | 38,302 tok/s | 4.3s\n",
            "  Step 18,570 | loss: 1.8406 | lr: 0.000090 | 37,830 tok/s | 4.3s\n",
            "  Step 18,580 | loss: 1.8323 | lr: 0.000090 | 37,567 tok/s | 4.4s\n",
            "  Step 18,590 | loss: 1.8258 | lr: 0.000090 | 37,940 tok/s | 4.3s\n",
            "  Step 18,600 | loss: 1.8344 | lr: 0.000090 | 36,862 tok/s | 4.4s\n",
            "  Step 18,610 | loss: 1.8177 | lr: 0.000090 | 37,905 tok/s | 4.3s\n",
            "  Step 18,620 | loss: 1.8302 | lr: 0.000090 | 38,284 tok/s | 4.3s\n",
            "  Step 18,630 | loss: 1.8446 | lr: 0.000090 | 37,298 tok/s | 4.4s\n",
            "  Step 18,640 | loss: 1.8100 | lr: 0.000090 | 38,728 tok/s | 4.2s\n",
            "  Step 18,650 | loss: 1.8303 | lr: 0.000090 | 38,168 tok/s | 4.3s\n",
            "  Step 18,660 | loss: 1.8037 | lr: 0.000090 | 39,118 tok/s | 4.2s\n",
            "  Step 18,670 | loss: 1.8216 | lr: 0.000090 | 39,042 tok/s | 4.2s\n",
            "  Step 18,680 | loss: 1.8414 | lr: 0.000090 | 38,173 tok/s | 4.3s\n",
            "  Step 18,690 | loss: 1.8386 | lr: 0.000090 | 38,127 tok/s | 4.3s\n",
            "  Step 18,700 | loss: 1.8315 | lr: 0.000090 | 39,539 tok/s | 4.1s\n",
            "  Step 18,710 | loss: 1.8196 | lr: 0.000090 | 39,703 tok/s | 4.1s\n",
            "  Step 18,720 | loss: 1.8079 | lr: 0.000090 | 39,807 tok/s | 4.1s\n",
            "  Step 18,730 | loss: 1.8181 | lr: 0.000090 | 39,473 tok/s | 4.2s\n",
            "  Step 18,740 | loss: 1.8328 | lr: 0.000090 | 39,221 tok/s | 4.2s\n",
            "  Step 18,750 | loss: 1.8268 | lr: 0.000090 | 39,368 tok/s | 4.2s\n",
            "  Step 18,760 | loss: 1.8204 | lr: 0.000090 | 38,944 tok/s | 4.2s\n",
            "  Step 18,770 | loss: 1.8247 | lr: 0.000090 | 38,973 tok/s | 4.2s\n",
            "  Step 18,780 | loss: 1.8362 | lr: 0.000090 | 39,200 tok/s | 4.2s\n",
            "  Step 18,790 | loss: 1.8265 | lr: 0.000090 | 39,751 tok/s | 4.1s\n",
            "  Step 18,800 | loss: 1.8139 | lr: 0.000090 | 39,413 tok/s | 4.2s\n",
            "  Step 18,810 | loss: 1.8293 | lr: 0.000090 | 39,247 tok/s | 4.2s\n",
            "  Step 18,820 | loss: 1.8261 | lr: 0.000090 | 39,397 tok/s | 4.2s\n",
            "  Step 18,830 | loss: 1.8400 | lr: 0.000090 | 39,810 tok/s | 4.1s\n",
            "  Step 18,840 | loss: 1.8074 | lr: 0.000090 | 39,371 tok/s | 4.2s\n",
            "  Step 18,850 | loss: 1.8211 | lr: 0.000090 | 38,602 tok/s | 4.2s\n",
            "  Step 18,860 | loss: 1.8420 | lr: 0.000090 | 39,268 tok/s | 4.2s\n",
            "  Step 18,870 | loss: 1.8409 | lr: 0.000090 | 38,808 tok/s | 4.2s\n",
            "  Step 18,880 | loss: 1.8279 | lr: 0.000090 | 39,544 tok/s | 4.1s\n",
            "  Step 18,890 | loss: 1.8297 | lr: 0.000089 | 39,740 tok/s | 4.1s\n",
            "  Step 18,900 | loss: 1.8328 | lr: 0.000089 | 38,888 tok/s | 4.2s\n",
            "  Step 18,910 | loss: 1.8187 | lr: 0.000089 | 39,530 tok/s | 4.1s\n",
            "  Step 18,920 | loss: 1.8240 | lr: 0.000089 | 38,946 tok/s | 4.2s\n",
            "  Step 18,930 | loss: 1.8060 | lr: 0.000089 | 39,312 tok/s | 4.2s\n",
            "  Step 18,940 | loss: 1.8334 | lr: 0.000089 | 39,663 tok/s | 4.1s\n",
            "  Step 18,950 | loss: 1.8136 | lr: 0.000089 | 39,279 tok/s | 4.2s\n",
            "  Step 18,960 | loss: 1.8084 | lr: 0.000089 | 39,949 tok/s | 4.1s\n",
            "  Step 18,970 | loss: 1.8271 | lr: 0.000089 | 39,618 tok/s | 4.1s\n",
            "  Step 18,980 | loss: 1.8392 | lr: 0.000089 | 39,191 tok/s | 4.2s\n",
            "  Step 18,990 | loss: 1.8267 | lr: 0.000089 | 39,919 tok/s | 4.1s\n",
            "  📊 Step 19,000 | train loss: 1.8156 | val loss: 1.8470 | lr: 0.000089\n",
            "  Step 19,000 | loss: 1.8272 | lr: 0.000089 | 15,920 tok/s | 10.3s\n",
            "  Step 19,010 | loss: 1.8309 | lr: 0.000089 | 37,378 tok/s | 4.4s\n",
            "  Step 19,020 | loss: 1.8158 | lr: 0.000089 | 39,448 tok/s | 4.2s\n",
            "  Step 19,030 | loss: 1.8526 | lr: 0.000089 | 39,667 tok/s | 4.1s\n",
            "  Step 19,040 | loss: 1.8381 | lr: 0.000089 | 40,037 tok/s | 4.1s\n",
            "  Step 19,050 | loss: 1.8150 | lr: 0.000089 | 39,047 tok/s | 4.2s\n",
            "  Step 19,060 | loss: 1.8311 | lr: 0.000089 | 39,697 tok/s | 4.1s\n",
            "  Step 19,070 | loss: 1.8283 | lr: 0.000089 | 38,940 tok/s | 4.2s\n",
            "  Step 19,080 | loss: 1.8279 | lr: 0.000089 | 38,097 tok/s | 4.3s\n",
            "  Step 19,090 | loss: 1.8312 | lr: 0.000089 | 39,154 tok/s | 4.2s\n",
            "  Step 19,100 | loss: 1.8285 | lr: 0.000089 | 39,511 tok/s | 4.1s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1675685474.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# Backward pass — gradients accumulate automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# \"Now .grad = (gradient₁ + ... + gradient₃₂) / 32\" (page 20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0maccum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 13: Load Best Model + Evaluate + Plot Curves                      ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "import math\n",
        "\n",
        "# ── Load best checkpoint ──\n",
        "print(\"📂 Loading best checkpoint...\")\n",
        "ckpt = torch.load(str(MODEL_DIR / \"best_model.pt\"), map_location=device, weights_only=False)\n",
        "model.load_state_dict(ckpt['model_state_dict'])\n",
        "\n",
        "best_iter = ckpt['iter']\n",
        "best_val = ckpt['best_val_loss']\n",
        "train_losses = ckpt.get('train_losses', [])\n",
        "val_losses = ckpt.get('val_losses', [])\n",
        "val_steps_log = ckpt.get('val_steps_log', [])\n",
        "\n",
        "print(f\"   ✅ Best model from iteration: {best_iter:,}\")\n",
        "print(f\"   ✅ Best val loss:  {best_val:.4f}\")\n",
        "print(f\"   ✅ Perplexity:     {math.exp(best_val):.2f}\")\n",
        "\n",
        "# ── Final evaluation ──\n",
        "print(f\"\\n📊 Final Evaluation:\")\n",
        "final_losses = estimate_loss(model)\n",
        "print(f\"   Train loss: {final_losses['train']:.4f}  (perplexity: {math.exp(final_losses['train']):.2f})\")\n",
        "print(f\"   Val loss:   {final_losses['val']:.4f}  (perplexity: {math.exp(final_losses['val']):.2f})\")\n",
        "\n",
        "# ── Plot loss curves ──\n",
        "if len(val_losses) > 1:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    steps = val_steps_log[:len(train_losses)]\n",
        "    axes[0].plot(steps, train_losses, label='Train Loss', alpha=0.8, linewidth=2)\n",
        "    axes[0].plot(val_steps_log[:len(val_losses)], val_losses, label='Val Loss', alpha=0.8, linewidth=2)\n",
        "    axes[0].set_xlabel('Training Steps')\n",
        "    axes[0].set_ylabel('Loss (Cross-Entropy)')\n",
        "    axes[0].set_title('Training & Validation Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Perplexity plot\n",
        "    train_ppl = [math.exp(l) for l in train_losses]\n",
        "    val_ppl = [math.exp(l) for l in val_losses]\n",
        "    axes[1].plot(steps, train_ppl, label='Train PPL', alpha=0.8, linewidth=2, color='tab:blue')\n",
        "    axes[1].plot(val_steps_log[:len(val_ppl)], val_ppl, label='Val PPL', alpha=0.8, linewidth=2, color='tab:orange')\n",
        "    axes[1].set_xlabel('Training Steps')\n",
        "    axes[1].set_ylabel('Perplexity')\n",
        "    axes[1].set_title('Perplexity (lower = better)')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    fig.suptitle('Gemma 3 270M — Pre-training on TinyStories', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(str(RESULTS_DIR / 'loss_curves.png'), dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"   📊 Saved: {RESULTS_DIR / 'loss_curves.png'}\")\n",
        "\n",
        "# ── Summary ──\n",
        "print(f\"\\n{'='*55}\")\n",
        "print(f\"  🏁 TRAINING COMPLETE!\")\n",
        "print(f\"  ─────────────────────────────────────\")\n",
        "print(f\"  Model:          Gemma 3 270M (164.6M params)\")\n",
        "print(f\"  Dataset:        TinyStories (471M tokens)\")\n",
        "print(f\"  Best iteration: {best_iter:,}\")\n",
        "print(f\"  Best val loss:  {best_val:.4f}\")\n",
        "print(f\"  Perplexity:     {math.exp(best_val):.2f}\")\n",
        "print(f\"  ─────────────────────────────────────\")\n",
        "print(f\"  🧠 Ready for inference!\")\n",
        "print(f\"{'='*55}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "Skywr22Uwgmr",
        "outputId": "38e3d9ed-99dc-42d5-e7dc-e3a4da043363"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Loading best checkpoint...\n",
            "   ✅ Best model from iteration: 13,000\n",
            "   ✅ Best val loss:  1.7845\n",
            "   ✅ Perplexity:     5.96\n",
            "\n",
            "📊 Final Evaluation:\n",
            "   Train loss: 1.7564  (perplexity: 5.79)\n",
            "   Val loss:   1.7892  (perplexity: 5.98)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   📊 Saved: /content/drive/MyDrive/Gemma3_270M_Project/results/loss_curves.png\n",
            "\n",
            "=======================================================\n",
            "  🏁 TRAINING COMPLETE!\n",
            "  ─────────────────────────────────────\n",
            "  Model:          Gemma 3 270M (164.6M params)\n",
            "  Dataset:        TinyStories (471M tokens)\n",
            "  Best iteration: 13,000\n",
            "  Best val loss:  1.7845\n",
            "  Perplexity:     5.96\n",
            "  ─────────────────────────────────────\n",
            "  🧠 Ready for inference!\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 14: Story Generation & Inference                                  ║\n",
        "# ║                                                                         ║\n",
        "# ║  From your notes (page 22):                                             ║\n",
        "# ║  Step 6: Inference                                                      ║\n",
        "# ║  1. Get logits → Index of highest value → Token ID                     ║\n",
        "# ║  2. Decode back to text                                                 ║\n",
        "# ║  3. Appended to previous inputs → produce next token                   ║\n",
        "# ║  4. Feed back to model → repeat                                        ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "model.eval()\n",
        "\n",
        "def generate_story(\n",
        "    prompt: str,\n",
        "    max_tokens: int = 300,\n",
        "    temperature: float = 0.8,\n",
        "    top_k: int = 50,\n",
        "    show_params: bool = True\n",
        ") -> str:\n",
        "    \"\"\"Generate a story from a prompt.\"\"\"\n",
        "    if show_params:\n",
        "        print(f\"   🎛️  temperature={temperature}, top_k={top_k}, max_tokens={max_tokens}\")\n",
        "\n",
        "    input_ids = torch.tensor([enc.encode_ordinary(prompt)], device=device)\n",
        "\n",
        "    with torch.no_grad(), autocast(device_type=\"cuda\", dtype=dtype):\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "        )\n",
        "\n",
        "    return enc.decode(output_ids[0].tolist())\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 📖 STORY GENERATION\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"📖 Gemma 3 270M — Story Generation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "prompts = [\n",
        "    \"Once upon a time, there was a little girl named Lily\",\n",
        "    \"The big brown dog ran to the park\",\n",
        "    \"Mom said it was time for bed, but Tommy\",\n",
        "    \"One day, a magical bird flew into the garden\",\n",
        "    \"The little boy was scared of the dark\",\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    print(f\"\\n{'─'*60}\")\n",
        "    print(f\"  📝 Story {i}\")\n",
        "    print(f\"{'─'*60}\")\n",
        "    print(f\"   Prompt: \\\"{prompt}\\\"\")\n",
        "    story = generate_story(prompt, max_tokens=250, temperature=0.8, top_k=50)\n",
        "    print(f\"\\n   {story}\")\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 🎛️ TEMPERATURE COMPARISON\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "\n",
        "print(f\"\\n\\n{'='*60}\")\n",
        "print(\"🌡️ Temperature Comparison (same prompt, different creativity)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_prompt = \"Once upon a time, there was a little cat\"\n",
        "\n",
        "for temp in [0.3, 0.7, 1.0]:\n",
        "    label = {0.3: \"Conservative (predictable)\", 0.7: \"Balanced\", 1.0: \"Creative (surprising)\"}\n",
        "    print(f\"\\n{'─'*60}\")\n",
        "    print(f\"  🌡️ Temperature = {temp} — {label[temp]}\")\n",
        "    print(f\"{'─'*60}\")\n",
        "    story = generate_story(test_prompt, max_tokens=150, temperature=temp, top_k=50)\n",
        "    print(f\"\\n   {story}\")\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 📊 MODEL CARD SUMMARY\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "\n",
        "print(f\"\\n\\n{'='*60}\")\n",
        "print(\"📋 MODEL CARD — Gemma 3 270M (TinyStories)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\"\"\n",
        "   Architecture:     Gemma 3 (custom 270M variant)\n",
        "   Parameters:       164.6M\n",
        "   Context Length:    32,768 tokens\n",
        "   Vocabulary:       50,257 (GPT-2 BPE tokenizer)\n",
        "   Layers:           18 (15 sliding + 3 full attention)\n",
        "   Embedding Dim:    640\n",
        "   Attention Heads:  4 (Multi-Query, 1 KV group)\n",
        "   Head Dimension:   256\n",
        "   FFN Hidden:       2,048 (GeGLU activation)\n",
        "\n",
        "   Training:\n",
        "   ─────────────────────────────\n",
        "   Dataset:          TinyStories (471M tokens)\n",
        "   Best Iteration:   {best_iter:,}\n",
        "   Best Val Loss:    {best_val:.4f}\n",
        "   Perplexity:       {math.exp(best_val):.2f}\n",
        "   Optimizer:        AdamW (β1=0.9, β2=0.95)\n",
        "   Learning Rate:    1e-4 → 5e-5 (cosine decay)\n",
        "   Precision:        bfloat16 mixed precision\n",
        "   Hardware:         NVIDIA A100 40GB\n",
        "\n",
        "   Key Features:\n",
        "   ─────────────────────────────\n",
        "   ✅ Sliding Window Attention (w=512)\n",
        "   ✅ Multi-Query Attention (4:1 Q:KV ratio)\n",
        "   ✅ RoPE with dual bases (10K local, 1M global)\n",
        "   ✅ QK Normalization (RMSNorm)\n",
        "   ✅ Gemma-style (1+weight) scaling in RMSNorm\n",
        "   ✅ GeGLU Feed-Forward Network\n",
        "\n",
        "   Credits: Vizuara Team - Raj\n",
        "\"\"\")\n",
        "\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  ✅ Cell 14 Done! Stories generated!\")\n",
        "print(f\"{'='*60}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMSKfp4J6raO",
        "outputId": "8db993b5-7e35-4291-ae60-a948067b912f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📖 Gemma 3 270M — Story Generation\n",
            "============================================================\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "  📝 Story 1\n",
            "────────────────────────────────────────────────────────────\n",
            "   Prompt: \"Once upon a time, there was a little girl named Lily\"\n",
            "   🎛️  temperature=0.8, top_k=50, max_tokens=250\n",
            "\n",
            "   Once upon a time, there was a little girl named Lily. She loved to play with her toys and her friends. One day, Lily's mom bought her a new doll for her birthday. Lily was so happy and said, \"Thank you, Mommy! I love my new doll.\" \n",
            "\n",
            "Lily's mom said, \"I'm sorry, I was worried about your doll. I won't let you play with it again.\" Lily was so happy to have her favorite doll back. She played with it every day and couldn't wait to use it again. \n",
            "\n",
            "One day, Lily's mom told her that she couldn't touch her doll and that she had to leave it alone. Lily felt sad to leave her doll behind, but she knew it was time for a surprise. She opened her new doll, but there was nothing to see. She felt really sad but knew that the new doll was special because it would come to her again soon. \n",
            "\n",
            "\n",
            "Lily's mom said, \"You're going to the park to play on the new doll, Lily. It's a beautiful doll that she loved to play with every day. But Lily's mom did not want to go to the park. She didn't want to go to the park. Her parents told\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "  📝 Story 2\n",
            "────────────────────────────────────────────────────────────\n",
            "   Prompt: \"The big brown dog ran to the park\"\n",
            "   🎛️  temperature=0.8, top_k=50, max_tokens=250\n",
            "\n",
            "   The big brown dog ran to the park and started to chase the bird. The bird flew in his eyes and looked for a place to hide. It was all alone, but then it saw something even more amazing. It saw a big tree and it was and it was so big that it was able to catch the bird. The bird was so happy that it flew around the tree all day long. From then on, it always followed the bird through the park, and it never forgot how it had flown to the top of the tree.Once upon a time, there was a little girl named Lily. She was very excited because she was going to the beach with her mommy and daddy. They had an extra trip to the beach.\n",
            "\n",
            "\n",
            "When they arrived at the beach, Lily's mommy and daddy decided to take a few hours of swimming. They set up a big ship on the ground.\n",
            "\n",
            "Soon, they had a big smile on their faces. The wind blew through the water and Lily could feel the waves. She was so happy to be in the water and the sun's warm air.\n",
            "\n",
            "\n",
            "The sun was getting bigger and bigger and the water was too high for her to reach. The ground was so far away, but Lily's mommy\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "  📝 Story 3\n",
            "────────────────────────────────────────────────────────────\n",
            "   Prompt: \"Mom said it was time for bed, but Tommy\"\n",
            "   🎛️  temperature=0.8, top_k=50, max_tokens=250\n",
            "\n",
            "   Mom said it was time for bed, but Tommy wasn't sure. He wanted to keep playing. His mom said, \"Tommy, it's time for bed now. You can play later.\" But Tommy didn't listen. He kept playing with the phone and made it go on the floor. \n",
            "\n",
            "But then, his mom said, \"Why don't you play and have some fun? You're both a good boy.\" Tommy replied, \"Okay mom, I won't play with the phone.\" His mom smiled, and they went to the bathroom, where Tommy was playing with his toys instead. He was happy, and he learned that he didn't play with the phone again!Once upon a time, there was an old man who lived in a small house. He was sad, but he was happy with his own voice. So he turned and jumped right into his bed. \n",
            "\n",
            "Tommy was sad. He had to take a nap. He was so tired that he fell asleep right away. He started feeling a little bit angry.\n",
            "\n",
            "\n",
            "Suddenly, his mom noticed his nose was missing. She asked him, \"What's wrong, Tommy. This is too loud. Please help me close the cabinet, okay?\"\n",
            "\n",
            "\n",
            "His mom had an idea\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "  📝 Story 4\n",
            "────────────────────────────────────────────────────────────\n",
            "   Prompt: \"One day, a magical bird flew into the garden\"\n",
            "   🎛️  temperature=0.8, top_k=50, max_tokens=250\n",
            "\n",
            "   One day, a magical bird flew into the garden. The bird said the power to not be scared, but the magical bird didn't allow it. But the bird stayed in the garden and the magical creature kept asking to fly.\n",
            "\n",
            "The bird tried again and again but the magical bird did not give up. Finally, the star said he would help and soon the bird was perched on the ground.\n",
            "\n",
            "\"Thank you,\" said the rainbow. The creature happily agreed, and the bird smiled. From that day forward, the brave little bird and the magical creature were best friends.Once upon a time, there was a little girl who loved to eat. She always asked her mom to get her some more fruit. One day, she saw something shiny and delicious. She asked her mom if it was anything she knew. Her mom said she would know when to reach her first tree. She said, â€œMum, I know it would be fun!â€ But the bird was too shy to be scared, so his mom said it must be ok. The little girl was a bit scared. She said, \"Will this tree will help me to have a friend.\"\n",
            "\n",
            "\n",
            "The bird looked at her and said, \"I think it would be a good\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "  📝 Story 5\n",
            "────────────────────────────────────────────────────────────\n",
            "   Prompt: \"The little boy was scared of the dark\"\n",
            "   🎛️  temperature=0.8, top_k=50, max_tokens=250\n",
            "\n",
            "   The little boy was scared of the dark, but he put down his magic wand and said, \"It's okay. I'm just here to keep you safe.\"\n",
            "\n",
            "The fireman thanked the little boy and said, \"Thank you for being so brave. I'm a part of your life.\" \n",
            "\n",
            "The little boy smiled and said, \"My friend, I'm so glad you're here.\"\n",
            "\n",
            "The fireman smiled and then he hugged the little boy, then he waved goodbye and drove away.\n",
            "\n",
            "The little boy was sad but he was happy that his friend was safe. He knew that he had to be brave and he would never be scared of the dark. He knew he was going back to his old home and he was very safe.Once upon a time, there was a small girl named Daisy. She was so excited to meet her new friend, she was so proud to be free. She smiled and said, \"This is great fun! Can we stay here every day?\"\n",
            "\n",
            "\n",
            "The fireman smiled and said, \"Yes, I would love to be there for you and your friend.\" So she said goodbye, and gave him a hug. \"I'm always here to encourage you!\"\n",
            "\n",
            "\n",
            "The next time Daisy and\n",
            "\n",
            "\n",
            "============================================================\n",
            "🌡️ Temperature Comparison (same prompt, different creativity)\n",
            "============================================================\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "  🌡️ Temperature = 0.3 — Conservative (predictable)\n",
            "────────────────────────────────────────────────────────────\n",
            "   🎛️  temperature=0.3, top_k=50, max_tokens=150\n",
            "\n",
            "   Once upon a time, there was a little cat named Mittens. Mittens loved to play with her toy mouse and eat cheese. One day, Mittens saw a big piece of cheese on the floor. She wanted to eat it, but she knew she had to wait until she was older. \n",
            "\n",
            "Mittens went to her friend, a cat named Mittens. Mittens was very good at eating cheese. Mittens saw Mittens and wanted to eat the cheese too. Mittens tried to run away, but Mittens was too fast. Mittens tried to catch Mittens, but she couldn't. \n",
            "\n",
            "Mittens was very sad and scared. Mittens tried to run away, but Mittens was too fast. Mittens tried to catch Mittens, but Mitt\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "  🌡️ Temperature = 0.7 — Balanced\n",
            "────────────────────────────────────────────────────────────\n",
            "   🎛️  temperature=0.7, top_k=50, max_tokens=150\n",
            "\n",
            "   Once upon a time, there was a little cat named Mittens. Mittens was very hungry and wanted to eat some food. She went outside to find some grass to eat. \n",
            "\n",
            "Mittens saw a big tree and decided to climb it. She climbed up and up until she reached the top. As she was in the tree, she saw a small bird with a broken wing. Mittens knew just what to do. She took the bird to her mom and asked for help. \n",
            "\n",
            "Her mom wanted her to help. She gave the bird some food and water. Mittens was happy and said thank you. She felt better knowing the bird might have a cozy nest for her to rest. \n",
            "\n",
            "\n",
            "When she got to the top of the tree, she saw her\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "  🌡️ Temperature = 1.0 — Creative (surprising)\n",
            "────────────────────────────────────────────────────────────\n",
            "   🎛️  temperature=1.0, top_k=50, max_tokens=150\n",
            "\n",
            "   Once upon a time, there was a little cat. It was walking in a field when it saw something shiny in the grass. It was a stone. He was so excited! It looked so delicious!\n",
            "\n",
            "The cat asked the stone if it could try. The stone was so big that it was very heavy. The cat tried and tried, but it could not do it. The cat started to cry. \n",
            "\n",
            "The cat found a small hole and decided to put the stone in the hole. The cat put the stone inside the cave. It was happy that it could do it. \n",
            "\n",
            "The cat was so grateful to the stone and watched it climb down into the hole. The fox showed it was safe. The cat couldn't get into the hole. It was too heavy\n",
            "\n",
            "\n",
            "============================================================\n",
            "📋 MODEL CARD — Gemma 3 270M (TinyStories)\n",
            "============================================================\n",
            "\n",
            "   Architecture:     Gemma 3 (custom 270M variant)\n",
            "   Parameters:       164.6M\n",
            "   Context Length:    32,768 tokens\n",
            "   Vocabulary:       50,257 (GPT-2 BPE tokenizer)\n",
            "   Layers:           18 (15 sliding + 3 full attention)\n",
            "   Embedding Dim:    640\n",
            "   Attention Heads:  4 (Multi-Query, 1 KV group)\n",
            "   Head Dimension:   256\n",
            "   FFN Hidden:       2,048 (GeGLU activation)\n",
            "   \n",
            "   Training:\n",
            "   ─────────────────────────────\n",
            "   Dataset:          TinyStories (471M tokens)\n",
            "   Best Iteration:   13,000\n",
            "   Best Val Loss:    1.7845\n",
            "   Perplexity:       5.96\n",
            "   Optimizer:        AdamW (β1=0.9, β2=0.95)\n",
            "   Learning Rate:    1e-4 → 5e-5 (cosine decay)\n",
            "   Precision:        bfloat16 mixed precision\n",
            "   Hardware:         NVIDIA A100 40GB\n",
            "   \n",
            "   Key Features:\n",
            "   ─────────────────────────────\n",
            "   ✅ Sliding Window Attention (w=512)\n",
            "   ✅ Multi-Query Attention (4:1 Q:KV ratio)\n",
            "   ✅ RoPE with dual bases (10K local, 1M global)\n",
            "   ✅ QK Normalization (RMSNorm)\n",
            "   ✅ Gemma-style (1+weight) scaling in RMSNorm\n",
            "   ✅ GeGLU Feed-Forward Network\n",
            "   \n",
            "   Credits: Vizuara Team - Raj\n",
            "\n",
            "============================================================\n",
            "  ✅ Cell 14 Done! Stories generated!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 15: Save Model & Push to HuggingFace Hub 🤗                      ║\n",
        "# ║                                                                         ║\n",
        "# ║  From your notes (page 22):                                             ║\n",
        "# ║  \"Save model to HuggingFace\"                                           ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "!pip install huggingface_hub -q\n",
        "\n",
        "from huggingface_hub import HfApi, login, create_repo\n",
        "import json, shutil\n",
        "\n",
        "# ── Step 1: Login to HuggingFace ──\n",
        "print(\"🔑 Login to HuggingFace\")\n",
        "print(\"   Go to: https://huggingface.co/settings/tokens\")\n",
        "print(\"   Create a token with WRITE access\")\n",
        "print(\"   Paste it below:\\n\")\n",
        "login()\n",
        "\n",
        "# ── Step 2: Prepare model files ──\n",
        "EXPORT_DIR = Path(\"/content/hf_export\")\n",
        "EXPORT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Save model weights (PyTorch format)\n",
        "print(\"\\n📦 Preparing model files...\")\n",
        "torch.save(model.state_dict(), str(EXPORT_DIR / \"pytorch_model.bin\"))\n",
        "print(f\"   ✅ pytorch_model.bin ({(EXPORT_DIR / 'pytorch_model.bin').stat().st_size / 1e6:.1f} MB)\")\n",
        "\n",
        "# Save config\n",
        "config_dict = {\n",
        "    \"architecture\": \"Gemma3Custom\",\n",
        "    \"vocab_size\": model_config.vocab_size,\n",
        "    \"context_length\": model_config.context_length,\n",
        "    \"emb_dim\": model_config.emb_dim,\n",
        "    \"n_layers\": model_config.n_layers,\n",
        "    \"n_heads\": model_config.n_heads,\n",
        "    \"head_dim\": model_config.head_dim,\n",
        "    \"hidden_dim\": model_config.hidden_dim,\n",
        "    \"n_kv_groups\": model_config.n_kv_groups,\n",
        "    \"qk_norm\": model_config.qk_norm,\n",
        "    \"query_pre_attn_scalar\": model_config.query_pre_attn_scalar,\n",
        "    \"rope_base\": model_config.rope_base,\n",
        "    \"rope_local_base\": model_config.rope_local_base,\n",
        "    \"sliding_window\": model_config.sliding_window,\n",
        "    \"dtype\": \"bfloat16\",\n",
        "    \"total_parameters\": sum(p.numel() for p in model.parameters()),\n",
        "}\n",
        "\n",
        "with open(EXPORT_DIR / \"config.json\", \"w\") as f:\n",
        "    json.dump(config_dict, f, indent=2)\n",
        "print(\"   ✅ config.json\")\n",
        "\n",
        "# Save training config\n",
        "train_dict = {\n",
        "    \"max_iters\": train_config.max_iters,\n",
        "    \"batch_size\": train_config.batch_size,\n",
        "    \"block_size\": train_config.block_size,\n",
        "    \"gradient_accumulation_steps\": train_config.gradient_accumulation_steps,\n",
        "    \"learning_rate\": train_config.learning_rate,\n",
        "    \"min_lr\": train_config.min_lr,\n",
        "    \"warmup_steps\": train_config.warmup_steps,\n",
        "    \"beta1\": train_config.beta1,\n",
        "    \"beta2\": train_config.beta2,\n",
        "    \"weight_decay\": train_config.weight_decay,\n",
        "    \"gradient_clip_norm\": train_config.gradient_clip_norm,\n",
        "    \"best_val_loss\": best_val,\n",
        "    \"best_iteration\": best_iter,\n",
        "    \"perplexity\": round(math.exp(best_val), 2),\n",
        "    \"dataset\": \"roneneldan/TinyStories\",\n",
        "    \"tokenizer\": \"gpt2 (tiktoken)\",\n",
        "    \"hardware\": \"NVIDIA A100 40GB\",\n",
        "}\n",
        "\n",
        "with open(EXPORT_DIR / \"training_config.json\", \"w\") as f:\n",
        "    json.dump(train_dict, f, indent=2)\n",
        "print(\"   ✅ training_config.json\")\n",
        "\n",
        "# Save layer types\n",
        "with open(EXPORT_DIR / \"layer_types.json\", \"w\") as f:\n",
        "    json.dump({\"layer_types\": model_config.layer_types}, f, indent=2)\n",
        "print(\"   ✅ layer_types.json\")\n",
        "\n",
        "# Copy loss curves\n",
        "if (RESULTS_DIR / 'loss_curves.png').exists():\n",
        "    shutil.copy(RESULTS_DIR / 'loss_curves.png', EXPORT_DIR / 'loss_curves.png')\n",
        "    print(\"   ✅ loss_curves.png\")\n",
        "\n",
        "if (RESULTS_DIR / 'lr_schedule.png').exists():\n",
        "    shutil.copy(RESULTS_DIR / 'lr_schedule.png', EXPORT_DIR / 'lr_schedule.png')\n",
        "    print(\"   ✅ lr_schedule.png\")\n",
        "\n",
        "# ── Step 3: Create Model Card (README.md) ──\n",
        "model_card = f\"\"\"---\n",
        "license: apache-2.0\n",
        "tags:\n",
        "  - gemma3\n",
        "  - language-model\n",
        "  - pre-training\n",
        "  - from-scratch\n",
        "  - tinystories\n",
        "  - transformer\n",
        "  - multi-query-attention\n",
        "  - sliding-window-attention\n",
        "  - rope\n",
        "language:\n",
        "  - en\n",
        "datasets:\n",
        "  - roneneldan/TinyStories\n",
        "metrics:\n",
        "  - perplexity\n",
        "pipeline_tag: text-generation\n",
        "---\n",
        "\n",
        "# Gemma 3 270M — Pre-trained from Scratch on TinyStories\n",
        "\n",
        "A custom implementation of the **Gemma 3 architecture** (scaled to 164.6M parameters), pre-trained from scratch on the TinyStories dataset.\n",
        "\n",
        "## 📊 Results\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| **Best Val Loss** | {best_val:.4f} |\n",
        "| **Perplexity** | {math.exp(best_val):.2f} |\n",
        "| **Best Iteration** | {best_iter:,} |\n",
        "| **Parameters** | 164.6M |\n",
        "\n",
        "![Training Loss Curves](loss_curves.png)\n",
        "\n",
        "## 🏗️ Architecture\n",
        "\n",
        "This model implements the **complete Gemma 3 architecture** with all modern innovations:\n",
        "\n",
        "| Component | Specification |\n",
        "|-----------|--------------|\n",
        "| Layers | 18 (15 sliding + 3 full attention) |\n",
        "| Embedding Dim | 640 |\n",
        "| Attention Heads | 4 (Multi-Query, 1 KV group) |\n",
        "| Head Dimension | 256 |\n",
        "| FFN Hidden | 2,048 (GeGLU activation) |\n",
        "| Context Length | 32,768 tokens |\n",
        "| Vocabulary | 50,257 (GPT-2 BPE) |\n",
        "\n",
        "### Key Features\n",
        "- **Sliding Window Attention** (w=512): O(n×w) instead of O(n²), 64× cheaper\n",
        "- **Multi-Query Attention**: All query heads share 1 K,V head — 4× less KV cache\n",
        "- **RoPE with Dual Bases**: 10K (local patterns) + 1M (long-range dependencies)\n",
        "- **QK Normalization**: RMSNorm on Q,K vectors before attention\n",
        "- **Gemma-style RMSNorm**: (1 + weight) scaling for stable initialization\n",
        "- **GeGLU Feed-Forward**: Gated GELU activation with 3.2× expansion\n",
        "\n",
        "### Layer Type Pattern\n",
        "```\n",
        "Layers 1-5:   Sliding Attention (local, base=10K)\n",
        "Layer 6:      Full Attention (global, base=1M)\n",
        "Layers 7-11:  Sliding Attention (local, base=10K)\n",
        "Layer 12:     Full Attention (global, base=1M)\n",
        "Layers 13-17: Sliding Attention (local, base=10K)\n",
        "Layer 18:     Full Attention (global, base=1M)\n",
        "```\n",
        "\n",
        "## 📖 Training\n",
        "\n",
        "- **Dataset**: TinyStories (2.1M stories, 471M tokens)\n",
        "- **Tokenizer**: GPT-2 BPE via tiktoken (50,257 vocab)\n",
        "- **Optimizer**: AdamW (β1=0.9, β2=0.95, ε=1e-9, weight_decay=0.1)\n",
        "- **Learning Rate**: 1e-4 → 5e-5 (cosine decay with 1K step warmup)\n",
        "- **Precision**: bfloat16 mixed precision\n",
        "- **Hardware**: NVIDIA A100 40GB (Google Colab Pro)\n",
        "- **Gradient Clipping**: max_norm=0.5\n",
        "\n",
        "## 💻 Usage\n",
        "```python\n",
        "import torch\n",
        "import tiktoken\n",
        "\n",
        "# Load model (you'll need the model class definition)\n",
        "model = Gemma3Model(config)\n",
        "state_dict = torch.load(\"pytorch_model.bin\", map_location=\"cpu\")\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "# Tokenize\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "prompt = \"Once upon a time\"\n",
        "input_ids = torch.tensor([enc.encode_ordinary(prompt)])\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    output = model.generate(input_ids, max_new_tokens=200, temperature=0.7)\n",
        "print(enc.decode(output[0].tolist()))\n",
        "```\n",
        "\n",
        "## 📝 Sample Outputs\n",
        "\n",
        "**Prompt**: \"Once upon a time, there was a little cat named Mittens\"\n",
        "\n",
        "**Temperature 0.7**: *Mittens was very hungry and wanted to eat some food. She went outside\n",
        "to find some grass to eat. Mittens saw a big tree and decided to climb it. She climbed up and\n",
        "up until she reached the top. As she was in the tree, she saw a small bird with a broken wing.\n",
        "Mittens knew just what to do. She took the bird to her mom and asked for help.*\n",
        "\n",
        "## 🙏 Credits\n",
        "\n",
        "- **Architecture Reference**: Vizuara Team - Raj ([Tutorial](https://youtu.be/bLDlwcl6hbA))\n",
        "- **Dataset**: TinyStories by Ronen Eldan & Yuanzhi Li\n",
        "- **Tokenizer**: OpenAI tiktoken (GPT-2 BPE)\n",
        "\n",
        "## 📄 License\n",
        "\n",
        "Apache 2.0\n",
        "\"\"\"\n",
        "\n",
        "with open(EXPORT_DIR / \"README.md\", \"w\") as f:\n",
        "    f.write(model_card)\n",
        "print(\"   ✅ README.md (model card)\")\n",
        "\n",
        "# ── Step 4: Upload to HuggingFace Hub ──\n",
        "print(f\"\\n🚀 Uploading to HuggingFace Hub...\")\n",
        "\n",
        "# CHANGE THIS to your HuggingFace username!\n",
        "HF_USERNAME = input(\"Enter your HuggingFace username: \").strip()\n",
        "REPO_NAME = f\"{HF_USERNAME}/gemma3-270m-tinystories\"\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# Create repo\n",
        "try:\n",
        "    create_repo(REPO_NAME, repo_type=\"model\", exist_ok=True)\n",
        "    print(f\"   ✅ Repo created: https://huggingface.co/{REPO_NAME}\")\n",
        "except Exception as e:\n",
        "    print(f\"   ℹ️  Repo may already exist: {e}\")\n",
        "\n",
        "# Upload all files\n",
        "api.upload_folder(\n",
        "    folder_path=str(EXPORT_DIR),\n",
        "    repo_id=REPO_NAME,\n",
        "    repo_type=\"model\",\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"  🎉 MODEL UPLOADED TO HUGGINGFACE!\")\n",
        "print(f\"  🔗 https://huggingface.co/{REPO_NAME}\")\n",
        "print(f\"{'='*60}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544,
          "referenced_widgets": [
            "3461c975279b484099c4d564dcbd0a9c",
            "f71713eaf41d40bbade776f3516791a1",
            "2fdcc207c648468794bd6e2773af212a",
            "fe6d1b2857e74255b7036dd1d0c3ee59",
            "3c4c1873f7a94276a42fa257ae5bd5d0",
            "9405abd4968343f3bcdd95420dd8faad",
            "cdaf48c24fcb47d79cf9ba8f27d7b861",
            "a0212db93c984c66bc609854d07d7b6a",
            "cd3e5a4134214a34a3af509efb42237d",
            "0778e0f4c6474c84be361f8d0a35b5a3",
            "fb91965f15f440d29c15784f173a2316",
            "81183fc28c5c4c469b1c145b8ed61bfd",
            "f29dcae923034d99a46f9edd8c656751",
            "81920ba5350640c0b1fd323316e35403",
            "546fcfa06b814091ba568656853db1e9",
            "c9a5de4a6e6a46ee8e090c73f6dba92b",
            "a573b3b59cea4ea681c762c9f383491f",
            "8852f7996aec4b48af87c92257e32b2e",
            "8c83b5a4083c4fb19d07e3e5569f2ea1",
            "08737d7be6ea43398e313a9ea9a99f3a",
            "0c33f9b80d0449a78ed15f5991926caf",
            "a2a6c7e725f045f388ddeaec533c7479",
            "d1e5713b9c3c47169f862ba44151d771",
            "d9b34d5823784867a650f83261f2ca2e",
            "ea65472728744f24b11f01fced4e4e95",
            "9a9f3333f0b849c599d8e5b47c4edb51",
            "342e204d53b14894aa9f187d2df5eee8",
            "3462bc37b0b94b1cb707b0a9b2ded188",
            "9134d1298c7f488aac90ba1800922ea8",
            "d3443086ca974654bdebae8194c94750",
            "32089d1b852e491bb3ff8bdaa3cacf20",
            "52d8a05076b4406a9349678c7d9bc6f1",
            "a44538869a46472fa4bb8ce162e4baae",
            "1778f07ab2624953922aa2932c76a0e9",
            "5963234a9e0940d2977a75e3cf558c1e",
            "fdd2c74a43644d989851a361eeb7f0f3",
            "be00eb2f82fb4b10a7621c8e452e7046",
            "2270581310614782a2dc3078c94d90e2",
            "99ff6b9d5e0347788f8f5eb47df63fc9",
            "edb4b23730f3477ea54c794cf8312d6f",
            "cb510280b81e4aa4b82f1d686f6fdaf6",
            "4cb71cffc4f848888c8b3af0be166501",
            "3b73a3ef5bd048638c34affa269eb83b",
            "bd96dd5cde22457bbf9233ae75c052e4"
          ]
        },
        "id": "ldws5ywn-_tH",
        "outputId": "113c2e23-0abd-4c92-cf94-7fb196d3c935"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔑 Login to HuggingFace\n",
            "   Go to: https://huggingface.co/settings/tokens\n",
            "   Create a token with WRITE access\n",
            "   Paste it below:\n",
            "\n",
            "\n",
            "📦 Preparing model files...\n",
            "   ✅ pytorch_model.bin (329.3 MB)\n",
            "   ✅ config.json\n",
            "   ✅ training_config.json\n",
            "   ✅ layer_types.json\n",
            "   ✅ loss_curves.png\n",
            "   ✅ lr_schedule.png\n",
            "   ✅ README.md (model card)\n",
            "\n",
            "🚀 Uploading to HuggingFace Hub...\n",
            "Enter your HuggingFace username: G3nadh\n",
            "   ✅ Repo created: https://huggingface.co/G3nadh/gemma3-270m-tinystories\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3461c975279b484099c4d564dcbd0a9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81183fc28c5c4c469b1c145b8ed61bfd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ..._export/pytorch_model.bin:   0%|          |  612kB /  329MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1e5713b9c3c47169f862ba44151d771"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...hf_export/loss_curves.png:   2%|1         | 1.72kB /  103kB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1778f07ab2624953922aa2932c76a0e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "  🎉 MODEL UPLOADED TO HUGGINGFACE!\n",
            "  🔗 https://huggingface.co/G3nadh/gemma3-270m-tinystories\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 15: Generate ALL Visual Assets for Blog & README                  ║\n",
        "# ║                                                                         ║\n",
        "# ║  This cell creates 12+ publication-quality visualizations:              ║\n",
        "# ║  1.  Parameter breakdown (pie + bar chart)                              ║\n",
        "# ║  2.  Layer types pattern (sliding vs full attention)                    ║\n",
        "# ║  3.  MQA vs Standard attention comparison                              ║\n",
        "# ║  4.  Sliding window vs full attention masking                           ║\n",
        "# ║  5.  RoPE dual-base frequency visualization                            ║\n",
        "# ║  6.  Training loss curves (enhanced)                                    ║\n",
        "# ║  7.  Learning rate schedule                                             ║\n",
        "# ║  8.  Model size comparison (us vs industry)                             ║\n",
        "# ║  9.  Token frequency distribution                                       ║\n",
        "# ║  10. Perplexity over training                                           ║\n",
        "# ║  11. Temperature effect on generation                                   ║\n",
        "# ║  12. Architecture overview (full pipeline)                              ║\n",
        "# ║  13. Memory usage breakdown                                             ║\n",
        "# ║  14. Dataset statistics                                                  ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n"
      ],
      "metadata": {
        "id": "wIcYDWlQ_Aj0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 15: Generate ALL Visual Assets for Blog & README                  ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.rcParams['figure.dpi'] = 150\n",
        "matplotlib.rcParams['savefig.dpi'] = 150\n",
        "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
        "matplotlib.rcParams['axes.spines.top'] = False\n",
        "matplotlib.rcParams['axes.spines.right'] = False\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.gridspec as gridspec\n",
        "import numpy as np\n",
        "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
        "from pathlib import Path\n",
        "\n",
        "VIS_DIR = Path(\"/content/drive/MyDrive/Gemma3_270M_Project/results/visuals\")\n",
        "VIS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "saved = []\n",
        "\n",
        "def save(fig, name):\n",
        "    path = VIS_DIR / name\n",
        "    fig.savefig(path, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
        "    plt.close(fig)\n",
        "    saved.append(name)\n",
        "    print(f\"   ✅ {name}\")\n",
        "\n",
        "print(\"🎨 Generating Visual Assets...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 1. PARAMETER BREAKDOWN\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "print(\"\\n📊 1. Parameter Breakdown\")\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "fig.suptitle(\"Gemma 3 270M — Parameter Distribution (164.6M total)\", fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "components = ['Embedding\\n(50,257×640)', 'Attention\\n(18 layers)', 'FFN\\n(18 layers)', 'Norms + Output\\nProjection']\n",
        "params = [32_164_480, 29_489_472, 70_778_880, 32_199_104]\n",
        "colors = ['#4CAF50', '#7B1FA2', '#00838F', '#FF9800']\n",
        "explode = (0.03, 0.03, 0.08, 0.03)\n",
        "\n",
        "wedges, texts, autotexts = ax1.pie(params, labels=components, colors=colors, autopct='%1.1f%%',\n",
        "    startangle=90, explode=explode, textprops={'fontsize': 10})\n",
        "for t in autotexts:\n",
        "    t.set_fontweight('bold')\n",
        "    t.set_fontsize(11)\n",
        "ax1.set_title(\"Where Do The Parameters Live?\", fontsize=13, fontweight='bold', pad=15)\n",
        "\n",
        "layer_components = ['embed_tokens\\n(50,257→640)', 'attn.q_proj\\n×18 layers', 'attn.k_proj\\n×18 layers',\n",
        "    'attn.v_proj\\n×18 layers', 'attn.o_proj\\n×18 layers', 'ffn.gate_proj\\n×18 layers',\n",
        "    'ffn.up_proj\\n×18 layers', 'ffn.down_proj\\n×18 layers', 'norms\\n(37 total)', 'output_proj\\n(640→50,257)']\n",
        "layer_params = [32_164_480, 11_796_480, 2_949_120, 2_949_120, 11_796_480,\n",
        "    23_592_960, 23_592_960, 23_592_960, 24_320, 32_164_480]\n",
        "layer_colors = ['#4CAF50', '#9C27B0', '#AB47BC', '#CE93D8', '#7B1FA2',\n",
        "    '#006064', '#00838F', '#4DD0E1', '#FFB74D', '#FF9800']\n",
        "\n",
        "bars = ax2.barh(range(len(layer_components)), [p/1e6 for p in layer_params], color=layer_colors)\n",
        "ax2.set_yticks(range(len(layer_components)))\n",
        "ax2.set_yticklabels(layer_components, fontsize=9)\n",
        "ax2.set_xlabel(\"Parameters (millions)\", fontsize=11)\n",
        "ax2.set_title(\"Parameters per Component\", fontsize=13, fontweight='bold')\n",
        "ax2.invert_yaxis()\n",
        "\n",
        "for bar, p in zip(bars, layer_params):\n",
        "    ax2.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height()/2, f'{p/1e6:.1f}M', va='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "save(fig, \"01_parameter_breakdown.png\")\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 2. LAYER TYPES PATTERN\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "print(\"📊 2. Layer Types Pattern\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 4))\n",
        "fig.suptitle(\"18-Layer Architecture — Sliding vs Full Attention Pattern\", fontsize=16, fontweight='bold')\n",
        "\n",
        "layer_types = model_config.layer_types\n",
        "colors_map = {'sliding_attention': '#42A5F5', 'full_attention': '#FF6F00'}\n",
        "\n",
        "for i, lt in enumerate(layer_types):\n",
        "    rect = FancyBboxPatch((i * 0.95, 0.1), 0.85, 0.8,\n",
        "        boxstyle=\"round,pad=0.05\", facecolor=colors_map[lt], edgecolor='white', linewidth=2)\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(i * 0.95 + 0.425, 0.5, f\"L{i+1}\", ha='center', va='center',\n",
        "        fontsize=10, fontweight='bold', color='white')\n",
        "    label = \"S\" if lt == \"sliding_attention\" else \"F\"\n",
        "    ax.text(i * 0.95 + 0.425, 0.2, label, ha='center', va='center',\n",
        "        fontsize=8, color=(1, 1, 1, 0.7))\n",
        "\n",
        "for i, lt in enumerate(layer_types):\n",
        "    base = \"10K\" if lt == \"sliding_attention\" else \"1M\"\n",
        "    color = '#1976D2' if lt == \"sliding_attention\" else '#E65100'\n",
        "    ax.text(i * 0.95 + 0.425, -0.15, base, ha='center', va='center', fontsize=8, color=color, fontweight='bold')\n",
        "\n",
        "ax.text(8.5, -0.35, \"RoPE Base →\", ha='center', fontsize=10, style='italic', color='#555')\n",
        "\n",
        "sliding_patch = mpatches.Patch(color='#42A5F5', label='Sliding Attention (local, window=512, RoPE base=10K)')\n",
        "full_patch = mpatches.Patch(color='#FF6F00', label='Full Attention (global, no window limit, RoPE base=1M)')\n",
        "ax.legend(handles=[sliding_patch, full_patch], loc='upper center', ncol=2, fontsize=10,\n",
        "    bbox_to_anchor=(0.5, 1.25), frameon=True, fancybox=True, shadow=True)\n",
        "\n",
        "ax.set_xlim(-0.2, 17.5)\n",
        "ax.set_ylim(-0.5, 1.1)\n",
        "ax.axis('off')\n",
        "plt.tight_layout()\n",
        "save(fig, \"02_layer_types_pattern.png\")\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 3. MQA vs STANDARD ATTENTION\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "print(\"📊 3. MQA vs Standard Attention\")\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n",
        "fig.suptitle(\"Multi-Query Attention vs Standard Multi-Head Attention\", fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "ax1.set_title(\"❌ Standard Multi-Head\\n(4× more KV cache)\", fontsize=13, fontweight='bold', color='#C62828')\n",
        "ax1.set_xlim(0, 10)\n",
        "ax1.set_ylim(0, 10)\n",
        "ax1.axis('off')\n",
        "\n",
        "for i in range(4):\n",
        "    rect = FancyBboxPatch((0.5 + i*2.2, 7), 1.8, 1.2, boxstyle=\"round,pad=0.1\",\n",
        "        facecolor='#1565C0', edgecolor='white', linewidth=2)\n",
        "    ax1.add_patch(rect)\n",
        "    ax1.text(1.4 + i*2.2, 7.6, f\"Q{i+1}\", ha='center', va='center', color='white', fontweight='bold', fontsize=12)\n",
        "\n",
        "for i in range(4):\n",
        "    rect = FancyBboxPatch((0.5 + i*2.2, 4.5), 1.8, 1.2, boxstyle=\"round,pad=0.1\",\n",
        "        facecolor='#C62828', edgecolor='white', linewidth=2)\n",
        "    ax1.add_patch(rect)\n",
        "    ax1.text(1.4 + i*2.2, 5.1, f\"K{i+1}\", ha='center', va='center', color='white', fontweight='bold', fontsize=12)\n",
        "    ax1.annotate('', xy=(1.4+i*2.2, 6.95), xytext=(1.4+i*2.2, 5.75),\n",
        "        arrowprops=dict(arrowstyle='->', color='#C62828', lw=1.5))\n",
        "\n",
        "for i in range(4):\n",
        "    rect = FancyBboxPatch((0.5 + i*2.2, 2), 1.8, 1.2, boxstyle=\"round,pad=0.1\",\n",
        "        facecolor='#2E7D32', edgecolor='white', linewidth=2)\n",
        "    ax1.add_patch(rect)\n",
        "    ax1.text(1.4 + i*2.2, 2.6, f\"V{i+1}\", ha='center', va='center', color='white', fontweight='bold', fontsize=12)\n",
        "\n",
        "rect = FancyBboxPatch((1, 0.2), 7.5, 1.2, boxstyle=\"round,pad=0.1\", facecolor='#FFCDD2', edgecolor='#C62828', linewidth=2)\n",
        "ax1.add_patch(rect)\n",
        "ax1.text(4.75, 0.8, \"KV Cache: 4 × 256 = 1,024 dims\", ha='center', va='center', fontsize=12, fontweight='bold', color='#C62828')\n",
        "\n",
        "ax2.set_title(\"✅ Multi-Query (Gemma 3)\\n(4× cheaper KV cache!)\", fontsize=13, fontweight='bold', color='#2E7D32')\n",
        "ax2.set_xlim(0, 10)\n",
        "ax2.set_ylim(0, 10)\n",
        "ax2.axis('off')\n",
        "\n",
        "for i in range(4):\n",
        "    rect = FancyBboxPatch((0.5 + i*2.2, 7), 1.8, 1.2, boxstyle=\"round,pad=0.1\",\n",
        "        facecolor='#1565C0', edgecolor='white', linewidth=2)\n",
        "    ax2.add_patch(rect)\n",
        "    ax2.text(1.4 + i*2.2, 7.6, f\"Q{i+1}\", ha='center', va='center', color='white', fontweight='bold', fontsize=12)\n",
        "\n",
        "rect = FancyBboxPatch((2.5, 4.5), 5, 1.2, boxstyle=\"round,pad=0.1\", facecolor='#C62828', edgecolor='white', linewidth=2)\n",
        "ax2.add_patch(rect)\n",
        "ax2.text(5, 5.1, \"1 Shared K Head\", ha='center', va='center', color='white', fontweight='bold', fontsize=13)\n",
        "\n",
        "for i in range(4):\n",
        "    ax2.annotate('', xy=(1.4+i*2.2, 6.95), xytext=(3.5+i*0.9, 5.75),\n",
        "        arrowprops=dict(arrowstyle='->', color='#C62828', lw=1.5, ls='--'))\n",
        "\n",
        "rect = FancyBboxPatch((2.5, 2), 5, 1.2, boxstyle=\"round,pad=0.1\", facecolor='#2E7D32', edgecolor='white', linewidth=2)\n",
        "ax2.add_patch(rect)\n",
        "ax2.text(5, 2.6, \"1 Shared V Head\", ha='center', va='center', color='white', fontweight='bold', fontsize=13)\n",
        "\n",
        "rect = FancyBboxPatch((1, 0.2), 7.5, 1.2, boxstyle=\"round,pad=0.1\", facecolor='#C8E6C9', edgecolor='#2E7D32', linewidth=2)\n",
        "ax2.add_patch(rect)\n",
        "ax2.text(4.75, 0.8, \"KV Cache: 1 × 256 = 256 dims  (4× cheaper!)\", ha='center', va='center', fontsize=12, fontweight='bold', color='#2E7D32')\n",
        "\n",
        "plt.tight_layout()\n",
        "save(fig, \"03_mqa_vs_standard.png\")\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 4. SLIDING WINDOW vs FULL ATTENTION MASKS\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "print(\"📊 4. Sliding Window vs Full Attention Masks\")\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 5))\n",
        "fig.suptitle(\"Attention Masking — What Each Token Can See\", fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "seq_len = 16\n",
        "\n",
        "causal = np.tril(np.ones((seq_len, seq_len)))\n",
        "ax1.imshow(causal, cmap='Blues', aspect='equal')\n",
        "ax1.set_title(\"Basic Causal Mask\\n(standard transformer)\", fontsize=12, fontweight='bold')\n",
        "ax1.set_xlabel(\"Key position (past)\")\n",
        "ax1.set_ylabel(\"Query position (current)\")\n",
        "ax1.set_xticks(range(0, seq_len, 4))\n",
        "ax1.set_yticks(range(0, seq_len, 4))\n",
        "\n",
        "window = 4\n",
        "sliding = np.zeros((seq_len, seq_len))\n",
        "for i in range(seq_len):\n",
        "    for j in range(seq_len):\n",
        "        if j <= i and i - j <= window:\n",
        "            sliding[i][j] = 1.0\n",
        "ax2.imshow(sliding, cmap='Purples', aspect='equal')\n",
        "ax2.set_title(f\"Sliding Window (w={window})\\n(15 of 18 layers — LOCAL)\", fontsize=12, fontweight='bold', color='#7B1FA2')\n",
        "ax2.set_xlabel(\"Key position\")\n",
        "ax2.set_ylabel(\"Query position\")\n",
        "ax2.set_xticks(range(0, seq_len, 4))\n",
        "ax2.set_yticks(range(0, seq_len, 4))\n",
        "\n",
        "ax3.imshow(causal, cmap='Oranges', aspect='equal')\n",
        "ax3.set_title(\"Full Causal Attention\\n(3 of 18 layers — GLOBAL)\", fontsize=12, fontweight='bold', color='#E65100')\n",
        "ax3.set_xlabel(\"Key position\")\n",
        "ax3.set_ylabel(\"Query position\")\n",
        "ax3.set_xticks(range(0, seq_len, 4))\n",
        "ax3.set_yticks(range(0, seq_len, 4))\n",
        "\n",
        "ax1.text(8, -2.5, \"O(n²) — sees everything before it\", ha='center', fontsize=10, style='italic', color='#1565C0')\n",
        "ax2.text(8, -2.5, f\"O(n×w) — only sees last {window} tokens (64× cheaper!)\", ha='center', fontsize=10, style='italic', color='#7B1FA2')\n",
        "ax3.text(8, -2.5, \"O(n²) — full context for long-range dependencies\", ha='center', fontsize=10, style='italic', color='#E65100')\n",
        "\n",
        "plt.tight_layout()\n",
        "save(fig, \"04_attention_masks.png\")\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 5. RoPE DUAL-BASE FREQUENCIES\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "print(\"📊 5. RoPE Dual-Base Frequencies\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle(\"RoPE — Rotary Position Encoding with Dual Bases\", fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "head_dim = 256\n",
        "positions = np.arange(512)\n",
        "\n",
        "local_base = 10_000\n",
        "i_vals = np.arange(0, head_dim, 2)\n",
        "local_freqs = 1.0 / (local_base ** (i_vals / head_dim))\n",
        "\n",
        "global_base = 1_000_000\n",
        "global_freqs = 1.0 / (global_base ** (i_vals / head_dim))\n",
        "\n",
        "ax = axes[0, 0]\n",
        "ax.semilogy(i_vals, local_freqs, color='#1565C0', linewidth=2, label='Local base=10K')\n",
        "ax.semilogy(i_vals, global_freqs, color='#E65100', linewidth=2, label='Global base=1M')\n",
        "ax.set_xlabel(\"Dimension index (i)\", fontsize=11)\n",
        "ax.set_ylabel(\"Frequency (log scale)\", fontsize=11)\n",
        "ax.set_title(\"Frequency Decay per Dimension\", fontsize=13, fontweight='bold')\n",
        "ax.legend(fontsize=11, frameon=True, fancybox=True)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "ax = axes[0, 1]\n",
        "for dim_i in [0, 32, 64, 96]:\n",
        "    freq_local = 1.0 / (local_base ** (dim_i / head_dim))\n",
        "    angles = positions * freq_local\n",
        "    ax.plot(positions, np.sin(angles), linewidth=1.5, label=f'dim {dim_i}', alpha=0.8)\n",
        "ax.set_xlabel(\"Position in sequence\", fontsize=11)\n",
        "ax.set_ylabel(\"sin(θ)\", fontsize=11)\n",
        "ax.set_title(\"Local Base (10K) — Fast Rotations\\n(Good for nearby tokens)\", fontsize=13, fontweight='bold', color='#1565C0')\n",
        "ax.legend(fontsize=9, frameon=True, loc='upper right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "ax = axes[1, 0]\n",
        "for dim_i in [0, 32, 64, 96]:\n",
        "    freq_global = 1.0 / (global_base ** (dim_i / head_dim))\n",
        "    angles = positions * freq_global\n",
        "    ax.plot(positions, np.sin(angles), linewidth=1.5, label=f'dim {dim_i}', alpha=0.8)\n",
        "ax.set_xlabel(\"Position in sequence\", fontsize=11)\n",
        "ax.set_ylabel(\"sin(θ)\", fontsize=11)\n",
        "ax.set_title(\"Global Base (1M) — Slow Rotations\\n(Good for distant tokens)\", fontsize=13, fontweight='bold', color='#E65100')\n",
        "ax.legend(fontsize=9, frameon=True, loc='upper right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "ax = axes[1, 1]\n",
        "ax.axis('off')\n",
        "table_data = [\n",
        "    [\"Property\", \"Local (10K)\", \"Global (1M)\"],\n",
        "    [\"Base frequency\", \"10,000\", \"1,000,000\"],\n",
        "    [\"Rotation speed\", \"Fast\", \"Slow\"],\n",
        "    [\"Best for\", \"Nearby tokens\", \"Distant tokens\"],\n",
        "    [\"Used in layers\", \"1-5, 7-11, 13-17\", \"6, 12, 18\"],\n",
        "    [\"Attention type\", \"Sliding window\", \"Full attention\"],\n",
        "    [\"Layers count\", \"15 layers\", \"3 layers\"],\n",
        "    [\"Analogy\", \"Hour hand\", \"Year hand\"],\n",
        "]\n",
        "table = ax.table(cellText=table_data[1:], colLabels=table_data[0],\n",
        "    cellLoc='center', loc='center', colColours=['#E3F2FD', '#E3F2FD', '#FFF3E0'])\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1, 1.6)\n",
        "for (i, j), cell in table.get_celld().items():\n",
        "    if i == 0:\n",
        "        cell.set_fontsize(11)\n",
        "        cell.set_text_props(fontweight='bold')\n",
        "    cell.set_edgecolor('#ddd')\n",
        "ax.set_title(\"Dual-Base Summary\", fontsize=13, fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "save(fig, \"05_rope_dual_base.png\")\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 6. ENHANCED TRAINING LOSS CURVES\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "print(\"📊 6. Enhanced Training Loss Curves\")\n",
        "\n",
        "ckpt = torch.load(str(\"/content/drive/MyDrive/Gemma3_270M_Project/checkpoints/best_model.pt\"), map_location=\"cpu\", weights_only=False)\n",
        "train_losses = ckpt.get('train_losses', [])\n",
        "val_losses = ckpt.get('val_losses', [])\n",
        "\n",
        "if train_losses and val_losses:\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    fig.suptitle(\"Gemma 3 270M — Training Progress on TinyStories\", fontsize=16, fontweight='bold', y=1.04)\n",
        "\n",
        "    steps = list(range(len(train_losses)))\n",
        "    eval_interval = 500\n",
        "    x_steps = [i * eval_interval for i in steps]\n",
        "\n",
        "    ax = axes[0]\n",
        "    ax.plot(x_steps, train_losses, color='#1565C0', linewidth=2, label='Train Loss', alpha=0.9)\n",
        "    ax.plot(x_steps, val_losses, color='#E65100', linewidth=2, label='Val Loss', alpha=0.9)\n",
        "    best_idx = val_losses.index(min(val_losses))\n",
        "    ax.scatter([x_steps[best_idx]], [val_losses[best_idx]], color='#E65100', s=100, zorder=5, marker='*')\n",
        "    ax.annotate(f'Best: {min(val_losses):.4f}\\n(step {x_steps[best_idx]:,})',\n",
        "        xy=(x_steps[best_idx], val_losses[best_idx]), xytext=(x_steps[best_idx]+1000, val_losses[best_idx]+0.3),\n",
        "        arrowprops=dict(arrowstyle='->', color='#E65100'), fontsize=10, fontweight='bold', color='#E65100')\n",
        "    ax.set_xlabel(\"Training Steps\", fontsize=11)\n",
        "    ax.set_ylabel(\"Cross-Entropy Loss\", fontsize=11)\n",
        "    ax.set_title(\"Training & Validation Loss\", fontsize=13, fontweight='bold')\n",
        "    ax.legend(fontsize=11, frameon=True)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    ax = axes[1]\n",
        "    train_ppl = [math.exp(l) for l in train_losses]\n",
        "    val_ppl = [math.exp(l) for l in val_losses]\n",
        "    ax.plot(x_steps, train_ppl, color='#1565C0', linewidth=2, label='Train PPL', alpha=0.9)\n",
        "    ax.plot(x_steps, val_ppl, color='#E65100', linewidth=2, label='Val PPL', alpha=0.9)\n",
        "    ax.axhline(y=math.exp(min(val_losses)), color='#E65100', linestyle='--', alpha=0.5, label=f'Best: {math.exp(min(val_losses)):.2f}')\n",
        "    ax.set_xlabel(\"Training Steps\", fontsize=11)\n",
        "    ax.set_ylabel(\"Perplexity (lower = better)\", fontsize=11)\n",
        "    ax.set_title(\"Perplexity Over Training\", fontsize=13, fontweight='bold')\n",
        "    ax.legend(fontsize=10, frameon=True)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    ax = axes[2]\n",
        "    if len(val_losses) > 1:\n",
        "        loss_delta = [val_losses[i] - val_losses[i-1] for i in range(1, len(val_losses))]\n",
        "        ax.bar(range(len(loss_delta)), loss_delta,\n",
        "            color=['#4CAF50' if d < 0 else '#F44336' for d in loss_delta], alpha=0.8)\n",
        "        ax.axhline(y=0, color='black', linewidth=0.5)\n",
        "        ax.set_xlabel(\"Evaluation Step\", fontsize=11)\n",
        "        ax.set_ylabel(\"Val Loss Change (delta)\", fontsize=11)\n",
        "        ax.set_title(\"Learning Progress\\n(green=improving, red=overfitting)\", fontsize=13, fontweight='bold')\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save(fig, \"06_training_curves.png\")\n",
        "else:\n",
        "    print(\"   ⚠️  No training logs in checkpoint, skipping\")\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 7. LEARNING RATE SCHEDULE\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "print(\"📊 7. Learning Rate Schedule\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "steps_range = np.arange(0, 60001)\n",
        "lrs = []\n",
        "for step in steps_range:\n",
        "    if step < 1000:\n",
        "        lr = 1e-4 * step / 1000\n",
        "    elif step < 60000:\n",
        "        progress = (step - 1000) / (60000 - 1000)\n",
        "        lr = 5e-5 + 0.5 * (1e-4 - 5e-5) * (1 + math.cos(math.pi * progress))\n",
        "    else:\n",
        "        lr = 5e-5\n",
        "    lrs.append(lr)\n",
        "\n",
        "ax.plot(steps_range, lrs, color='#1565C0', linewidth=2.5)\n",
        "ax.fill_between(steps_range[:1000], 0, lrs[:1000], alpha=0.15, color='#4CAF50', label='Warmup (0-1K)')\n",
        "ax.fill_between(steps_range[1000:60000], 0, lrs[1000:60000], alpha=0.1, color='#1565C0', label='Cosine Decay (1K-60K)')\n",
        "ax.fill_between(steps_range[60000:], 0, lrs[60000:], alpha=0.15, color='#FF9800', label='Min LR (60K+)')\n",
        "\n",
        "ax.axvline(x=best_iter, color='#E65100', linestyle='--', linewidth=2, alpha=0.7, label=f'Best model (step {best_iter:,})')\n",
        "\n",
        "ax.set_xlabel(\"Training Steps\", fontsize=12)\n",
        "ax.set_ylabel(\"Learning Rate\", fontsize=12)\n",
        "ax.set_title(\"Cosine Learning Rate Schedule with Linear Warmup\", fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=11, frameon=True, fancybox=True)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.ticklabel_format(axis='y', style='scientific', scilimits=(-4,-4))\n",
        "\n",
        "save(fig, \"07_lr_schedule.png\")\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 8. MODEL SIZE COMPARISON\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "print(\"📊 8. Model Size Comparison\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "models = ['Our Gemma 3\\n(164M)', 'GPT-2\\nSmall', 'GPT-2\\nMedium', 'GPT-2\\nLarge', 'GPT-2\\nXL',\n",
        "    'Llama 3.2\\n1B', 'Llama 3.2\\n3B', 'Llama 3.1\\n8B', 'Llama 3.1\\n70B', 'GPT-4\\n(estimated)']\n",
        "sizes = [0.165, 0.124, 0.355, 0.774, 1.558, 1.0, 3.0, 8.0, 70.0, 1000.0]\n",
        "bar_colors = ['#E65100'] + ['#90CAF9']*4 + ['#66BB6A']*4 + ['#CE93D8']\n",
        "\n",
        "bars = ax.barh(range(len(models)), sizes, color=bar_colors, edgecolor='white', linewidth=1.5)\n",
        "ax.set_yticks(range(len(models)))\n",
        "ax.set_yticklabels(models, fontsize=10)\n",
        "ax.set_xlabel(\"Parameters (Billions)\", fontsize=12)\n",
        "ax.set_title(\"Model Size Comparison — Where Does Our Model Fit?\", fontsize=14, fontweight='bold')\n",
        "ax.set_xscale('log')\n",
        "ax.invert_yaxis()\n",
        "\n",
        "for bar, size in zip(bars, sizes):\n",
        "    label = f\"{size:.3f}B\" if size < 1 else f\"{size:.0f}B\"\n",
        "    ax.text(bar.get_width() * 1.2, bar.get_y() + bar.get_height()/2, label, va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "bars[0].set_edgecolor('#E65100')\n",
        "bars[0].set_linewidth(3)\n",
        "\n",
        "ax.annotate('YOU ARE HERE\\n(built from scratch!)',\n",
        "    xy=(0.165, 0), fontsize=11, fontweight='bold', color='#E65100',\n",
        "    xytext=(3, 0.5), arrowprops=dict(arrowstyle='->', color='#E65100', lw=2))\n",
        "\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "save(fig, \"08_model_size_comparison.png\")\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 9. DATASET STATISTICS\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "print(\"📊 9. Dataset Statistics\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "fig.suptitle(\"TinyStories Dataset — 2.1 Million Children's Stories\", fontsize=16, fontweight='bold', y=1.04)\n",
        "\n",
        "ax = axes[0]\n",
        "categories = ['Train\\nTokens', 'Val\\nTokens']\n",
        "token_counts = [471_000_000, 4_700_000]\n",
        "bars = ax.bar(categories, [t/1e6 for t in token_counts], color=['#1565C0', '#E65100'], width=0.6)\n",
        "ax.set_ylabel(\"Tokens (Millions)\", fontsize=11)\n",
        "ax.set_title(\"Token Count\", fontsize=13, fontweight='bold')\n",
        "for bar, count in zip(bars, token_counts):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, f'{count/1e6:.0f}M',\n",
        "        ha='center', fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "ax = axes[1]\n",
        "vocab_cats = ['Total\\nVocab', 'Commonly\\nUsed', 'Rare\\n(<10 times)']\n",
        "vocab_counts = [50257, 12000, 30000]\n",
        "vc_colors = ['#42A5F5', '#4CAF50', '#FF9800']\n",
        "bars = ax.bar(vocab_cats, [v/1000 for v in vocab_counts], color=vc_colors, width=0.6)\n",
        "ax.set_ylabel(\"Tokens (Thousands)\", fontsize=11)\n",
        "ax.set_title(\"Vocabulary Distribution\", fontsize=13, fontweight='bold')\n",
        "for bar, count in zip(bars, vocab_counts):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{count/1000:.0f}K',\n",
        "        ha='center', fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "ax = axes[2]\n",
        "ax.axis('off')\n",
        "config_data = [\n",
        "    [\"Setting\", \"Value\"],\n",
        "    [\"Batch Size\", \"32 sequences\"],\n",
        "    [\"Sequence Length\", \"512 tokens\"],\n",
        "    [\"Grad Accumulation\", \"4 steps\"],\n",
        "    [\"Effective Batch\", \"128 seq = 65,536 tok\"],\n",
        "    [\"Optimizer\", \"AdamW\"],\n",
        "    [\"Peak LR\", \"1e-4\"],\n",
        "    [\"Min LR\", \"5e-5\"],\n",
        "    [\"Weight Decay\", \"0.1\"],\n",
        "    [\"Grad Clip\", \"0.5\"],\n",
        "    [\"Precision\", \"bfloat16\"],\n",
        "]\n",
        "table = ax.table(cellText=config_data[1:], colLabels=config_data[0],\n",
        "    cellLoc='center', loc='center', colColours=['#E3F2FD', '#E3F2FD'])\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1, 1.5)\n",
        "for (i, j), cell in table.get_celld().items():\n",
        "    if i == 0:\n",
        "        cell.set_fontsize(11)\n",
        "        cell.set_text_props(fontweight='bold')\n",
        "    cell.set_edgecolor('#ddd')\n",
        "ax.set_title(\"Training Configuration\", fontsize=13, fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "save(fig, \"09_dataset_statistics.png\")\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 10. GeGLU FFN ARCHITECTURE\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "print(\"📊 10. GeGLU FFN Architecture\")\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "fig.suptitle(\"Feed-Forward Network — GeGLU (Gated GELU) Architecture\", fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "ax1.axis('off')\n",
        "ax1.set_xlim(0, 10)\n",
        "ax1.set_ylim(0, 10)\n",
        "\n",
        "rect = FancyBboxPatch((3.5, 8.5), 3, 0.8, boxstyle=\"round,pad=0.1\", facecolor='#E3F2FD', edgecolor='#1565C0', linewidth=2)\n",
        "ax1.add_patch(rect)\n",
        "ax1.text(5, 8.9, \"Input (640 dim)\", ha='center', va='center', fontsize=12, fontweight='bold', color='#1565C0')\n",
        "\n",
        "rect = FancyBboxPatch((0.5, 6.2), 3.5, 1.2, boxstyle=\"round,pad=0.1\", facecolor='#7B1FA2', edgecolor='white', linewidth=2)\n",
        "ax1.add_patch(rect)\n",
        "ax1.text(2.25, 6.8, \"gate_proj\\n640 -> 2048\", ha='center', va='center', fontsize=11, fontweight='bold', color='white')\n",
        "\n",
        "rect = FancyBboxPatch((5.5, 6.2), 3.5, 1.2, boxstyle=\"round,pad=0.1\", facecolor='#00838F', edgecolor='white', linewidth=2)\n",
        "ax1.add_patch(rect)\n",
        "ax1.text(7.25, 6.8, \"up_proj\\n640 -> 2048\", ha='center', va='center', fontsize=11, fontweight='bold', color='white')\n",
        "\n",
        "rect = FancyBboxPatch((0.5, 4.5), 3.5, 0.8, boxstyle=\"round,pad=0.1\", facecolor='#FF9800', edgecolor='white', linewidth=2)\n",
        "ax1.add_patch(rect)\n",
        "ax1.text(2.25, 4.9, \"GELU activation\", ha='center', va='center', fontsize=11, fontweight='bold', color='white')\n",
        "\n",
        "ax1.add_patch(plt.Circle((5, 4.9), 0.4, facecolor='#F44336', edgecolor='white', linewidth=2))\n",
        "ax1.text(5, 4.9, \"x\", ha='center', va='center', fontsize=18, fontweight='bold', color='white')\n",
        "\n",
        "rect = FancyBboxPatch((3, 2.2), 4, 1, boxstyle=\"round,pad=0.1\", facecolor='#1565C0', edgecolor='white', linewidth=2)\n",
        "ax1.add_patch(rect)\n",
        "ax1.text(5, 2.7, \"down_proj\\n2048 -> 640\", ha='center', va='center', fontsize=11, fontweight='bold', color='white')\n",
        "\n",
        "rect = FancyBboxPatch((3.5, 0.5), 3, 0.8, boxstyle=\"round,pad=0.1\", facecolor='#E3F2FD', edgecolor='#1565C0', linewidth=2)\n",
        "ax1.add_patch(rect)\n",
        "ax1.text(5, 0.9, \"Output (640 dim)\", ha='center', va='center', fontsize=12, fontweight='bold', color='#1565C0')\n",
        "\n",
        "for start, end in [((5, 8.5), (2.25, 7.45)), ((5, 8.5), (7.25, 7.45)),\n",
        "    ((2.25, 6.2), (2.25, 5.35)), ((2.25, 4.5), (4.65, 4.9)), ((7.25, 6.2), (5.35, 4.9)),\n",
        "    ((5, 4.45), (5, 3.25)), ((5, 2.2), (5, 1.35))]:\n",
        "    ax1.annotate('', xy=end, xytext=start, arrowprops=dict(arrowstyle='->', color='#555', lw=1.5))\n",
        "\n",
        "ax1.set_title(\"GeGLU Architecture\", fontsize=13, fontweight='bold')\n",
        "\n",
        "ax2.set_title(\"GELU vs ReLU Activation Functions\", fontsize=13, fontweight='bold')\n",
        "x = np.linspace(-4, 4, 1000)\n",
        "gelu = 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))\n",
        "relu = np.maximum(0, x)\n",
        "\n",
        "ax2.plot(x, gelu, color='#FF9800', linewidth=2.5, label='GELU (Gemma 3 uses this)')\n",
        "ax2.plot(x, relu, color='#90CAF9', linewidth=2, linestyle='--', label='ReLU (older models)')\n",
        "ax2.axhline(y=0, color='gray', linewidth=0.5)\n",
        "ax2.axvline(x=0, color='gray', linewidth=0.5)\n",
        "ax2.set_xlabel(\"Input\", fontsize=11)\n",
        "ax2.set_ylabel(\"Output\", fontsize=11)\n",
        "ax2.legend(fontsize=11, frameon=True, fancybox=True)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.annotate(\"Smooth curve\\n(better gradients!)\", xy=(-1, -0.17), fontsize=10, color='#FF9800',\n",
        "    xytext=(-3, -1.5), arrowprops=dict(arrowstyle='->', color='#FF9800'))\n",
        "\n",
        "plt.tight_layout()\n",
        "save(fig, \"10_geglu_ffn.png\")\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 11. RMSNorm VISUALIZATION\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "print(\"📊 11. RMSNorm Visualization\")\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "fig.suptitle(\"RMSNorm — Gemma 3's (1 + weight) Scaling Innovation\", fontsize=16, fontweight='bold', y=1.04)\n",
        "\n",
        "ax1.axis('off')\n",
        "ax1.set_xlim(0, 10)\n",
        "ax1.set_ylim(0, 10)\n",
        "ax1.set_title(\"Step-by-Step Example: x = [2, -6]\", fontsize=13, fontweight='bold')\n",
        "\n",
        "steps_info = [\n",
        "    (\"Step 1: Variance\", \"var(x) = (4 + 36)/2 = 20\", 8.5, '#42A5F5'),\n",
        "    (\"Step 2: RMS\", \"rms(x) = sqrt(20 + eps) = 4.472\", 7.0, '#66BB6A'),\n",
        "    (\"Step 3: Normalize\", \"x_hat = [2/4.472, -6/4.472] = [0.447, -1.342]\", 5.5, '#FF9800'),\n",
        "    (\"Step 4: Scale\", \"y = x_hat * (1 + weight)\", 4.0, '#E91E63'),\n",
        "    (\"Step 5: Result\", \"weights=0.5 -> y = [0.447*1.5, -1.342*1.5]\", 2.5, '#7B1FA2'),\n",
        "    (\"\", \"= [0.671, -2.013]\", 1.5, '#7B1FA2'),\n",
        "]\n",
        "\n",
        "for label, formula, y_pos, color in steps_info:\n",
        "    if label:\n",
        "        ax1.text(0.5, y_pos, label, fontsize=11, fontweight='bold', color=color)\n",
        "    ax1.text(0.5, y_pos - 0.6, formula, fontsize=10, color='#333', family='monospace')\n",
        "\n",
        "ax2.set_title(\"Why (1 + weight) Is Better\", fontsize=13, fontweight='bold')\n",
        "x_range = np.linspace(-3, 3, 100)\n",
        "\n",
        "ax2.plot(x_range, 1.0 * x_range, color='#90CAF9', linewidth=2, linestyle='--',\n",
        "    label='Standard: weight * x\\n(weight init=1, can drift)')\n",
        "ax2.plot(x_range, (1 + 0.0) * x_range, color='#4CAF50', linewidth=2.5,\n",
        "    label='Gemma: (1+0) * x = identity\\n(stable start, gradual learning)')\n",
        "ax2.plot(x_range, (1 + 0.5) * x_range, color='#FF9800', linewidth=1.5, alpha=0.7,\n",
        "    label='After training: (1+0.5) * x')\n",
        "\n",
        "ax2.axhline(y=0, color='gray', linewidth=0.5)\n",
        "ax2.axvline(x=0, color='gray', linewidth=0.5)\n",
        "ax2.set_xlabel(\"Normalized Input\", fontsize=11)\n",
        "ax2.set_ylabel(\"Scaled Output\", fontsize=11)\n",
        "ax2.legend(fontsize=10, frameon=True, fancybox=True, loc='upper left')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "save(fig, \"11_rmsnorm.png\")\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 12. TEMPERATURE EFFECT\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "print(\"📊 12. Temperature Effect on Sampling\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "fig.suptitle(\"Temperature Controls Creativity — Probability Distribution for Next Word\",\n",
        "    fontsize=16, fontweight='bold', y=1.04)\n",
        "\n",
        "words = ['mat', 'floor', 'couch', 'bed', 'rug', 'table', 'chair', 'wall', 'roof', 'door']\n",
        "raw_logits = np.array([3.5, 2.8, 2.1, 1.8, 1.5, 1.0, 0.7, 0.3, -0.2, -0.8])\n",
        "\n",
        "for ax, temp, title, color in [\n",
        "    (axes[0], 0.3, \"Temperature = 0.3\\n(Conservative)\", '#1565C0'),\n",
        "    (axes[1], 0.7, \"Temperature = 0.7\\n(Balanced)\", '#4CAF50'),\n",
        "    (axes[2], 1.0, \"Temperature = 1.0\\n(Creative)\", '#E65100'),\n",
        "]:\n",
        "    logits = raw_logits / temp\n",
        "    probs = np.exp(logits) / np.exp(logits).sum()\n",
        "\n",
        "    bars = ax.bar(range(len(words)), probs * 100, color=color, alpha=0.8, edgecolor='white')\n",
        "    ax.set_xticks(range(len(words)))\n",
        "    ax.set_xticklabels(words, rotation=45, ha='right', fontsize=9)\n",
        "    ax.set_ylabel(\"Probability (%)\", fontsize=10)\n",
        "    ax.set_title(title, fontsize=12, fontweight='bold', color=color)\n",
        "    ax.set_ylim(0, max(probs*100) * 1.2)\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    top_idx = np.argmax(probs)\n",
        "    ax.text(top_idx, probs[top_idx]*100 + 1, f'{probs[top_idx]*100:.0f}%', ha='center',\n",
        "        fontsize=11, fontweight='bold', color=color)\n",
        "\n",
        "plt.tight_layout()\n",
        "save(fig, \"12_temperature_effect.png\")\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 13. ARCHITECTURE SUMMARY TABLE\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "print(\"📊 13. Architecture Summary Table\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 9))\n",
        "ax.axis('off')\n",
        "fig.suptitle(\"Gemma 3 270M — Complete Architecture Specification\", fontsize=16, fontweight='bold')\n",
        "\n",
        "table_data = [\n",
        "    [\"Component\", \"Specification\", \"Parameters\", \"Key Innovation\"],\n",
        "    [\"Embedding\", \"50,257 -> 640 dim\", \"32.2M (19.5%)\", \"x sqrt(640) scaling\"],\n",
        "    [\"Attention (x18)\", \"4 Q heads, 1 KV head\", \"29.5M (17.9%)\", \"Multi-Query Attention\"],\n",
        "    [\"  Q Projection\", \"640 -> 4 x 256 = 1024\", \"655K/layer\", \"4 heads for diversity\"],\n",
        "    [\"  K Projection\", \"640 -> 1 x 256\", \"164K/layer\", \"1 shared head (4x savings)\"],\n",
        "    [\"  V Projection\", \"640 -> 1 x 256\", \"164K/layer\", \"1 shared head (4x savings)\"],\n",
        "    [\"  O Projection\", \"1024 -> 640\", \"655K/layer\", \"Concat heads -> project\"],\n",
        "    [\"  QK Norm\", \"RMSNorm on Q and K\", \"512/layer\", \"Stabilizes attention\"],\n",
        "    [\"  RoPE\", \"Dual bases: 10K / 1M\", \"0 (precomputed)\", \"Position via rotation\"],\n",
        "    [\"  Masking\", \"Sliding w=512 / Full\", \"0 (computed)\", \"15 local + 3 global\"],\n",
        "    [\"FFN (x18)\", \"GeGLU: 640->2048->640\", \"70.8M (43.0%)\", \"Gated GELU activation\"],\n",
        "    [\"  gate_proj\", \"640 -> 2048\", \"1.3M/layer\", \"Learns which features matter\"],\n",
        "    [\"  up_proj\", \"640 -> 2048\", \"1.3M/layer\", \"Expands to higher dim\"],\n",
        "    [\"  down_proj\", \"2048 -> 640\", \"1.3M/layer\", \"Compresses back down\"],\n",
        "    [\"Norms (x37)\", \"RMSNorm, (1+w) scale\", \"24K (0.01%)\", \"Stable initialization\"],\n",
        "    [\"Output Proj\", \"640 -> 50,257\", \"32.2M (19.5%)\", \"Logits for all vocab words\"],\n",
        "    [\"TOTAL\", \"\", \"164.6M\", \"\"],\n",
        "]\n",
        "\n",
        "row_colors = ['#E8F5E9', '#F3E5F5', '#F3E5F5', '#F3E5F5', '#F3E5F5', '#F3E5F5',\n",
        "    '#F3E5F5', '#F3E5F5', '#F3E5F5', '#E0F2F1', '#E0F2F1', '#E0F2F1',\n",
        "    '#E0F2F1', '#FFF3E0', '#FFF3E0', '#FFEBEE']\n",
        "\n",
        "table = ax.table(cellText=table_data[1:], colLabels=table_data[0],\n",
        "    cellLoc='center', loc='center',\n",
        "    colColours=['#1565C0', '#1565C0', '#1565C0', '#1565C0'])\n",
        "\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(9)\n",
        "table.scale(1, 1.45)\n",
        "\n",
        "for (i, j), cell in table.get_celld().items():\n",
        "    cell.set_edgecolor('#ddd')\n",
        "    if i == 0:\n",
        "        cell.set_fontsize(11)\n",
        "        cell.set_text_props(fontweight='bold', color='white')\n",
        "        cell.set_facecolor('#1565C0')\n",
        "    else:\n",
        "        idx = min(i-1, len(row_colors)-1)\n",
        "        cell.set_facecolor(row_colors[idx])\n",
        "    if i == len(table_data) - 1:\n",
        "        cell.set_text_props(fontweight='bold')\n",
        "        cell.set_facecolor('#FFCDD2')\n",
        "\n",
        "table.auto_set_column_width([0, 1, 2, 3])\n",
        "plt.tight_layout()\n",
        "save(fig, \"13_architecture_table.png\")\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 14. TRAINING PIPELINE OVERVIEW\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "print(\"📊 14. Training Pipeline Overview\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 6))\n",
        "ax.axis('off')\n",
        "ax.set_xlim(0, 16)\n",
        "ax.set_ylim(0, 6)\n",
        "fig.suptitle(\"Gemma 3 270M — Complete Training Pipeline\", fontsize=16, fontweight='bold')\n",
        "\n",
        "pipeline = [\n",
        "    (0.2, \"TinyStories\\n2.1M stories\", '#4CAF50', \"DATASET\"),\n",
        "    (2.7, \"GPT-2 BPE\\n50,257 vocab\", '#2196F3', \"TOKENIZE\"),\n",
        "    (5.2, \"Batch: 32x512\\n16K tokens\", '#9C27B0', \"DATALOADER\"),\n",
        "    (7.7, \"Gemma 3\\n164.6M params\", '#E65100', \"MODEL\"),\n",
        "    (10.2, \"Cross-Entropy\\nNLL Loss\", '#F44336', \"LOSS\"),\n",
        "    (12.7, \"AdamW +\\nGrad Clip 0.5\", '#00838F', \"OPTIMIZER\"),\n",
        "]\n",
        "\n",
        "for x, label, color, title in pipeline:\n",
        "    rect = FancyBboxPatch((x, 1.5), 2.2, 3, boxstyle=\"round,pad=0.15\",\n",
        "        facecolor=color, edgecolor='white', linewidth=2, alpha=0.9)\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(x + 1.1, 3.6, title, ha='center', va='center', fontsize=9,\n",
        "        fontweight='bold', color=(1, 1, 1, 0.7))\n",
        "    ax.text(x + 1.1, 2.7, label, ha='center', va='center', fontsize=10,\n",
        "        fontweight='bold', color='white', linespacing=1.4)\n",
        "    if x < 12:\n",
        "        ax.annotate('', xy=(x + 2.5, 3), xytext=(x + 2.25, 3),\n",
        "            arrowprops=dict(arrowstyle='->', color='#555', lw=2))\n",
        "\n",
        "facts = [\n",
        "    \"bfloat16 Mixed Precision\", \"Gradient Accumulation (4 steps)\",\n",
        "    \"Cosine LR: 1e-4 -> 5e-5\", \"Converged at 13K steps\",\n",
        "    f\"Best Val Loss: {best_val:.4f}\", f\"Perplexity: {math.exp(best_val):.2f}\"\n",
        "]\n",
        "for i, fact in enumerate(facts):\n",
        "    x_pos = 0.5 + i * 2.6\n",
        "    ax.text(x_pos, 0.5, f\"  {fact}\", fontsize=9, color='#333', fontweight='bold')\n",
        "\n",
        "ax.annotate('', xy=(14.5, 4.8), xytext=(0.5, 4.8),\n",
        "    arrowprops=dict(arrowstyle='<-', color='#F44336', lw=2, ls='--'))\n",
        "ax.text(7.5, 5.3, \"Backpropagation: Update weights -> Repeat 13,000 times\", ha='center',\n",
        "    fontsize=11, fontweight='bold', color='#F44336')\n",
        "\n",
        "plt.tight_layout()\n",
        "save(fig, \"14_training_pipeline.png\")\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# 15. SKIP CONNECTIONS\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "print(\"📊 15. Skip Connections Explained\")\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "fig.suptitle(\"Skip (Residual) Connections — Why Deep Networks Can Learn\",\n",
        "    fontsize=16, fontweight='bold', y=1.04)\n",
        "\n",
        "ax1.axis('off')\n",
        "ax1.set_xlim(0, 10)\n",
        "ax1.set_ylim(0, 10)\n",
        "ax1.set_title(\"Without Skip Connections\\n(Gradients vanish in deep networks)\", fontsize=12, fontweight='bold', color='#C62828')\n",
        "\n",
        "for i, (y, alpha) in enumerate([(8.5, 1.0), (6.5, 0.7), (4.5, 0.4), (2.5, 0.15), (0.5, 0.03)]):\n",
        "    r_val = min(int(200 * alpha + 55), 255)\n",
        "    hex_color = f'#{r_val:02x}{30:02x}{30:02x}'\n",
        "    rect = FancyBboxPatch((2, y), 6, 1.3, boxstyle=\"round,pad=0.1\",\n",
        "        facecolor=hex_color, edgecolor='white', linewidth=2)\n",
        "    ax1.add_patch(rect)\n",
        "    ax1.text(5, y+0.65, f\"Layer {i+1}  (gradient: {alpha:.0%})\", ha='center', va='center',\n",
        "        fontsize=11, fontweight='bold', color='white')\n",
        "    if i < 4:\n",
        "        ax1.annotate('', xy=(5, y+0.05), xytext=(5, y+0.02),\n",
        "            arrowprops=dict(arrowstyle='->', color='#C62828', lw=2))\n",
        "\n",
        "ax1.text(5, -0.5, \"By layer 5, gradient = 3%\\nModel can't learn!\", ha='center',\n",
        "    fontsize=11, color='#C62828', fontweight='bold')\n",
        "\n",
        "ax2.axis('off')\n",
        "ax2.set_xlim(0, 10)\n",
        "ax2.set_ylim(0, 10)\n",
        "ax2.set_title(\"With Skip Connections (Gemma 3)\\n(Gradients flow freely!)\", fontsize=12, fontweight='bold', color='#2E7D32')\n",
        "\n",
        "for i, y in enumerate([8.5, 6.5, 4.5, 2.5, 0.5]):\n",
        "    rect = FancyBboxPatch((2, y), 6, 1.3, boxstyle=\"round,pad=0.1\",\n",
        "        facecolor='#2E7D32', edgecolor='white', linewidth=2)\n",
        "    ax2.add_patch(rect)\n",
        "    ax2.text(5, y+0.65, f\"Layer {i+1}  (gradient: ~100%)\", ha='center', va='center',\n",
        "        fontsize=11, fontweight='bold', color='white')\n",
        "\n",
        "    if i < 4:\n",
        "        ax2.annotate('', xy=(8.8, y+0.5), xytext=(8.8, y+2.3),\n",
        "            arrowprops=dict(arrowstyle='->', color='#FF9800', lw=2.5, ls='--'))\n",
        "        ax2.add_patch(plt.Circle((8.8, y+0.5), 0.25, facecolor='#FF9800', edgecolor='white', linewidth=2))\n",
        "        ax2.text(8.8, y+0.5, \"+\", ha='center', va='center', fontsize=14, fontweight='bold', color='white')\n",
        "\n",
        "ax2.text(5, -0.5, \"output = layer(x) + x (skip)\\nGradients flow directly through '+'\", ha='center',\n",
        "    fontsize=11, color='#2E7D32', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "save(fig, \"15_skip_connections.png\")\n",
        "\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "# SUMMARY\n",
        "# ══════════════════════════════════════════════════════════════\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"  ALL VISUALS GENERATED!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"   Location: {VIS_DIR}\")\n",
        "print(f\"   Files created: {len(saved)}\")\n",
        "for i, name in enumerate(saved, 1):\n",
        "    print(f\"   {i:2d}. {name}\")\n",
        "print(f\"\\n   Ready for README and Blog!\")\n",
        "print(f\"{'='*60}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_45fon36EO4v",
        "outputId": "22138fc6-8695-4dda-c0d9-87fce5fdd4cc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 Generating Visual Assets...\n",
            "============================================================\n",
            "\n",
            "📊 1. Parameter Breakdown\n",
            "   ✅ 01_parameter_breakdown.png\n",
            "📊 2. Layer Types Pattern\n",
            "   ✅ 02_layer_types_pattern.png\n",
            "📊 3. MQA vs Standard Attention\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3674306027.py:180: UserWarning: Glyph 10060 (\\N{CROSS MARK}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-3674306027.py:180: UserWarning: Glyph 9989 (\\N{WHITE HEAVY CHECK MARK}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-3674306027.py:25: UserWarning: Glyph 10060 (\\N{CROSS MARK}) missing from font(s) DejaVu Sans.\n",
            "  fig.savefig(path, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
            "/tmp/ipython-input-3674306027.py:25: UserWarning: Glyph 9989 (\\N{WHITE HEAVY CHECK MARK}) missing from font(s) DejaVu Sans.\n",
            "  fig.savefig(path, bbox_inches='tight', facecolor='white', edgecolor='none')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✅ 03_mqa_vs_standard.png\n",
            "📊 4. Sliding Window vs Full Attention Masks\n",
            "   ✅ 04_attention_masks.png\n",
            "📊 5. RoPE Dual-Base Frequencies\n",
            "   ✅ 05_rope_dual_base.png\n",
            "📊 6. Enhanced Training Loss Curves\n",
            "   ✅ 06_training_curves.png\n",
            "📊 7. Learning Rate Schedule\n",
            "   ✅ 07_lr_schedule.png\n",
            "📊 8. Model Size Comparison\n",
            "   ✅ 08_model_size_comparison.png\n",
            "📊 9. Dataset Statistics\n",
            "   ✅ 09_dataset_statistics.png\n",
            "📊 10. GeGLU FFN Architecture\n",
            "   ✅ 10_geglu_ffn.png\n",
            "📊 11. RMSNorm Visualization\n",
            "   ✅ 11_rmsnorm.png\n",
            "📊 12. Temperature Effect on Sampling\n",
            "   ✅ 12_temperature_effect.png\n",
            "📊 13. Architecture Summary Table\n",
            "   ✅ 13_architecture_table.png\n",
            "📊 14. Training Pipeline Overview\n",
            "   ✅ 14_training_pipeline.png\n",
            "📊 15. Skip Connections Explained\n",
            "   ✅ 15_skip_connections.png\n",
            "\n",
            "============================================================\n",
            "  ALL VISUALS GENERATED!\n",
            "============================================================\n",
            "   Location: /content/drive/MyDrive/Gemma3_270M_Project/results/visuals\n",
            "   Files created: 15\n",
            "    1. 01_parameter_breakdown.png\n",
            "    2. 02_layer_types_pattern.png\n",
            "    3. 03_mqa_vs_standard.png\n",
            "    4. 04_attention_masks.png\n",
            "    5. 05_rope_dual_base.png\n",
            "    6. 06_training_curves.png\n",
            "    7. 07_lr_schedule.png\n",
            "    8. 08_model_size_comparison.png\n",
            "    9. 09_dataset_statistics.png\n",
            "   10. 10_geglu_ffn.png\n",
            "   11. 11_rmsnorm.png\n",
            "   12. 12_temperature_effect.png\n",
            "   13. 13_architecture_table.png\n",
            "   14. 14_training_pipeline.png\n",
            "   15. 15_skip_connections.png\n",
            "\n",
            "   Ready for README and Blog!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 16: Package Everything for GitHub                                 ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "GITHUB_DIR = Path(\"/content/drive/MyDrive/Gemma3_270M_Project/github_repo\")\n",
        "if GITHUB_DIR.exists():\n",
        "    shutil.rmtree(GITHUB_DIR)\n",
        "GITHUB_DIR.mkdir(parents=True)\n",
        "\n",
        "# ── 1. Copy visuals ──\n",
        "assets_dir = GITHUB_DIR / \"assets\" / \"visuals\"\n",
        "assets_dir.mkdir(parents=True)\n",
        "\n",
        "VIS_DIR = Path(\"/content/drive/MyDrive/Gemma3_270M_Project/results/visuals\")\n",
        "vis_count = 0\n",
        "for img in sorted(VIS_DIR.glob(\"*.png\")):\n",
        "    shutil.copy(img, assets_dir / img.name)\n",
        "    vis_count += 1\n",
        "    print(f\"   ✅ assets/visuals/{img.name}\")\n",
        "\n",
        "# Also copy loss curves from results\n",
        "RESULTS_DIR = Path(\"/content/drive/MyDrive/Gemma3_270M_Project/results\")\n",
        "for img in ['loss_curves.png', 'lr_schedule.png']:\n",
        "    src = RESULTS_DIR / img\n",
        "    if src.exists() and not (assets_dir / img).exists():\n",
        "        shutil.copy(src, assets_dir / img)\n",
        "        print(f\"   ✅ assets/visuals/{img}\")\n",
        "\n",
        "\n",
        "# ── 2. Create model.py (complete single-file implementation) ──\n",
        "model_code = '''\"\"\"\n",
        "Gemma 3 270M - Complete Implementation from Scratch\n",
        "====================================================\n",
        "Pre-trained on TinyStories dataset.\n",
        "\n",
        "Architecture: Custom Gemma 3 (164.6M parameters)\n",
        "Features: MQA, Sliding Window, RoPE (dual base), QK-Norm, GeGLU\n",
        "HuggingFace: https://huggingface.co/G3nadh/gemma3-270m-tinystories\n",
        "Credits: Vizuara Team - Raj (https://youtu.be/bLDlwcl6hbA)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.amp import autocast\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "from pathlib import Path\n",
        "import math\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "# ======= Configuration =======\n",
        "\n",
        "@dataclass\n",
        "class Gemma3Config:\n",
        "    vocab_size: int = 50_257\n",
        "    context_length: int = 32_768\n",
        "    emb_dim: int = 640\n",
        "    n_layers: int = 18\n",
        "    n_heads: int = 4\n",
        "    head_dim: int = 256\n",
        "    hidden_dim: int = 2048\n",
        "    n_kv_groups: int = 1\n",
        "    qk_norm: bool = True\n",
        "    query_pre_attn_scalar: int = 256\n",
        "    rope_base: float = 1_000_000.0\n",
        "    rope_local_base: float = 10_000.0\n",
        "    sliding_window: int = 512\n",
        "    dtype: str = \"bfloat16\"\n",
        "    layer_types: List[str] = field(default_factory=lambda: [\n",
        "        \"sliding_attention\", \"sliding_attention\", \"sliding_attention\",\n",
        "        \"sliding_attention\", \"sliding_attention\", \"full_attention\",\n",
        "        \"sliding_attention\", \"sliding_attention\", \"sliding_attention\",\n",
        "        \"sliding_attention\", \"sliding_attention\", \"full_attention\",\n",
        "        \"sliding_attention\", \"sliding_attention\", \"sliding_attention\",\n",
        "        \"sliding_attention\", \"sliding_attention\", \"full_attention\",\n",
        "    ])\n",
        "\n",
        "\n",
        "# ======= RMSNorm =======\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, emb_dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        rms = torch.sqrt(x.float().pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
        "        x_normed = x.float() / rms\n",
        "        return (x_normed * (1.0 + self.weight.float())).to(x.dtype)\n",
        "\n",
        "\n",
        "# ======= RoPE =======\n",
        "\n",
        "def precompute_rope_frequencies(head_dim, max_seq_len, rope_base=10_000.0, device=None):\n",
        "    assert head_dim % 2 == 0\n",
        "    i = torch.arange(0, head_dim, 2, device=device).float()\n",
        "    freqs = 1.0 / (rope_base ** (i / head_dim))\n",
        "    positions = torch.arange(max_seq_len, device=device).float()\n",
        "    angles = torch.outer(positions, freqs)\n",
        "    return torch.polar(torch.ones_like(angles), angles)\n",
        "\n",
        "\n",
        "def apply_rope(x, freqs_cis):\n",
        "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "    freqs_cis = freqs_cis.unsqueeze(0).unsqueeze(2)\n",
        "    x_rotated = x_complex * freqs_cis\n",
        "    return torch.view_as_real(x_rotated).reshape(*x.shape).to(x.dtype)\n",
        "\n",
        "\n",
        "# ======= Multi-Query Attention =======\n",
        "\n",
        "class MultiQueryAttention(nn.Module):\n",
        "    def __init__(self, config: Gemma3Config, layer_type: str):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_type = layer_type\n",
        "        self.n_heads = config.n_heads\n",
        "        self.n_kv = config.n_kv_groups\n",
        "        self.head_dim = config.head_dim\n",
        "        self.scaling = config.query_pre_attn_scalar ** -0.5\n",
        "\n",
        "        self.q_proj = nn.Linear(config.emb_dim, config.n_heads * config.head_dim, bias=False)\n",
        "        self.k_proj = nn.Linear(config.emb_dim, config.n_kv_groups * config.head_dim, bias=False)\n",
        "        self.v_proj = nn.Linear(config.emb_dim, config.n_kv_groups * config.head_dim, bias=False)\n",
        "        self.o_proj = nn.Linear(config.n_heads * config.head_dim, config.emb_dim, bias=False)\n",
        "\n",
        "        if config.qk_norm:\n",
        "            self.q_norm = RMSNorm(config.head_dim)\n",
        "            self.k_norm = RMSNorm(config.head_dim)\n",
        "\n",
        "        self.sliding_window = config.sliding_window if layer_type == \"sliding_attention\" else None\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        B, S, _ = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.head_dim)\n",
        "        k = self.k_proj(x).view(B, S, self.n_kv, self.head_dim)\n",
        "        v = self.v_proj(x).view(B, S, self.n_kv, self.head_dim)\n",
        "\n",
        "        if self.config.qk_norm:\n",
        "            q, k = self.q_norm(q), self.k_norm(k)\n",
        "\n",
        "        q, k = apply_rope(q, freqs_cis), apply_rope(k, freqs_cis)\n",
        "\n",
        "        if self.n_kv < self.n_heads:\n",
        "            k = k.expand(B, S, self.n_heads, self.head_dim)\n",
        "            v = v.expand(B, S, self.n_heads, self.head_dim)\n",
        "\n",
        "        q, k, v = [t.transpose(1, 2) for t in (q, k, v)]\n",
        "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scaling\n",
        "        attn = self._apply_mask(attn, S)\n",
        "        attn = F.softmax(attn, dim=-1, dtype=torch.float32).to(q.dtype)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, S, -1)\n",
        "        return self.o_proj(out)\n",
        "\n",
        "    def _apply_mask(self, attn, seq_len):\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len, device=attn.device, dtype=torch.bool), diagonal=1)\n",
        "        if self.sliding_window is not None:\n",
        "            mask = mask | torch.tril(torch.ones(seq_len, seq_len, device=attn.device, dtype=torch.bool),\n",
        "                diagonal=-(self.sliding_window + 1))\n",
        "        return attn.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "\n",
        "\n",
        "# ======= Feed Forward (GeGLU) =======\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config: Gemma3Config):\n",
        "        super().__init__()\n",
        "        self.gate_proj = nn.Linear(config.emb_dim, config.hidden_dim, bias=False)\n",
        "        self.up_proj = nn.Linear(config.emb_dim, config.hidden_dim, bias=False)\n",
        "        self.down_proj = nn.Linear(config.hidden_dim, config.emb_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(F.gelu(self.gate_proj(x), approximate=\"tanh\") * self.up_proj(x))\n",
        "\n",
        "\n",
        "# ======= Transformer Block =======\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config: Gemma3Config, layer_idx: int):\n",
        "        super().__init__()\n",
        "        self.layer_type = config.layer_types[layer_idx]\n",
        "        self.attn_norm = RMSNorm(config.emb_dim)\n",
        "        self.attention = MultiQueryAttention(config, self.layer_type)\n",
        "        self.ffn_norm = RMSNorm(config.emb_dim)\n",
        "        self.ffn = FeedForward(config)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        x = x + self.attention(self.attn_norm(x), freqs_cis)\n",
        "        x = x + self.ffn(self.ffn_norm(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# ======= Complete Model =======\n",
        "\n",
        "class Gemma3Model(nn.Module):\n",
        "    def __init__(self, config: Gemma3Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embed_tokens = nn.Embedding(config.vocab_size, config.emb_dim)\n",
        "        self.layers = nn.ModuleList([TransformerBlock(config, i) for i in range(config.n_layers)])\n",
        "        self.final_norm = RMSNorm(config.emb_dim)\n",
        "        self.output_proj = nn.Linear(config.emb_dim, config.vocab_size, bias=False)\n",
        "\n",
        "        self.register_buffer(\"freqs_local\", precompute_rope_frequencies(\n",
        "            config.head_dim, config.context_length, config.rope_local_base), persistent=False)\n",
        "        self.register_buffer(\"freqs_global\", precompute_rope_frequencies(\n",
        "            config.head_dim, config.context_length, config.rope_base), persistent=False)\n",
        "\n",
        "        self.emb_scale = config.emb_dim ** 0.5\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, input_ids, targets=None):\n",
        "        B, S = input_ids.shape\n",
        "        x = self.embed_tokens(input_ids) * self.emb_scale\n",
        "        for layer in self.layers:\n",
        "            freqs = self.freqs_local[:S] if layer.layer_type == \"sliding_attention\" else self.freqs_global[:S]\n",
        "            x = layer(x, freqs)\n",
        "        logits = self.output_proj(self.final_norm(x))\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, self.config.vocab_size), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_ids, max_new_tokens=200, temperature=0.8, top_k=50):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx = input_ids if input_ids.size(1) <= self.config.context_length else input_ids[:, -self.config.context_length:]\n",
        "            logits, _ = self(idx)\n",
        "            logits = logits[:, -1, :] / max(temperature, 1e-5)\n",
        "            if top_k and top_k > 0:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = float('-inf')\n",
        "            input_ids = torch.cat([input_ids, torch.multinomial(F.softmax(logits, dim=-1), 1)], dim=1)\n",
        "        return input_ids\n",
        "\n",
        "\n",
        "# ======= Inference Helpers =======\n",
        "\n",
        "def load_model(checkpoint_path: str, device: str = \"cuda\"):\n",
        "    config = Gemma3Config()\n",
        "    model = Gemma3Model(config)\n",
        "    state_dict = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    if 'model_state_dict' in state_dict:\n",
        "        state_dict = state_dict['model_state_dict']\n",
        "    model.load_state_dict(state_dict)\n",
        "    model = model.to(device=device, dtype=torch.bfloat16)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def generate_text(model, prompt: str, max_tokens=200, temperature=0.7, top_k=50, device=\"cuda\"):\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    input_ids = torch.tensor([enc.encode_ordinary(prompt)], device=device)\n",
        "    with torch.no_grad(), autocast(device_type=device if device != \"cpu\" else \"cpu\", dtype=torch.bfloat16):\n",
        "        output = model.generate(input_ids, max_tokens, temperature, top_k)\n",
        "    return enc.decode(output[0].tolist())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    ckpt = sys.argv[1] if len(sys.argv) > 1 else \"pytorch_model.bin\"\n",
        "\n",
        "    print(f\"Loading model from {ckpt}...\")\n",
        "    model = load_model(ckpt, device)\n",
        "    print(f\"Model loaded! ({sum(p.numel() for p in model.parameters()):,} parameters)\")\n",
        "\n",
        "    prompts = [\n",
        "        \"Once upon a time, there was a little girl named Lily\",\n",
        "        \"The big brown dog ran to the park\",\n",
        "        \"One day, a magical bird flew into the garden\",\n",
        "    ]\n",
        "\n",
        "    for prompt in prompts:\n",
        "        print(f\"\\\\nPrompt: \\\\\"{prompt}\\\\\"\")\n",
        "        print(f\"Story:  {generate_text(model, prompt, device=device)}\")\n",
        "        print(\"-\" * 60)\n",
        "'''\n",
        "\n",
        "with open(GITHUB_DIR / \"model.py\", \"w\") as f:\n",
        "    f.write(model_code)\n",
        "print(\"✅ model.py\")\n",
        "\n",
        "\n",
        "# ── 3. Create requirements.txt ──\n",
        "reqs = \"\"\"torch>=2.0.0\n",
        "tiktoken>=0.5.0\n",
        "numpy>=1.24.0\n",
        "datasets>=2.14.0\n",
        "matplotlib>=3.7.0\n",
        "tqdm>=4.65.0\n",
        "huggingface_hub>=0.19.0\n",
        "\"\"\"\n",
        "with open(GITHUB_DIR / \"requirements.txt\", \"w\") as f:\n",
        "    f.write(reqs)\n",
        "print(\"✅ requirements.txt\")\n",
        "\n",
        "\n",
        "# ── 4. Create .gitignore ──\n",
        "gitignore = \"\"\"__pycache__/\n",
        "*.pyc\n",
        "*.pth\n",
        "*.bin\n",
        "*.pt\n",
        "data/\n",
        "checkpoints/\n",
        "*.egg-info/\n",
        ".env\n",
        "wandb/\n",
        ".ipynb_checkpoints/\n",
        "\"\"\"\n",
        "with open(GITHUB_DIR / \".gitignore\", \"w\") as f:\n",
        "    f.write(gitignore)\n",
        "print(\"✅ .gitignore\")\n",
        "\n",
        "\n",
        "# ── 5. Create notebooks directory ──\n",
        "notebooks_dir = GITHUB_DIR / \"notebooks\"\n",
        "notebooks_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Placeholder - you'll copy your actual notebook here\n",
        "with open(notebooks_dir / \"README.md\", \"w\") as f:\n",
        "    f.write(\"# Notebooks\\n\\nPlace your `Gemma3_Pretrain.ipynb` Colab notebook here.\\n\")\n",
        "print(\"✅ notebooks/README.md\")\n",
        "\n",
        "\n",
        "# ── 6. Summary ──\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"  GITHUB REPO PACKAGED!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"   Location: {GITHUB_DIR}\\n\")\n",
        "\n",
        "total_files = 0\n",
        "for f in sorted(GITHUB_DIR.rglob(\"*\")):\n",
        "    if f.is_file():\n",
        "        total_files += 1\n",
        "        rel = f.relative_to(GITHUB_DIR)\n",
        "        size = f.stat().st_size\n",
        "        print(f\"   {rel}  ({size/1024:.1f} KB)\")\n",
        "\n",
        "print(f\"\\n   Total files: {total_files}\")\n",
        "print(f\"   Visuals: {vis_count} images\")\n",
        "\n",
        "print(f\"\"\"\n",
        "   NEXT STEPS:\n",
        "   ───────────────────────────────────────\n",
        "   1. Create a new repo on github.com\n",
        "   2. Download this folder from Google Drive\n",
        "   3. Push to GitHub:\n",
        "      cd github_repo\n",
        "      git init\n",
        "      git add .\n",
        "      git commit -m \"Gemma 3 270M pretrained from scratch\"\n",
        "      git branch -M main\n",
        "      git remote add origin https://github.com/YOUR_USERNAME/gemma3-270m-pretrained.git\n",
        "      git push -u origin main\n",
        "   4. Paste README.md from Claude into the repo\n",
        "\n",
        "   Your HuggingFace: https://huggingface.co/G3nadh/gemma3-270m-tinystories\n",
        "\"\"\")\n",
        "print(f\"{'='*60}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUjzQnCTFm-m",
        "outputId": "650549f5-f0fd-4342-c2fa-f9accd09f66f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✅ assets/visuals/01_parameter_breakdown.png\n",
            "   ✅ assets/visuals/02_layer_types_pattern.png\n",
            "   ✅ assets/visuals/03_mqa_vs_standard.png\n",
            "   ✅ assets/visuals/04_attention_masks.png\n",
            "   ✅ assets/visuals/05_rope_dual_base.png\n",
            "   ✅ assets/visuals/06_training_curves.png\n",
            "   ✅ assets/visuals/07_lr_schedule.png\n",
            "   ✅ assets/visuals/08_model_size_comparison.png\n",
            "   ✅ assets/visuals/09_dataset_statistics.png\n",
            "   ✅ assets/visuals/10_geglu_ffn.png\n",
            "   ✅ assets/visuals/11_rmsnorm.png\n",
            "   ✅ assets/visuals/12_temperature_effect.png\n",
            "   ✅ assets/visuals/13_architecture_table.png\n",
            "   ✅ assets/visuals/14_training_pipeline.png\n",
            "   ✅ assets/visuals/15_skip_connections.png\n",
            "   ✅ assets/visuals/loss_curves.png\n",
            "   ✅ assets/visuals/lr_schedule.png\n",
            "✅ model.py\n",
            "✅ requirements.txt\n",
            "✅ .gitignore\n",
            "✅ notebooks/README.md\n",
            "\n",
            "============================================================\n",
            "  GITHUB REPO PACKAGED!\n",
            "============================================================\n",
            "   Location: /content/drive/MyDrive/Gemma3_270M_Project/github_repo\n",
            "\n",
            "   .gitignore  (0.1 KB)\n",
            "   assets/visuals/01_parameter_breakdown.png  (155.0 KB)\n",
            "   assets/visuals/02_layer_types_pattern.png  (53.0 KB)\n",
            "   assets/visuals/03_mqa_vs_standard.png  (105.1 KB)\n",
            "   assets/visuals/04_attention_masks.png  (102.2 KB)\n",
            "   assets/visuals/05_rope_dual_base.png  (887.2 KB)\n",
            "   assets/visuals/06_training_curves.png  (152.1 KB)\n",
            "   assets/visuals/07_lr_schedule.png  (80.0 KB)\n",
            "   assets/visuals/08_model_size_comparison.png  (81.6 KB)\n",
            "   assets/visuals/09_dataset_statistics.png  (123.8 KB)\n",
            "   assets/visuals/10_geglu_ffn.png  (128.8 KB)\n",
            "   assets/visuals/11_rmsnorm.png  (147.5 KB)\n",
            "   assets/visuals/12_temperature_effect.png  (94.7 KB)\n",
            "   assets/visuals/13_architecture_table.png  (213.2 KB)\n",
            "   assets/visuals/14_training_pipeline.png  (109.8 KB)\n",
            "   assets/visuals/15_skip_connections.png  (150.5 KB)\n",
            "   assets/visuals/loss_curves.png  (100.3 KB)\n",
            "   assets/visuals/lr_schedule.png  (40.0 KB)\n",
            "   model.py  (9.9 KB)\n",
            "   notebooks/README.md  (0.1 KB)\n",
            "   requirements.txt  (0.1 KB)\n",
            "\n",
            "   Total files: 21\n",
            "   Visuals: 15 images\n",
            "\n",
            "   NEXT STEPS:\n",
            "   ───────────────────────────────────────\n",
            "   1. Create a new repo on github.com\n",
            "   2. Download this folder from Google Drive\n",
            "   3. Push to GitHub:\n",
            "      cd github_repo\n",
            "      git init\n",
            "      git add .\n",
            "      git commit -m \"Gemma 3 270M pretrained from scratch\"\n",
            "      git branch -M main\n",
            "      git remote add origin https://github.com/YOUR_USERNAME/gemma3-270m-pretrained.git\n",
            "      git push -u origin main\n",
            "   4. Paste README.md from Claude into the repo\n",
            "\n",
            "   Your HuggingFace: https://huggingface.co/G3nadh/gemma3-270m-tinystories\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2M7t4T6RJ544"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}